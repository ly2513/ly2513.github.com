[{"title":"mysql 性能优化方案","date":"2018-02-27T06:35:13.000Z","path":"2018/02/27/mysql-性能优化方案/","text":"网 上有不少mysql 性能优化方案，不过，mysql的优化同sql server相比，更为麻烦与复杂，同样的设置，在不同的环境下 ，由于内存，访问量，读写频率，数据差异等等情况，可能会出现不同的结果，因此简单地根据某个给出方案来配置mysql是行不通的，最好能使用 status信息对mysql进行具体的优化。 1mysql&gt; show global status; 可以列出mysql服务器运行各种状态值，另外，查询mysql服务器配置信息语句：1mysql&gt; show variables; 一、慢查询123456789101112131415mysql&gt; show variables like &apos;slow%&apos;;+------------------+-------+| variable_name | value |+------------------+-------+| log_slow_queries | on || slow_launch_time | 2 |+------------------+-------+mysql&gt; show global status like &apos;slow%&apos;;+---------------------+-------+| variable_name | value |+---------------------+-------+| slow_launch_threads | 0 || slow_queries | 4148 |+---------------------+-------+ 配 置中打开了记录慢查询，执行时间超过2秒的即为慢查询，系统显示有4148个慢查询，你可以分析慢查询日志，找出有问题的sql语句，慢查询时间不宜设置 过长，否则意义不大，最好在5秒以内，如果你需要微秒级别的慢查询，可以考虑给mysql打补丁：http://www.percona.com /docs/wiki/release:start，记得找对应的版本。 打开慢查询日志可能会对系统性能有一点点影响，如果你的mysql是主－从结构，可以考虑打开其中一台从服务器的慢查询日志，这样既可以监控慢查询，对系统性能影响又小。 二、连接数经 常会遇见”mysql: error 1040: too many connections”的情况，一种是访问量确实很高，mysql服务器抗不住，这个时候就要考虑增加从服务器分散读压力，另外一种情况是mysql配 置文件中max_connections值过小： 123456mysql&gt; show variables like &apos;max_connections&apos;;+-----------------+-------+| variable_name | value |+-----------------+-------+| max_connections | 256 |+-----------------+-------+ 这台mysql服务器最大连接数是256，然后查询一下服务器响应的最大连接数： 1mysql&gt; show global status like &apos;max_used_connections&apos;; mysql服务器过去的最大连接数是245，没有达到服务器连接数上限256，应该没有出现1040错误，比较理想的设置是 1max_used_connections / max_connections * 100% ≈ 85% 最大连接数占上限连接数的85％左右，如果发现比例在10%以下，mysql服务器连接数上限设置的过高了。 三、key_buffer_sizekey_buffer_size是对myisam表性能影响最大的一个参数，下面一台以myisam为主要存储引擎服务器的配置： 123456mysql&gt; show variables like &apos;key_buffer_size&apos;;+-----------------+------------+| variable_name | value |+-----------------+------------+| key_buffer_size | 536870912 |+-----------------+------------+ 分配了512mb内存给key_buffer_size，我们再看一下key_buffer_size的使用情况： 1234567mysql&gt; show global status like &apos;key_read%&apos;;+------------------------+-------------+| variable_name | value |+------------------------+-------------+| key_read_requests | 27813678764 || key_reads | 6798830 |+------------------------+-------------+ 一共有27813678764个索引读取请求，有6798830个请求在内存中没有找到直接从硬盘读取索引，计算索引未命中缓存的概率： 1key_cache_miss_rate ＝ key_reads / key_read_requests * 100% 比 如上面的数据，key_cache_miss_rate为0.0244%，4000个索引读取请求才有一个直接读硬盘，已经很bt 了，key_cache_miss_rate在0.1%以下都很好（每1000个请求有一个直接读硬盘），如果key_cache_miss_rate在 0.01%以下的话，key_buffer_size分配的过多，可以适当减少。 mysql服务器还提供了keyblocks*参数： 1234567mysql&gt; show global status like &apos;key_blocks_u%&apos;;+------------------------+-------------+| variable_name | value |+------------------------+-------------+| key_blocks_unused | 0 || key_blocks_used | 413543 |+------------------------+-------------+ key_blocks_unused 表示未使用的缓存簇(blocks)数，key_blocks_used表示曾经用到的最大的blocks数，比如这台服务器，所有的缓存都用到了，要么 增加key_buffer_size，要么就是过渡索引了，把缓存占满了。比较理想的设置： 1key_blocks_used / (key_blocks_unused + key_blocks_used) * 100% ≈ 80% 四、临时表12345678mysql&gt; show global status like &apos;created_tmp%&apos;;+-------------------------+---------+| variable_name | value |+-------------------------+---------+| created_tmp_disk_tables | 21197 || created_tmp_files | 58 || created_tmp_tables | 1771587 |+-------------------------+---------+ 每次创建临时表，created_tmp_tables增加，如果是在磁盘上创建临时表，created_tmp_disk_tables也增加,created_tmp_files表示mysql服务创建的临时文件文件数，比较理想的配置是：created_tmp_disk_tables / created_tmp_tables 100% &lt;= 25%比如上面的服务器created_tmp_disk_tables / created_tmp_tables 100% ＝ 1.20%，应该相当好了。我们再看一下mysql服务器对临时表的配置： 1234567mysql&gt; show variables where variable_name in (&apos;tmp_table_size&apos;, &apos;max_heap_table_size&apos;);+---------------------+-----------+| variable_name | value |+---------------------+-----------+| max_heap_table_size | 268435456 || tmp_table_size | 536870912 |+---------------------+-----------+ 只有256mb以下的临时表才能全部放内存，超过的就会用到硬盘临时表。 五、open table情况1234567mysql&gt; show global status like &apos;open%tables%&apos;;+---------------+-------+| variable_name | value |+---------------+-------+| open_tables | 919 || opened_tables | 1951 |+---------------+-------+ open_tables 表示打开表的数量，opened_tables表示打开过的表数量，如果opened_tables数量过大，说明配置中 table_cache(5.1.3之后这个值叫做table_open_cache)值可能太小，我们查询一下服务器table_cache值： 123456mysql&gt; show variables like &apos;table_cache&apos;;+---------------+-------+| variable_name | value |+---------------+-------+| table_cache | 2048 |+---------------+-------+ 比较合适的值为： 12open_tables / opened_tables * 100% &gt;= 85%open_tables / table_cache * 100% &lt;= 95% 六、进程使用情况123456789mysql&gt; show global status like &apos;thread%&apos;;+-------------------+-------+| variable_name | value |+-------------------+-------+| threads_cached | 46 || threads_connected | 2 || threads_created | 570 || threads_running | 1 |+-------------------+-------+ 如 果我们在mysql服务器配置文件中设置了thread_cache_size，当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户 而不是销毁（前提是缓存数未达上限）。threads_created表示创建过的线程数，如果发现threads_created值过大的话，表明 mysql服务器一直在创建线程，这也是比较耗资源，可以适当增加配置文件中thread_cache_size值，查询服务器 thread_cache_size配置： 123456mysql&gt; show variables like &apos;thread_cache_size&apos;;+-------------------+-------+| variable_name | value |+-------------------+-------+| thread_cache_size | 64 |+-------------------+-------+ 示例中的服务器还是挺健康的。 七、查询缓存(query cache)12345678910111213mysql&gt; show global status like &apos;qcache%&apos;;+-------------------------+-----------+| variable_name | value |+-------------------------+-----------+| qcache_free_blocks | 22756 || qcache_free_memory | 76764704 || qcache_hits | 213028692 || qcache_inserts | 208894227 || qcache_lowmem_prunes | 4010916 || qcache_not_cached | 13385031 || qcache_queries_in_cache | 43560 || qcache_total_blocks | 111212 |+-------------------------+-----------+ mysql查询缓存变量解释： qcache_free_blocks：缓存中相邻内存块的个数。数目大说明可能有碎片。flush query cache会对缓存中的碎片进行整理，从而得到一个空闲块。 qcache_free_memory：缓存中的空闲内存。 qcache_hits：每次查询在缓存中命中时就增大 qcache_inserts：每次插入一个查询时就增大。命中次数除以插入次数就是不中比率。 qcache_lowmem_prunes： 缓存出现内存不足并且必须要进行清理以便为更多查询提供空间的次数。这个数字最好长时间来看；如果这个数字在不断增长，就表示可能碎片非常严重，或者内存 很少。（上面的 free_blocks和free_memory可以告诉您属于哪种情况） qcache_not_cached：不适合进行缓存的查询的数量，通常是由于这些查询不是 select 语句或者用了now()之类的函数。 qcache_queries_in_cache：当前缓存的查询（和响应）的数量。 qcache_total_blocks：缓存中块的数量。 我们再查询一下服务器关于query_cache的配置： 12345678910mysql&gt; show variables like &apos;query_cache%&apos;;+------------------------------+-----------+| variable_name | value |+------------------------------+-----------+| query_cache_limit | 2097152 || query_cache_min_res_unit | 4096 || query_cache_size | 203423744 || query_cache_type | on || query_cache_wlock_invalidate | off |+------------------------------+-----------+ 各字段的解释： query_cache_limit：超过此大小的查询将不缓存 query_cache_min_res_unit：缓存块的最小大小 query_cache_size：查询缓存大小 query_cache_type：缓存类型，决定缓存什么样的查询，示例中表示不缓存 select sql_no_cache 查询 query_cache_wlock_invalidate：当有其他客户端正在对myisam表进行写操作时，如果查询在query cache中，是否返回cache结果还是等写操作完成再读表获取结果。 query_cache_min_res_unit的配置是一柄”双刃剑”，默认是4kb，设置值大对大数据查询有好处，但如果你的查询都是小数据查询，就容易造成内存碎片和浪费。 查询缓存碎片率 = qcache_free_blocks / qcache_total_blocks * 100% 如果查询缓存碎片率超过20%，可以用flush query cache整理缓存碎片，或者试试减小query_cache_min_res_unit，如果你的查询都是小数据量的话。 查询缓存利用率 = (query_cache_size - qcache_free_memory) / query_cache_size * 100% 查询缓存利用率在25%以下的话说明query_cache_size设置的过大，可适当减小；查询缓存利用率在80％以上而且qcache_lowmem_prunes &gt; 50的话说明query_cache_size可能有点小，要不就是碎片太多。 查询缓存命中率 = (qcache_hits - qcache_inserts) / qcache_hits * 100% 示例服务器 查询缓存碎片率 ＝ 20.46％，查询缓存利用率 ＝ 62.26％，查询缓存命中率 ＝ 1.94％，命中率很差，可能写操作比较频繁吧，而且可能有些碎片。 八、排序使用情况123456789mysql&gt; show global status like &apos;sort%&apos;;+-------------------+------------+| variable_name | value |+-------------------+------------+| sort_merge_passes | 29 || sort_range | 37432840 || sort_rows | 9178691532 || sort_scan | 1860569 |+-------------------+------------+ sort_merge_passes 包括两步。mysql 首先会尝试在内存中做排序，使用的内存大小由系统变量 sort_buffer_size 决定，如果它的大小不够把所有的记录都读到内存中，mysql 就会把每次在内存中排序的结果存到临时文件中，等 mysql 找到所有记录之后，再把临时文件中的记录做一次排序。这再次排序就会增加 sort_merge_passes。实际上，mysql 会用另一个临时文件来存再次排序的结果，所以通常会看到 sort_merge_passes 增加的数值是建临时文件数的两倍。因为用到了临时文件，所以速度可能会比较慢，增加 sort_buffer_size 会减少 sort_merge_passes 和 创建临时文件的次数。但盲目的增加 sort_buffer_size 并不一定能提高速度，见 how fast can you sort data with mysql?（引自http://qroom.blogspot.com/2007/09/mysql-select-sort.html，貌似被墙） 另外，增加read_rnd_buffer_size(3.2.3是record_rnd_buffer_size)的值对排序的操作也有一点的好处，参见：http://www.mysqlperformanceblog.com/2007/07/24/what-exactly-is-read_rnd_buffer_size/ 九、文件打开数(open_files)12345678910111213mysql&gt; show global status like &apos;open_files&apos;;+---------------+-------+| variable_name | value |+---------------+-------+| open_files | 1410 |+---------------+-------+mysql&gt; show variables like &apos;open_files_limit&apos;;+------------------+-------+| variable_name | value |+------------------+-------+| open_files_limit | 4590 |+------------------+-------+ 比较合适的设置：open_files / open_files_limit * 100% &lt;= 75％ 十、表锁情况1234567mysql&gt; show global status like &apos;table_locks%&apos;;+-----------------------+-----------+| variable_name | value |+-----------------------+-----------+| table_locks_immediate | 490206328 || table_locks_waited | 2084912 |+-----------------------+-----------+ table_locks_immediate表示立即释放表锁数，table_locks_waited表示需要等待的表锁数，如果 table_locks_immediate / table_locks_waited &gt; 5000，最好采用innodb引擎，因为innodb是行锁而myisam是表锁，对于高并发写入的应用innodb效果会好些。示例中的服务器 table_locks_immediate / table_locks_waited ＝ 235，myisam就足够了。 十一、表扫描情况1234567891011mysql&gt; show global status like &apos;handler_read%&apos;;+-----------------------+-------------+| variable_name | value |+-----------------------+-------------+| handler_read_first | 5803750 || handler_read_key | 6049319850 || handler_read_next | 94440908210 || handler_read_prev | 34822001724 || handler_read_rnd | 405482605 || handler_read_rnd_next | 18912877839 |+-----------------------+-------------+ 各字段解释参见http://hi.baidu.com/thinkinginlamp/blog/item/31690cd7c4bc5cdaa144df9c.html，调出服务器完成的查询请求次数： 123456mysql&gt; show global status like &apos;com_select&apos;;+---------------+-----------+| variable_name | value |+---------------+-----------+| com_select | 222693559 |+---------------+-----------+ 计算表扫描率： 表扫描率 ＝ handler_read_rnd_next / com_select 如果表扫描率超过4000，说明进行了太多表扫描，很有可能索引没有建好，增加read_buffer_size值会有一些好处，但最好不要超过8mb。 后记： 文中提到一些数字都是参考值，了解基本原理就可以，除了mysql提供的各种status值外，操作系统的一些性能指标也很重要，比如常用的top,iostat等，尤其是iostat，现在的系统瓶颈一般都在磁盘io上，关于iostat的使用，可以参考：http://www.php-oa.com/2009/02/03/iostat.html","tags":[{"name":"mysql","slug":"mysql","permalink":"ly2513.github.com/tags/mysql/"}]},{"title":"Kafka端到端审计","date":"2018-01-30T15:13:38.000Z","path":"2018/01/30/Kafka端到端审计/","text":"概述Kafka端到端审计是指生产者生产的消息存入至broker，以及消费者从broker中消费消息这个过程之间消息个数及延迟的审计，以此可以检测是否有数据丢失，是否有数据重复以及端到端的延迟等。目前主要调研了3个产品： Chaperone (Uber) Confluent Control Center（非开源，收费） Kafka Monitor (LinkedIn)对于Kafka端到端的审计主要通过： 消息payload中内嵌时间戳timestamp 消息payload中内嵌全局index 消息payload中内嵌timestamp和index 内嵌timestamp的方式 主要是通过设置一个审计的时间间隔（这里称之为time_bucket_interval，可以设置几秒或者几分钟，这个可以自定义）, 每个timestamp都会被分配到相应的桶中，算法有： timestamp - timestamp%time_bucket_interval floor((timestamp /15)*15) 这样可以获得相应time_bucket的起始时间time_bucket_start，一个time_bucket的区间可以记录为[time_bucket_start, time_bucket_start+time_bucket_interval]。每发送或者消费一条消息可以根据消息payload内嵌的时间戳，分配到相应桶中，然后对桶进行计数，之后进行存储，简单的可以存储到，比如：Map之中。 内嵌index的方式 这种方式就更容易理解了，对于每条消息都分配一个全局唯一的index，如果topic及相应的partition固定的话，可以为每一个topic-partition设置一个全局的index，当有消息发送到某个topic-partition中，那么首先获取其topic-partition对应的index, 然后内嵌到payload中，之后再发送到broker。消费者进行消费审计，可以判断出哪个消息丢失，哪个消息重复等等。如果要计算端到端延迟的话，还需要在payload中内嵌timestamp以作相应的计算。下面来简要分析下三个产品。 Chaperone github地址：https://github.com/uber/chaperone 官方介绍（中文）：http://www.infoq.com/cn/news/2016/12/Uber-Chaperone-Kafka 官方介绍（英文）：https://eng.uber.com/chaperone/ Chaperone进行消息端到端的校验主要是基于message内置timestamp实现的，根据timestamp将message分配到不同的bucket中。之后就是对这个bucket中的消息进行计数等一系列的audit操作，然后将这个audit操作之后的信息auditMessage保存起来，auditMessage的内容： topicName：被audit的topic time_bucket_start：bucket的起始时间 time_bucket_end metrics_count：time_bucket中的个数 metrics_mean_latency, metrics_p95_latency, metrics_p99_latency，metrics_max_latency：延迟 tier hostname datacenter uuid 注意: 这里的latency的计算规则是：currentTimeMillis - (timestamp*1000)。 Chaperone的架构Chaperone的整体架构分为：AuditLibrary, ChaperoneService, ChaperoneCollector和WebService, 它们会收集数据，并进行相关计算，自动检测出丢失和延迟的数据，并展示审计结果。从Chaperone的github上的源码来看：Chaperone分为ChaperoneClient, ChaperoneCollector, ChaperoneDistribution, ChaperoneServiceController, ChaperoneServiceWorker这5个子项目。对比着上面的架构图来分析。 ChaperoneClient对应着AuditLibrary，主要是用来audit message的库（library），并不以实际服务运行，可以在Producer或者Consumer客户端中调用，默认使用10mins的滚动时间bucket来不断地从每个主题收集消息。然后发送到kafka的chaperone-audit这个topic中。官方文档介绍说AuditLibrary会被ChaperoneService, ChaperoneCollector和WebService这三个组件所依赖，但代码中来看并非完全如此，略有出入。 ChaperoneDistribution可以忽略 ChaperoneServiceController和ChaperoneServiceWorker对应架构图中的ChaperoneService，ChaperoneServiceController主要用来检测topics并分配topic-partitions给ChaperoneServiceWorker用以审计（audit）。ChaperoneServiceWorker主要是audit message的一个服务。 ChaperoneServiceWorker采用scala语言编写，内部又将ChaperoneClient或者说AuditLibrary又重新用Scala实现了一番，并丰富了一下应用，比如采用hsqldb存储数据，zk存取offsets来实现WAL(预写式日志，具体可见下段介绍) Chaperone认为message中内嵌timestamp是十分必须的，但是从ChaperoneServiceWorker的代码来看消息没有timestamp也能运行，当消息没有时间戳，那么会记录noTimeMsgCount，Chaperone介绍会有一个牛逼的算法来分析消息中的timestamp(其实就是读取消息的开头部分，而不是全部整条消息，类似报文截断解析，下面也有涉及介绍)，如果解析timestamp失败，会记录malformedMsgCount。 ChaperoneCollector对是用来读取audit的数据，然后持久化操作，默认存入mysql中，看代码也可选存入redis中。 源码中没有WebService这个东西，估计是uber内部的web系统，读取下mysql中的内容展示到页面而已。 如果程序段内嵌Audit Library（ChaperoneClient）,那么整个audit过程如下： 如果producer端或者consumer端需要进行消息审计，那么可以内嵌Audit Library。就以发送端为例，消息先发送到kafka中，然后对这条消息进行审计，将审计的信息存入到kafka中，之后有专门的ChaperoneServiceCollector进行数据消费，之后存入mysql中，也可以选择存入redis中。页面系统webService可以查询mysql(redis)中的数据，之后进而在页面中展示。如果使用ChanperoneServiceWork，整个流转过程如下： 上面是对broker端进行审计的过程。首先从存储消息的kafka(图中上面的kafka)中消费数据，之后对收到的消息进行审计操作，之后将审计消息auditmsg以及相应的offset存储起来（auditmsg存入hsqldb中，offset存到用来存储审计数据的kafka的zk之中），之后再将审计消息auditmsg存入kafka(图中下面的kafka)中，最后成功存储并返回给消费端（Consumer1，即ChaperoneServiceWork）,之后再把hsqldb中的auditmsg标记为已统计。之后ChaperoneServiceCollector和producer端（consumer端）内嵌Audit Library时相同。官方文档部分介绍如下： 每个消息只被审计一次 为了确保每个消息只被审计一次，ChaperoneService使用了预写式日志（WAL）。ChaperoneService每次在触发Kafka审计消息时，会往审计消息里添加一个UUID。这个带有相关偏移量的消息在发送到Kafka之前被保存在WAL里。在得到Kafka的确认之后，WAL里的消息被标记为已完成。如果ChaperoneService崩溃，在重启后它可以重新发送WAL里未被标记的审计消息，并定位到最近一次的审计偏移量，然后继续消费。WAL确保了每个Kafka消息只被审计一次，而且每个审计消息至少会被发送一次。接下来，ChaperoneCollector使用ChaperoneService之前添加过的UUID来移除重复消息。有了UUID和WAL，我们可以确保审计的一次性。在代理客户端和服务器端难以实现一次性保证，因为这样会给它们带来额外的开销。我们依赖它们的优雅关闭操作，这样它们的状态才会被冲刷出去。 在层间使用一致性的时间戳 因为Chaperone可以在多个层里看到相同的Kafka消息，所以为消息内嵌时间戳是很有必要的。如果没有这些时间戳，在计数时会发生时间错位。在Uber，大部分发送到Kafka的数据要么使用avro风格的schema编码，要么使用JSON格式。对于使用schema编码的消息，可以直接获取时间戳。而对于JSON格式的消息，需要对JSON数据进行解码才能拿到时间戳。为了加快这个过程，我们实现了一个基于流的JSON消息解析器，这个解析器无需预先解码整个消息就可以扫描到时间戳。这个解析器用在ChaperoneService里是很高效的，不过对代理客户端和服务器来说仍然需要付出很高代价。所以在这两个层里，我们使用的是消息的处理时间戳。因为时间戳的不一致造成的层间计数差异可能会触发错误的数据丢失警告。我们正在着手解决时间戳不一致问题，之后也会把解决方案公布出来。 温馨提示： github上的quickstart中，如果不能根据脚本自动安装kafka和zk，而是自己安装kafka和zk的话，需要改动脚本、配置文件甚至源码才能将服务运行起来。另外需要安装hsqldb和mysql（redis）。 Confluent Control Center文档地址：http://docs.confluent.io/3.0.0/control-center/docs/index.html这是个收费产品，文档中介绍的并不多。和Chaperone相同，主要也是根据消息payload内嵌timestamp来实现，计算time_bucket的算法是：floor((timestamp /15)*15)。架构图如下： 主要是在producer端或者consumer端内嵌审计程序（相当于Chaperone的Audit Library）继续审计，最终将审计消息同样存入kafka中，最后的web系统是直接消费kafka中的审计消息进行内容呈现。web系统部分呈现如下： Kafka Monitorgithub地址：https://github.com/linkedin/kafka-monitorKafka Monitor是基于在消息payload内嵌index和timestamp来实现审计：消息丢失，消息重复以及端到端延迟等。web系统部分呈现如下： 几种典型的metrics解释： name description produce-avaliablility-avg The average produce availability consume-avaliability-avg The average consume availability records-produced-total The total number of records that are produced records-consumed-total The total number of records that are consumed records-lost-total The total number of records that are lost records-duplicated-total The total number of records that are duplicated records-delay-ms-avg The average latency of records from producer to consumer records-produced-rate The average number of records per second that are produced produce-error-rate The average number of errors per second consume-error-rate The average number of errors per second records-delay-ms-99th The 99th percentile latency of records from producer to consu records-delay-ms-999th The 999th percentile latency of records from producer to consumer records-delay-ms-max The maximum latency of records from producer to consumer","tags":[{"name":"kafka","slug":"kafka","permalink":"ly2513.github.com/tags/kafka/"}]},{"title":"每天掌握一个Linux命令(61) wget命令","date":"2018-01-28T14:42:33.000Z","path":"2018/01/28/每天掌握一个Linux命令-61-wget命令/","text":"Linux系统中的wget是一个下载文件的工具，它用在命令行下。对于Linux用户是必不可少的工具，我们经常要下载一些软件或从远程服务器恢复备份到本地服务器。wget支持HTTP，HTTPS和FTP协议，可以使用HTTP代理。所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。wget可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。wget非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。 1．命令格式： wget [参数] [URL地址] 2．命令功能： 用于从网络上下载资源，没有指定目录，下载资源回默认为当前目录。wget虽然功能强大，但是使用起来还是比较简单：1）支持断点下传功能；这一点，也是网络蚂蚁和FlashGet当年最大的卖点，现在，Wget也可以使用此功能，那些网络不是太好的用户可以放心了；2）同时支持FTP和HTTP下载方式；尽管现在大部分软件可以使用HTTP方式下载，但是，有些时候，仍然需要使用FTP方式下载软件；3）支持代理服务器；对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能；4）设置方便简单；可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标；5）程序小，完全免费；程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的。 3．命令参数： 启动参数：-V, –version 显示wget的版本后退出-h, –help 打印语法帮助-b, –background 启动后转入后台执行-e, –execute=COMMAND 执行`.wgetrc’格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc 记录和输入文件参数：-o, –output-file=FILE 把记录写到FILE文件中-a, –append-output=FILE 把记录追加到FILE文件中-d, –debug 打印调试输出-q, –quiet 安静模式(没有输出)-v, –verbose 冗长模式(这是缺省设置)-nv, –non-verbose 关掉冗长模式，但不是安静模式-i, –input-file=FILE 下载在FILE文件中出现的URLs-F, –force-html 把输入文件当作HTML格式文件对待-B, –base=URL 将URL作为在-F -i参数指定的文件中出现的相对链接的前缀–sslcertfile=FILE 可选客户端证书–sslcertkey=KEYFILE 可选客户端证书的KEYFILE–egd-file=FILE 指定EGD socket的文件名 下载参数：–bind-address=ADDRESS 指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)-t, –tries=NUMBER 设定最大尝试链接次数(0 表示无限制).-O –output-document=FILE 把文档写到FILE文件中-nc, –no-clobber 不要覆盖存在的文件或使用.#前缀-c, –continue 接着下载没下载完的文件–progress=TYPE 设定进程条标记-N, –timestamping 不要重新下载文件除非比本地文件新-S, –server-response 打印服务器的回应–spider 不下载任何东西-T, –timeout=SECONDS 设定响应超时的秒数-w, –wait=SECONDS 两次尝试之间间隔SECONDS秒–waitretry=SECONDS 在重新链接之间等待1…SECONDS秒–random-wait 在下载之间等待0…2*WAIT秒-Y, –proxy=on/off 打开或关闭代理-Q, –quota=NUMBER 设置下载的容量限制–limit-rate=RATE 限定下载输率 目录参数：-nd –no-directories 不创建目录-x, –force-directories 强制创建目录-nH, –no-host-directories 不创建主机目录-P, –directory-prefix=PREFIX 将文件保存到目录 PREFIX/…–cut-dirs=NUMBER 忽略 NUMBER层远程目录 HTTP 选项参数：–http-user=USER 设定HTTP用户名为 USER.–http-passwd=PASS 设定http密码为 PASS-C, –cache=on/off 允许/不允许服务器端的数据缓存 (一般情况下允许)-E, –html-extension 将所有text/html文档以.html扩展名保存–ignore-length 忽略 Content-Length头域–header=STRING 在headers中插入字符串 STRING–proxy-user=USER 设定代理的用户名为 USER–proxy-passwd=PASS 设定代理的密码为 PASS–referer=URL 在HTTP请求中包含 `Referer: URL’头-s, –save-headers 保存HTTP头到文件-U, –user-agent=AGENT 设定代理的名称为 AGENT而不是 Wget/VERSION–no-http-keep-alive 关闭 HTTP活动链接 (永远链接)–cookies=off 不使用 cookies–load-cookies=FILE 在开始会话前从文件 FILE中加载cookie–save-cookies=FILE 在会话结束后将 cookies保存到 FILE文件中 FTP 选项参数：-nr, –dont-remove-listing 不移走 `.listing’文件-g, –glob=on/off 打开或关闭文件名的 globbing机制–passive-ftp 使用被动传输模式 (缺省值).–active-ftp 使用主动传输模式–retr-symlinks 在递归的时候，将链接指向文件(而不是目录) 递归下载参数：-r, –recursive 递归下载－－慎用!-l, –level=NUMBER 最大递归深度 (inf 或 0 代表无穷)–delete-after 在现在完毕后局部删除文件-k, –convert-links 转换非相对链接为相对链接-K, –backup-converted 在转换文件X之前，将之备份为 X.orig-m, –mirror 等价于 -r -N -l inf -nr-p, –page-requisites 下载显示HTML文件的所有图片 递归下载中的包含和不包含(accept/reject)：-A, –accept=LIST 分号分隔的被接受扩展名的列表-R, –reject=LIST 分号分隔的不被接受的扩展名的列表-D, –domains=LIST 分号分隔的被接受域的列表–exclude-domains=LIST 分号分隔的不被接受的域的列表–follow-ftp 跟踪HTML文档中的FTP链接–follow-tags=LIST 分号分隔的被跟踪的HTML标签的列表-G, –ignore-tags=LIST 分号分隔的被忽略的HTML标签的列表-H, –span-hosts 当递归时转到外部主机-L, –relative 仅仅跟踪相对链接-I, –include-directories=LIST 允许目录的列表-X, –exclude-directories=LIST 不被包含目录的列表-np, –no-parent 不要追溯到父目录wget -S –spider url 不下载只显示过程 4．使用实例： 实例一：使用wget下载单个文件命令：wget http://www.minjieren.com/wordpress-3.1-zh_CN.zip说明：以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。 实例二：使用wget -O下载并以不同的文件名保存命令：wget -O wordpress.zip http://www.minjieren.com/download.aspx?id=1080说明：wget默认会以最后一个符合”/”的后面的字符来命令，对于动态链接的下载通常文件名会不正确。错误：下面的例子会下载一个文件并以名称download.aspx?id=1080保存wget http://www.minjieren.com/download?id=1即使下载的文件是zip格式，它仍然以download.php?id=1080命令。正确：为了解决这个问题，我们可以使用参数-O来指定一个文件名：wget -O wordpress.zip http://www.minjieren.com/download.aspx?id=1080 实例三：使用wget –limit -rate限速下载命令：wget –limit-rate=300k http://www.minjieren.com/wordpress-3.1-zh_CN.zip说明：当你执行wget的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。 实例四：使用wget -c断点续传命令：wget -c http://www.minjieren.com/wordpress-3.1-zh_CN.zip说明：使用wget -c重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c参数。 实例五：使用wget -b后台下载命令：wget -b http://www.minjieren.com/wordpress-3.1-zh_CN.zip说明：对于下载非常大的文件的时候，我们可以使用参数-b进行后台下载。wget -b http://www.minjieren.com/wordpress-3.1-zh_CN.zipContinuing in background, pid 1840.Output will be written to `wget-log’.你可以使用以下命令来察看下载进度：tail -f wget-log 实例六：伪装代理名称下载命令：wget –user-agent=”Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16” http://www.minjieren.com/wordpress-3.1-zh_CN.zip说明：有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过–user-agent参数伪装。 实例七：使用wget –spider测试下载链接命令：wget –spider URL说明：当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加–spider参数进行检查。wget –spider URL如果下载链接正确，将会显示wget –spider URLSpider mode enabled. Check if remote file exists.HTTP request sent, awaiting response… 200 OKLength: unspecified [text/html]Remote file exists and could contain further links,but recursion is disabled – not retrieving.这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误wget –spider urlSpider mode enabled. Check if remote file exists.HTTP request sent, awaiting response… 404 Not FoundRemote file does not exist – broken link!!!你可以在以下几种情况下使用spider参数：定时下载之前进行检查间隔检测网站是否可用检查网站页面的死链接 实例八：使用wget –tries增加重试次数命令：wget –tries=40 URL说明：如果网络有问题或下载一个大文件也有可能失败。wget默认重试20次连接下载文件。如果需要，你可以使用–tries增加重试次数。 实例九：使用wget -i下载多个文件命令：wget -i filelist.txt说明：首先，保存一份下载链接文件cat &gt; filelist.txturl1url2url3url4接着使用这个文件和参数-i下载 实例十：使用wget –mirror镜像网站命令：wget –mirror -p –convert-links -P ./LOCAL URL说明：下载整个网站到本地。–miror:开户镜像下载-p:下载所有为了html页面显示正常的文件–convert-links:下载后，转换成本地的链接-P ./LOCAL：保存所有文件和目录到本地指定目录 实例十一：使用wget –reject过滤指定格式下载命令：wget –reject=gif ur说明：下载一个网站，但你不希望下载图片，可以使用以下命令。 实例十二：使用wget -o把下载信息存入日志文件命令：wget -o download.log URL说明：不希望下载信息直接显示在终端而是在一个日志文件，可以使用 实例十三：使用wget -Q限制总下载文件大小命令：wget -Q5m -i filelist.txt说明：当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。 实例十四：使用wget -r -A下载指定格式文件命令：wget -r -A.pdf url说明：可以在以下情况使用该功能：下载一个网站的所有图片下载一个网站的所有视频下载一个网站的所有PDF文件 实例十五：使用wget FTP下载命令：wget ftp-urlwget –ftp-user=USERNAME –ftp-password=PASSWORD url说明：可以使用wget来完成ftp链接的下载。使用wget匿名ftp下载：wget ftp-url使用wget用户名和密码认证的ftp下载wget –ftp-user=USERNAME –ftp-password=PASSWORD url备注：编译安装使用如下命令编译安装：12345# tar zxvf wget-1.9.1.tar.gz# cd wget-1.9.1# ./configure# make# make install","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"wget","slug":"wget","permalink":"ly2513.github.com/tags/wget/"}]},{"title":"怎么实现单点登录","date":"2018-01-23T08:43:24.000Z","path":"2018/01/23/怎么实现单点登录/","text":"一、单点登录简介假设一个场景：公司内部有财务、OA、订单服务等各类相互独立的应用系统，员工张三对这些系统有操作权限，如果张三想要登录某个系统进行业务操作，那么他需要输入相应的账号与密码。想象一下，当公司内部有 100 个应用系统，张三是不是要输入 100 次用户名和密码进行登录，然后分别才能进行业务操作呢？显然这是很不好的体验，因此我们需要引入一个这样的机制：张三只要输入一次用户名和密码登录，成功登录后，他就可以访问财务系统、OA 系统、订单服务等系统。这就是单点登录。 单点登录的英文全称是Single Sign On，简称是 SSO。它的意思是说用户只需要登录一次，就可以在个人权限范围内，访问所有相互信任应用的功能模块，不管整个应用群的内部有多么复杂，对用户而言，都是一个统一的整体。用户访问 Web 系统的整个应用群与访问单个系统一样，登录和注销分别只要一次就够了。举个简单的例子，你登录了百度网页之后，点击跳转到百度贴吧，这时可以发现你已经自动登录了百度贴吧。 二、我们的技术实现 SSO的技术实现要想做好并不容易，我们认为需求优先级应该先是单点登录和单点注销功能，然后是应用接入的门槛，最后是数据安全性，安全性对于 SSO 也非常重要。SSO 的核心是认证中心，但要实现用户一次登录，到处访问的效果，技术实现需要建立在用户系统、认证中心、权限系统、企业门户的基础上，各职责如下：用户系统：负责用户名、密码等帐户信息管理，包括增加、修改、启用、停用用户帐号，同时为认证中心提供对用户名和密码的校验。认证中心：负责凭证 token 的生成、加密、颁发、验证、销毁、登入 Login、登出 Logout。用户只有拥有凭证并验证通过才能访问企业门户。权限系统：负责角色管理、资源设置、授权设置、鉴定权限，具体实现可参考 RBAC。权限系统可为企业门户提供用户权限范围内的导航。企业门户：作为应用系统的集成门户 (Portal)，集成了多个应用系统的功能，为用户提供链接导航、用户信息和登出功能等。 2.1、服务端功能实现登录认证：接收登录帐号信息，让用户系统验证用户的登录信息。凭证生成：创建授权凭证 token，生成的凭证一般包含用户帐号信息、过期时间等信息，它是一串加密的字符串，加密算法如 AES｛凭证明文 +MD5 加信息｝，可采用 JWT 标准。凭证颁发：与 SSO 客户端通信，发送凭证给 SSO 客户端。凭证验证：接收并校验来自 SSO 客户端的凭证有效性，凭证验证包括算法验证和数据验证。凭证销毁与登出：接收来自 SSO 客户端的登出请求，记录并销毁凭证，跳转至登录页面。 2.2、客户端功能实现1、请求拦截：拦截应用未登录请求，跳转至登录页面。2、获取凭证：接收并存储由 SSO 服务端发来的凭证，凭证存储的方式有 Cookie、Session、网址传参、Header 等。3、提交凭证验证：与 SSO 服务端通信，发出校验凭证有效性的请求。4、获取用户权限：获取该凭证的用户权限，并返回受保护资源给用户。5、凭证销毁与登出：销毁本地会话，然后跳转至登出页面。 2.3、用户单点登录流程 登录：将用户输入的用户名和密码发送至认证中心，然后认证中心调用用户系统来验证登录信息。生成并颁发凭证：通过登录信息的验证后，认证中心创建授权凭证 token，然后把这个授权凭证 token 返回给 SSO 客户端。SSO 客户端拿到这个 token，进行存储。在后续请求中，在 HTTP 请求数据中都得加上这个 token。凭证验证：SSO 客户端发送凭证 token 给认证中心，认证中心校验这个 token 的有效性。凭证验证有算法验证和数据验证，算法验证可在 SSO 客户端完成。 2.4、用户访问流程和单点注销 以上是用户的访问流程，如果用户没有有效的凭证，认证中心将强制用户进入登录流程。对于单点注销，用户如果注销了应用群内的其中一个应用，那么全局 token 也会被销毁，应用群内的所有应用将不能再被访问。 2.5、具体接入与集成 我们的应用接入与集成具体如下：1、用户系统：接入国内机票平台的用户系统，负责登录认证。2、权限系统：接入国内机票平台的权限系统。3、认证中心：负责生成并颁发凭证、销毁凭证，改造国内机票平台的登入、登出。4、凭证验证：在国内机票、国际机票应用系统中调用 SSO 客户端组件实现凭证的验证。5、企业门户：由国内机票平台、国际机票平台承担。 三、JWT 标准 JSON Web Token (JWT) 是目前应用最为广泛的 token 格式，是为了在网络应用环境间传递声明而执行的一种基于 JSON 的开放标准（RFC 7519)。该 token 设计紧凑且安全，特别适用于分布式站点的单点登录、API 网关等场景。JWT 的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息。该 token 也可直接被用于认证，也可被加密。JWT 信息体由 3 部分构成：头 Header+ 载荷 Payload+ 签名 Signature，具体优点如下：JWT 支持多种语言，C#、Java、JavaScript、Node.js、PHP 等很多语言都可以使用。JWT 可以自身存储一些和业务逻辑有关的所必要的非敏感信息，因为有了 Payload 部分。利于传输，因为 JWT 的构成非常简单，字节占用很小。不需要在服务端保存会话信息，不仅省去服务端资源开销，而且使得应用易于扩展。","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"架构","slug":"架构","permalink":"ly2513.github.com/tags/架构/"}]},{"title":"权限系统的设计","date":"2018-01-23T08:18:12.000Z","path":"2018/01/23/权限系统的设计/","text":"一、权限的本质权限管理，首先要理清权限的本质：权限就是对受保护资源的有限许可访问。理解了权限的本质，就好谈权限的管理了。权限就是对受保护资源的有限许可访问–这句话包含两层含义： 受保护的资源 有限的许可访问但，本质上谈的都是对资源的访问，所以探讨权限问题，首先要定义资源。 二、资源的概念资源是一个抽象的概念，按照百科的词条解释，“资源”是指一国或一定地区内拥有的物力、财力、人力等各种物质要素的总称。分为自然资源和社会资源两大类。前者如阳光、空气、水、土地、森林、草原、动物、矿藏等；后者包括人力资源、信息资源以及经过劳动创造的各种物质财富等。在计算机软件中，资源指的是软件使用过程中使用的各种对象，功能，文件，网络等各种要素的总称。比如一个按钮所操作功能也算是一种资源；这样，菜单，按钮，页面等等，都可以算是资源。甚至，数据库的某个字段，也是资源。如果按照RESTFull API的思路去理解资源可能更好理解。 2.1 资源的识别资源多了，管理起来也有点麻烦，比如一个软件有很多按钮，要搞清楚这些按钮是比较困难的。把资源硬编码一个代号，对资源进行命名，对一类资源进行组织归类，这样复杂的系统就好管理了。 2.2 有限的资源资源有很多，但并不是所有资源都是我们在当前领域需要关心的，我们要从所有资源中，整理出那些是受保护的有限资源。两个定语：受保护，有限。公开的且无限的资源，不需要保护了（当然不是完全不需要），比如阳光，比如以前的空气。但是，随着工业化的发展，空气也越来越需要保护了。水资源，虽说是公开的，但是它是有限的，所以需要严格保护。野生动物，森林，都是需要保护的，它们的数量都是有限的。由于人类活动范围的持续增大，几乎地球上所有的资源都不够了，总有一天，阳光也会成为稀缺的资源。所以，资源总是给人一种有限的感觉。正因为资源是有限的特点，我们不能随意并且大量的使用，所以需要对这些资源进行保护，要访问（使用）资源，需要授权。所以，权限就是对受保护资源的有限许可访问。 三、权限的概念 3.1 权限的分类权限并不是一个独立的东西，它不是主体，是客体，所以它必须依附于一个主体。所以，我们常常根据权限所依附的主体来给权限分类：按照授权方式区分的权限类型：部门权限，人员权限，角色权限。按照软件层级区分：功能权限，业务权限，数据权限。功能权限是指可访问的页面、菜单、按钮等，这些功能一般都在软件的“视图层”；业务权限是指可以处理的一类业务，通常包含很多功能，比如一个功能，A业务可以修改，但是B业务只能查询；数据权限是指哪些数据可以被什么对象访问。通常在数据库级别进行控制，比如：同样一个表，员工只能查询，经理可以修改。 3.2 权限的控制模型 权限，实质上分为权限的受体和权限的配体，权限的受体在资源对象上，而权限的配体是权限访问者持有的访问秘钥，可以用细胞分子来理解受体和配体。只有当受体和配体有效结合以后，才代表权限执行成功，被访问的资源的状态发生了改变。可以用锁来举例理解，要开锁必须有钥匙，钥匙是权限的“配体”，而锁是权限的受体。锁接受了钥匙成功进行了配对，打开了锁，于是人打开了门，进入了房间，那么房间这个资源的状态就改变了。另外，权限还要区分拥有者和执行者，访问受保护资源的访问者，只要持有代表行使权限的访问秘钥，那么访问者就可以访问这个资源，也就说这个访问者有了访问这个资源的权限。这里有三个关键词：访问者，资源，秘钥访问者就是是权限的执行者，它可以是权限的拥有者，也可以是权限拥有者授权的代理人，就像公司的董事长和总经理的关系，董事长授权总经理经营管理公司，总经理代表董事长行使公司资产使用的权利。但是不管谁来行使访问权限，都要持有访问资源的秘钥，比如银行账户的密码。其实这个过程，做过微服务权限控制，都能明白。权限系统，跟国家的权力架构一样，也分为制定权限资源（立法），执行权限访问（司法），授予权限（行政）。组成权限系统的3个部分是相互分立，相互制约的。把权限系统的原理搞清楚了，那么设计权限系统的程序，就很简单了，能够做到完备且灵活。 3.3 权限的授权 权限系统里面最常出现的就是角色，这叫做角色授权，当系统权限多了，势必要对权限进行一个分组（分类），这就是角色。所以，角色，仅仅是一个权限集合而已，因此授权的时候按角色授权要方便些。但实际上，也可以按部门授权，或者按个人授权，或者，3者交叉授权。所以，最终一个系统的权限管理，复杂就复杂在这个地方，而如果权限授权不清晰，也容易出现推诿，扯皮的事情。所以，管理者授权，是很考验管理智慧的地方。 总结权限系统分为三大部分： 系统使用的资源（菜单，按钮，页面，数据等等有限的可操作可访问的对象）； 权限的识别，对资源系统中找出那些是需要进行受保护访问的而不是公开可操作的资源，对这样的资源进行分组和命名； 权限的授权（按角色授权，按部门授权，按个人授权等）。","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"}]},{"title":"每天掌握一个Linux命令(60) scp命令","date":"2018-01-20T15:04:27.000Z","path":"2018/01/20/每天掌握一个Linux命令-60-scp命令/","text":"scp是secure copy的简写，用于在Linux下进行远程拷贝文件的命令，和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。当你服务器硬盘变为只读 read only system时，用scp可以帮你把文件移出来。另外，scp还非常不占资源，不会提高多少系统负荷，在这一点上，rsync就远远不及它了。虽然 rsync比scp会快一点，但当小文件众多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。 1．命令格式： scp [参数] [原路径] [目标路径] 2．命令功能： scp是 secure copy的缩写, scp是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。linux的scp命令可以在linux服务器之间复制文件和目录。 3．命令参数： -1 强制scp命令使用协议ssh1-2 强制scp命令使用协议ssh2-4 强制scp命令只使用IPv4寻址-6 强制scp命令只使用IPv6寻址-B 使用批处理模式（传输过程中不询问传输口令或短语）-C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）-p 保留原文件的修改时间，访问时间和访问权限。-q 不显示传输进度条。-r 递归复制整个目录。-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。-c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。-F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。-i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。-l limit 限定用户所能使用的带宽，以Kbit/s为单位。-o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式，-P port 注意是大写的P, port是指定数据传输用到的端口号-S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。 4．使用实例： scp命令的实际应用概述： 从本地服务器复制到远程服务器： 实例一：复制文件：命令格式：scp local_file remote_username@remote_ip:remote_folder或者 scp local_file remote_username@remote_ip:remote_file或者 scp local_file remote_ip:remote_folder或者 scp local_file remote_ip:remote_file第1,2个指定了用户名，命令执行后需要输入用户密码，第1个仅指定了远程的目录，文件名字不变，第2个指定了文件名第3,4个没有指定用户名，命令执行后需要输入用户名和密码，第3个仅指定了远程的目录，文件名字不变，第4个指定了文件名 实例二：复制目录：命令格式：scp -r local_folder remote_username@remote_ip:remote_folder或者 scp -r local_folder remote_ip:remote_folder第1个指定了用户名，命令执行后需要输入用户密码；第2个没有指定用户名，命令执行后需要输入用户名和密码；从远程服务器复制到本地服务器：从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。 实例三：从远处复制文件到本地目录命令：scp root@192.168.120.204:/opt/soft/nginx-0.5.38.tar.gz /opt/soft/ 输出： 说明：从192.168.120.204机器上的/opt/soft/的目录中下载nginx-0.5.38.tar.gz 文件到本地/opt/soft/目录中 实例四：从远处复制到本地命令：scp -r root@192.168.120.204:/opt/soft/mongodb /opt/soft/输出： 说明：从192.168.120.204机器上的/opt/soft/中下载mongodb 目录到本地的/opt/soft/目录来。 实例五：上传本地文件到远程机器指定目录命令：scp /opt/soft/nginx-0.5.38.tar.gz root@192.168.120.204:/opt/soft/scptest输出：上传前目标机器的目标目录：1234567891011121314[root@localhost soft]# cd scptest/[root@localhost scptest]# ll总计 0[root@localhost scptest]# ll# 本地机器上传：[root@localhost soft]# scp /opt/soft/nginx-0.5.38.tar.gz root@192.168.120.204:/opt/soft/scptestroot@192.168.120.204&apos;s password:nginx-0.5.38.tar.gz 100% 479KB 478.7KB/s 00:00[root@localhost soft]## 上传后目标机器的目标目录：[root@localhost scptest]# ll总计 484-rw-r--r-- 1 root root 490220 03-15 09:25 nginx-0.5.38.tar.gz[root@localhost scptest]# 说明：复制本地opt/soft/目录下的文件nginx-0.5.38.tar.gz 到远程机器192.168.120.204的opt/soft/scptest目录 实例六：上传本地目录到远程机器指定目录命令：scp -r /opt/soft/mongodb root@192.168.120.204:/opt/soft/scptest输出：说明：上传本地目录 /opt/soft/mongodb到远程机器192.168.120.204上/opt/soft/scptest的目录中去","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"scp","slug":"scp","permalink":"ly2513.github.com/tags/scp/"}]},{"title":"每天掌握一个Linux命令(59) rcp命令","date":"2018-01-18T14:09:08.000Z","path":"2018/01/18/每天掌握一个Linux命令-59-rcp命令/","text":"rcp代表“remote file copy”（远程文件拷贝）。该命令用于在计算机之间拷贝文件。rcp命令有两种格式。第一种格式用于文件到文件的拷贝；第二种格式用于把文件或目录拷贝到另一个目录中。 1．命令格式： rcp [参数] [源文件] [目标文件] 2．命令功能： rcp命令用在远端复制文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则它会把前面指定的所有文件或目录复制到该目录中。 3．命令参数： 各选项含义：-r 递归地把源目录中的所有内容拷贝到目的目录中。要使用这个选项，目的必须是一个目录。-p 试图保留源文件的修改时间和模式，忽略umask。-k 请求rcp获得在指定区域内的远程主机的Kerberos 许可，而不是获得由krb_relmofhost⑶确定的远程主机区域内的远程主机的Kerberos许可。-x 为传送的所有数据打开DES加密。这会影响响应时间和CPU利用率，但是可以提高安全性。如果在文件名中指定的路径不是完整的路径名，那么这个路径被解释为相对远程机上同名用户的主目录。如果没有给出远程用户名，就使用当前用户名。如果远程机上的路径包含特殊shell字符，需要用反斜线（\\）、双引号（”）或单引号（’）括起来，使所有的shell元字符都能被远程地解释。需要说明的是，rcp不提示输入口令，它通过rsh命令来执行拷贝。directory 每个文件或目录参数既可以是远程文件名也可以是本地文件名。远程文件名具有如下形式：rname@rhost：path，其中rname是远程用户名，rhost是远程计算机名，path是这个文件的路径。 4．使用实例： 要使用 rcp，需要具备以下条件：如果系统中有/etc/hosts文件，系统管理员应确保该文件包含要与之进行通信的远程主机的项。/etc/hosts文件中有一行文字，其中包含每个远程系统的以下信息：internet_address official_name alias例如：9.186.10.* webserver1.com.58.webserver.rhosts 文件.rhosts 文件位于远程系统的主目录下，其中包含本地系统的名称和本地登录名。例如，远程系统的 .rhosts 文件中的项可能是：webserver1 root其中，webserver1 是本地系统的名称，root 是本地登录名。这样，webserver1 上的 root 即可在包含 .rhosts 文件的远程系统中来回复制文件。配置过程:只对root用户生效1.在双方root用户根目录下建立.rhosts文件,并将双方的hostname加进去.在此之前应在双方的 /etc/hosts文件中加入对方的IP和hostname 2.把rsh服务启动起来,redhat默认是不启动的。 方法：用执行ntsysv命令,在rsh选项前用空格键选中,确定退出。然后执行： service xinetd restart即可。 3.到/etc/pam.d/目录下,把rsh文件中的auth required /lib/security/pam_securetty.so 一行用“#”注释掉即可。（只有注释掉这一行，才能用root用户登录）命令使用:将文件复制到远程系统,要将本地系统中的文件复制到远程系统，请使用以下命令：rcplocal_fileremote_hostname:remote_fileEnter注意，如果当前目录下没有 local_file，则除本地文件名外，还需要提供相对路径（自当前目录开始）或绝对路径名（自 / 开始）。仅当希望将 remote_hostname 上的 remote_file 放到其他目录（远程主目录除外）下时，才需要为其指定完整的（绝对）路径。 实例一：将当前目录下的 test1 复制到名为 webserver1的远程系统命令：rcp test1 webserver1:/home/root/test3说明：在这种情况下，test1 被复制到远程子目录 test3下，名称仍为 test1 。如果仅提供了远程主机名，rcp 将把 test1 复制到远程主目录下，名称仍为 test1 。还可以在目的目录中包含文件名。例如，将文件复制到名为 webserver1的系统中：rcp test1 webserver1:/home/root/test3在这种情况下，将 test1 复制到远程目录root 下并将其命名为 test3。 实例二：从远程系统复制文件：要将远程系统中的文件复制到本地目录下命令：rcp remote_hostname:remote_file local_fileEnter 实例三：将远程系统 webserver1中的 test2 复制到当前目录：命令：rcp webserver1:/home/root/test2 .Enter说明：点 (.) 是“当前目录”的简写形式。在这种情况下，远程目录中的 test2 被复制到当前目录下，名称仍为 test2 。如果希望用新名称复制文件，请提供目标文件名。如果希望将 test2 复制到本地系统中的其他目录下，请使用以下绝对或相对路径名：rcp webserver1:/home/root/test2 otherdir/ Enter或者，如果希望用其他文件名将文件复制到其他目录下：rcp webserver1:/home/root/test2 otherdir/otherfile Enter 实例四：将目录复制到远程系统：要将本地目录及其文件和子目录复制到远程系统，请同时使用 rcp 和 -r（递归）选项。命令：rcp –r local_dir remote_hostname:remote_dir Enter 说明：如果当前目录下没有 local_dir，则除本地目录名外，还需要提供相对路径名（自当前目录开始）或绝对路径名（自 / 顶级目录开始）。另外，如果主目录下没有 remote_dir，则 remote_dir 将需要一个相对路径（自主目录开始）或绝对路径（自 / 开始）。 实例五：要将名为 work 的子目录完整地复制到 webserver1远程计算机中的主目录下名为 products 的目录，请键入以下内容：rcp –r work webserver1:/home/root/products Enter此命令在 webserver1:/home/root/products 下创建名为 work 的目录及其全部内容（假定 /home/root/products 已存在于 webserver1中）。本示例假定用户处于包含 work 的本地目录下。否则，必须提供该目录的相对或绝对路径，如 /home/root/work。 实例六：从远程系统复制目录：要将远程目录及其所有文件和子目录复制到本地目录，请在以下语法中使用 rcp 和 -r（递归）选项。命令：rcp –r remote_hostname:remote_dir local_dir Enter要将名为 work 的远程目录复制到当前目录，请键入以下内容：rcp –r webserver1:/home/root/work .Enter点 (.) 表示当前目录。将在此目录下创建 work 目录。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"rcp","slug":"rcp","permalink":"ly2513.github.com/tags/rcp/"}]},{"title":"每天掌握一个Linux命令(58) telnet命令","date":"2018-01-17T13:49:25.000Z","path":"2018/01/17/每天掌握一个Linux命令-58-telnet命令/","text":"telnet命令通常用来远程登录。telnet程序是基于TELNET协议的远程登录客户端程序。Telnet协议是TCP/IP协议族中的一员，是Internet远程登陆服务的标准协议和主要方式。它为用户提供了在本地计算机上完成远程主机工作的 能力。在终端使用者的电脑上使用telnet程序，用它连接到服务器。终端使用者可以在telnet程序中输入命令，这些命令会在服务器上运行，就像直接在服务器的控制台上输入一样。可以在本地就能控制服务器。要开始一个telnet会话，必须输入用户名和密码来登录服务器。Telnet是常用的远程控制Web服务器的方法。但是，telnet因为采用明文传送报文，安全性不好，很多Linux服务器都不开放telnet服务，而改用更安全的ssh方式了。但仍然有很多别的系统可能采用了telnet方式来提供远程登录，因此弄清楚telnet客户端的使用方式仍是很有必要的。telnet命令还可做别的用途，比如确定远程服务的状态，比如确定远程服务器的某个端口是否能访问。 1．命令格式： telnet[参数][主机] 2．命令功能： 执行telnet指令开启终端机阶段作业，并登入远端主机。 3．命令参数： -8 允许使用8位字符资料，包括输入与输出。-a 尝试自动登入远端系统。-b&lt;主机别名&gt; 使用别名指定远端主机名称。-c 不读取用户专属目录里的.telnetrc文件。-d 启动排错模式。-e&lt;脱离字符&gt; 设置脱离字符。-E 滤除脱离字符。-f 此参数的效果和指定”-F”参数相同。-F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。-k&lt;域名&gt; 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。-K 不自动登入远端主机。-l&lt;用户名称&gt; 指定要登入远端主机的用户名称。-L 允许输出8位字符资料。-n&lt;记录文件&gt; 指定文件记录相关信息。-r 使用类似rlogin指令的用户界面。-S&lt;服务类型&gt; 设置telnet连线所需的IP TOS信息。-x 假设主机有支持数据加密的功能，就使用它。-X&lt;认证形态&gt; 关闭指定的认证形态。 4．使用实例： 实例一：远程服务器无法访问命令：telnet 192.168.120.206输出：12345[root@localhost ~]# telnet 192.168.120.209Trying 192.168.120.209...telnet: connect to address 192.168.120.209: No route to hosttelnet: Unable to connect to remote host: No route to host[root@localhost ~]# 说明：处理这种情况方法：（1）确认ip地址是否正确？（2）确认ip地址对应的主机是否已经开机？（3）如果主机已经启动，确认路由设置是否设置正确？（使用route命令查看）（4）如果主机已经启动，确认主机上是否开启了telnet服务？（使用netstat命令查看，TCP的23端口是否有LISTEN状态的行）（5）如果主机已经启动telnet服务，确认防火墙是否放开了23端口的访问？（使用iptables-save查看） 实例二：域名无法解析命令：telnet www.baidu.com输出：123[root@localhost ~]# telnet www.baidu.comwww.baidu.com/telnet: Temporary failure in name resolution[root@localhost ~]# 说明：处理这种情况方法：（1）确认域名是否正确（2）确认本机的域名解析有关的设置是否正确（/etc/resolv.conf中nameserver的设置是否正确，如果没有，可以使用nameserver 8.8.8.8）（3）确认防火墙是否放开了UDP53端口的访问（DNS使用UDP协议，端口53，使用iptables-save查看） 实例三：命令：输出：12345[root@localhost ~]# telnet 192.168.120.206Trying 192.168.120.206...telnet: connect to address 192.168.120.206: Connection refusedtelnet: Unable to connect to remote host: Connection refused[root@localhost ~]# 说明：处理这种情况：（1）确认ip地址或者主机名是否正确？（2）确认端口是否正确，是否默认的23端口 实例四：启动telnet服务命令：service xinetd restart输出：1234567891011121314151617181920212223242526272829303132333435363738[root@localhost ~]# cd /etc/xinetd.d/[root@localhost xinetd.d]# ll总计 124-rw-r--r-- 1 root root 1157 2011-05-31 chargen-dgram-rw-r--r-- 1 root root 1159 2011-05-31 chargen-stream-rw-r--r-- 1 root root 523 2009-09-04 cvs-rw-r--r-- 1 root root 1157 2011-05-31 daytime-dgram-rw-r--r-- 1 root root 1159 2011-05-31 daytime-stream-rw-r--r-- 1 root root 1157 2011-05-31 discard-dgram-rw-r--r-- 1 root root 1159 2011-05-31 discard-stream-rw-r--r-- 1 root root 1148 2011-05-31 echo-dgram-rw-r--r-- 1 root root 1150 2011-05-31 echo-stream-rw-r--r-- 1 root root 323 2004-09-09 eklogin-rw-r--r-- 1 root root 347 2005-09-06 ekrb5-telnet-rw-r--r-- 1 root root 326 2004-09-09 gssftp-rw-r--r-- 1 root root 310 2004-09-09 klogin-rw-r--r-- 1 root root 323 2004-09-09 krb5-telnet-rw-r--r-- 1 root root 308 2004-09-09 kshell-rw-r--r-- 1 root root 317 2004-09-09 rsync-rw-r--r-- 1 root root 1212 2011-05-31 tcpmux-server-rw-r--r-- 1 root root 1149 2011-05-31 time-dgram-rw-r--r-- 1 root root 1150 2011-05-31 time-stream[root@localhost xinetd.d]# cat krb5-telnet# default: off# description: The kerberized telnet server accepts normal telnet sessions, \\# but can also use Kerberos 5 authentication.service telnet&#123; flags = REUSE socket_type = stream wait = no user = root server = /usr/kerberos/sbin/telnetd log_on_failure += USERID disable = yes&#125;[root@localhost xinetd.d]# 说明：配置参数，通常的配置如下：12345678910111213141516171819service telnet&#123;disable = no #启用flags = REUSE #socket可重用socket_type = stream #连接方式为TCPwait = no #为每个请求启动一个进程user = root #启动服务的用户为rootserver = /usr/sbin/in.telnetd #要激活的进程log_on_failure += USERID #登录失败时记录登录用户名&#125; 如果要配置允许登录的客户端列表，加入1only_from = 192.168.0.2 #只允许192.168.0.2登录 如果要配置禁止登录的客户端列表，加入1no_access = 192.168.0.&#123;2,3,4&#125; #禁止192.168.0.2、192.168.0.3、192.168.0.4登录 如果要设置开放时段，加入access_times = 9:00-12:00 13:00-17:00 # 每天只有这两个时段开放服务（我们的上班时间：P）如果你有两个IP地址，一个是私网的IP地址如192.168.0.2，一个是公网的IP地址如218.75.74.83，如果你希望用户只能从私网来登录telnet服务，那么加入bind = 192.168.0.2各配置项具体的含义和语法可参考xined配置文件属性说明（man xinetd.conf）配置端口，修改services文件：1234# vi /etc/services找到以下两句telnet 23/tcptelnet 23/udp 如果前面有#字符，就去掉它。telnet的默认端口是23，这个端口也是黑客端口扫描的主要对象，因此最好将这个端口修改掉，修改的方法很简单，就是将23这个数字修改掉，改成大一点的数字，比如61123。注意，1024以下的端口号是internet保留的端口号，因此最好不要用，还应该注意不要与其它服务的端口冲突。启动服务：1service xinetd restart 实例五：正常telnet命令：telnet 192.168.120.204输出：1234567891011[root@andy ~]# telnet 192.168.120.204Trying 192.168.120.204...Connected to 192.168.120.204 (192.168.120.204).Escape character is &apos;^]&apos;. localhost (Linux release 2.6.18-274.18.1.el5 #1 SMP Thu Feb 9 12:45:44 EST 2012) (1)login: rootPassword:Login incorrect 说明：一般情况下不允许root从远程登录，可以先用普通账号登录，然后再用su -切到root用户。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"telnet","slug":"telnet","permalink":"ly2513.github.com/tags/telnet/"}]},{"title":"每天掌握一个Linux命令(57) ss命令","date":"2018-01-15T06:42:21.000Z","path":"2018/01/15/每天掌握一个Linux命令-57-ss命令/","text":"ss是Socket Statistics的缩写。顾名思义，ss命令可以用来获取socket统计信息，它可以显示和netstat类似的内容。但ss的优势在于它能够显示更多更详细的有关TCP和连接状态的信息，而且比netstat更快速更高效。当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是直接cat /proc/net/tcp，执行速度都会很慢。可能你不会有切身的感受，但请相信我，当服务器维持的连接达到上万个的时候，使用netstat等于浪费 生命，而用ss才是节省时间。天下武功唯快不破。ss快的秘诀在于，它利用到了TCP协议栈中tcp_diag。tcp_diag是一个用于分析统计的模块，可以获得Linux内核中第一手的信息，这就确保了ss的快捷高效。当然，如果你的系统中没有tcp_diag，ss也可以正常运行，只是效率会变得稍慢。（但仍然比netstat要快。） 1.命令格式: ss [参数]ss [参数] [过滤] 2.命令功能： ss(Socket Statistics的缩写)命令可以用来获取 socket统计信息，此命令输出的结果类似于netstat输出的内容，但它能显示更多更详细的TCP连接状态的信息，且比netstat更快速高效。它使用了TCP协议栈中tcp_diag（是一个用于分析统计的模块），能直接从获得第一手内核信息，这就使得ss命令快捷高效。在没有tcp_diag，ss也可以正常运行。 3.命令参数： -h, –help 帮助信息-V, –version 程序版本信息-n, –numeric 不解析服务名称-r, –resolve 解析主机名-a, –all 显示所有套接字（sockets）-l, –listening 显示监听状态的套接字（sockets）-o, –options 显示计时器信息-e, –extended 显示详细的套接字（sockets）信息-m, –memory 显示套接字（socket）的内存使用情况-p, –processes 显示使用套接字（socket）的进程-i, –info 显示 TCP内部信息-s, –summary 显示套接字（socket）使用概况-4, –ipv4 仅显示IPv4的套接字（sockets）-6, –ipv6 仅显示IPv6的套接字（sockets）-0, –packet 显示 PACKET 套接字（socket）-t, –tcp 仅显示 TCP套接字（sockets）-u, –udp 仅显示 UCP套接字（sockets）-d, –dccp 仅显示 DCCP套接字（sockets）-w, –raw 仅显示 RAW套接字（sockets）-x, –unix 仅显示 Unix套接字（sockets）-f, –family=FAMILY 显示 FAMILY类型的套接字（sockets），FAMILY可选，支持 unix, inet, inet6, link, netlink-A, –query=QUERY, –socket=QUERY QUERY := {all|inet|tcp|udp|raw|unix|packet|netlink}[,QUERY]-D, –diag=FILE 将原始TCP套接字（sockets）信息转储到文件-F, –filter=FILE 从文件中都去过滤器信息 FILTER := [ state TCP-STATE ] [ EXPRESSION ] 4.使用实例： 实例一：显示TCP连接命令：ss -t -a输出： 实例二：显示 Sockets 摘要命令：ss -s输出：说明：列出当前的established, closed, orphaned and waiting TCP sockets 实例三：列出所有打开的网络连接端口命令：ss -l输出：123456[root@localhost ~]# ss -lRecv-Q Send-Q Local Address:Port Peer Address:Port0 0 127.0.0.1:smux *:*0 0 *:3690 *:*0 0 *:ssh *:*[root@localhost ~]# 实例四：查看进程使用的socket命令：ss -pl输出：1234567[root@localhost ~]# ss -plRecv-Q Send-Q Local Address:Port Peer Address:Port0 0 127.0.0.1:smux *:* users:((&quot;snmpd&quot;,2716,8))0 0 *:3690 *:* users:((&quot;svnserve&quot;,3590,3))0 0 *:ssh *:* users:((&quot;sshd&quot;,2735,3))[root@localhost ~]# 实例五：找出打开套接字/端口应用程序命令：ss -lp | grep 3306输出：1234567[root@localhost ~]# ss -lp|grep 19350 0 *:1935 *:* users:((&quot;fmsedge&quot;,2913,18))0 0 127.0.0.1:19350 *:* users:((&quot;fmsedge&quot;,2913,17))[root@localhost ~]# ss -lp|grep 33060 0 *:3306 *:* users:((&quot;mysqld&quot;,2871,10))[root@localhost ~]# 实例六：显示所有UDP Sockets命令：ss -u -a输出：1234567[root@localhost ~]# ss -u -aState Recv-Q Send-Q Local Address:Port Peer Address:PortUNCONN 0 0 127.0.0.1:syslog *:*UNCONN 0 0 *:snmp *:*ESTAB 0 0 192.168.120.203:39641 10.58.119.119:domain[root@localhost ~]# 实例七：显示所有状态为established的SMTP连接命令：ss -o state established ‘( dport = :smtp or sport = :smtp )’输出：123[root@localhost ~]# ss -o state established &apos;( dport = :smtp or sport = :smtp )&apos;Recv-Q Send-Q Local Address:Port Peer Address:Port[root@localhost ~]# 实例八：显示所有状态为Established的HTTP连接命令：ss -o state established ‘( dport = :http or sport = :http )’输出：1234[root@localhost ~]# ss -o state established &apos;( dport = :http or sport = :http )&apos;Recv-Q Send-Q Local Address:Port Peer Address:Port0 0 75.126.153.214:2164 192.168.10.42:http[root@localhost ~]# 实例九：列举出处于 FIN-WAIT-1状态的源端口为 80或者 443，目标网络为 193.233.7/24所有 tcp套接字命令：ss -o state fin-wait-1 ‘( sport = :http or sport = :https )’ dst 193.233.7/24 实例十：用TCP 状态过滤Sockets:命令：ss -4 state FILTER-NAME-HEREss -6 state FILTER-NAME-HERE输出：123[root@localhost ~]#ss -4 state closingRecv-Q Send-Q Local Address:Port Peer Address:Port1 11094 75.126.153.214:http 192.168.10.42:4669 说明：FILTER-NAME-HERE 可以代表以下任何一个： establishedsyn-sentsyn-recvfin-wait-1fin-wait-2time-waitclosedclose-waitlast-acklistenclosingall : 所有以上状态connected : 除了listen and closed的所有状态synchronized :所有已连接的状态除了syn-sentbucket : 显示状态为maintained as minisockets,如：time-wait和syn-recv.big : 和bucket相反. 实例十一：匹配远程地址和端口号命令：ss dst ADDRESS_PATTERNss dst 192.168.1.5ss dst 192.168.119.113:httpss dst 192.168.119.113:smtpss dst 192.168.119.113:443输出： 实例十二：匹配本地地址和端口号命令：ss src ADDRESS_PATTERNss src 192.168.119.103ss src 192.168.119.103:httpss src 192.168.119.103:80ss src 192.168.119.103:smtpss src 192.168.119.103:25输出： 实例十三：将本地或者远程端口和一个数比较命令：ss dport OP PORTss sport OP PORT输出： 说明：ss dport OP PORT 远程端口和一个数比较；ss sport OP PORT 本地端口和一个数比较。OP 可以代表以下任意一个:123456&lt;= or le : 小于或等于端口号&gt;= or ge : 大于或等于端口号== or eq : 等于端口号!= or ne : 不等于端口号&lt; or gt : 小于端口号&gt; or lt : 大于端口号 实例十四：ss 和 netstat 效率对比命令：time netstat -attime ss 输出：说明：用time 命令分别获取通过netstat和ss命令获取程序和概要占用资源所使用的时间。在服务器连接数比较多的时候，netstat的效率完全没法和ss比。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"ss","slug":"ss","permalink":"ly2513.github.com/tags/ss/"}]},{"title":"每天掌握一个Linux命令(56) netstat命令","date":"2018-01-15T05:48:19.000Z","path":"2018/01/15/每天掌握一个Linux命令-56-netstat命令/","text":"netstat命令用于显示与IP、TCP、UDP和ICMP协议相关的统计数据，一般用于检验本机各端口的网络连接情况。netstat是在内核中访问网络及相关信息的程序，它能提供TCP连接，TCP和UDP监听，进程内存管理的相关报告。如果你的计算机有时候接收到的数据报导致出错数据或故障，你不必感到奇怪，TCP/IP可以容许这些类型的错误，并能够自动重发数据报。但如果累计的出错情况数目占到所接收的IP数据报相当大的百分比，或者它的数目正迅速增加，那么你就应该使用netstat查一查为什么会出现这些情况了。 1．命令格式： netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][–ip] 2．命令功能： netstat用于显示与IP、TCP、UDP和ICMP协议相关的统计数据，一般用于检验本机各端口的网络连接情况。 3．命令参数： -a或–all 显示所有连线中的Socket。-A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。-c或–continuous 持续列出网络状态。-C或–cache 显示路由器配置的快取信息。-e或–extend 显示网络其他相关信息。-F或–fib 显示FIB。-g或–groups 显示多重广播功能群组组员名单。-h或–help 在线帮助。-i或–interfaces 显示网络界面信息表单。-l或–listening 显示监控中的服务器的Socket。-M或–masquerade 显示伪装的网络连线。-n或–numeric 直接使用IP地址，而不通过域名服务器。-N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。-o或–timers 显示计时器。-p或–programs 显示正在使用Socket的程序识别码和程序名称。-r或–route 显示Routing Table。-s或–statistice 显示网络工作信息统计表。-t或–tcp 显示TCP传输协议的连线状况。-u或–udp 显示UDP传输协议的连线状况。-v或–verbose 显示指令执行过程。-V或–version 显示版本信息。-w或–raw 显示RAW传输协议的连线状况。-x或–unix 此参数的效果和指定”-A unix”参数相同。–ip或–inet 此参数的效果和指定”-A inet”参数相同。 4．使用实例： 实例一：无参数使用命令：netstat输出：1234567891011121314[root@localhost ~]# netstatActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 268 192.168.120.204:ssh 10.2.0.68:62420 ESTABLISHEDudp 0 0 192.168.120.204:4371 10.58.119.119:domain ESTABLISHEDActive UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 1491 @/org/kernel/udev/udevdunix 4 [ ] DGRAM 7337 /dev/logunix 2 [ ] DGRAM 708823unix 2 [ ] DGRAM 7539unix 3 [ ] STREAM CONNECTED 7287unix 3 [ ] STREAM CONNECTED 7286[root@localhost ~]# 说明：从整体上看，netstat的输出结果可以分为两个部分：一个是Active Internet connections，称为有源TCP连接，其中”Recv-Q”和”Send-Q”指的是接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积。这种情况只能在非常少的情况见到。另一个是Active UNIX domain sockets，称为有源Unix域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。Proto显示连接使用的协议,RefCnt表示连接到本套接口上的进程号,Types显示套接口的类型,State显示套接口当前的状态,Path表示连接到套接口的其它进程使用的路径名。套接口类型：1234567-t ：TCP-u ：UDP-raw ：RAW类型--unix ：UNIX域类型--ax25 ：AX25类型--ipx ：ipx类型--netrom ：netrom类型 状态说明： LISTEN：侦听来自远方的TCP端口的连接请求SYN-SENT：再发送连接请求后等待匹配的连接请求（如果有大量这样的状态包，检查是否中招了）SYN-RECEIVED：再收到和发送一个连接请求后等待对方对连接请求的确认（如有大量此状态，估计被flood攻击了）ESTABLISHED：代表一个打开的连接FIN-WAIT-1：等待远程TCP连接中断请求，或先前的连接中断请求的确认FIN-WAIT-2：从远程TCP等待连接中断请求CLOSE-WAIT：等待从本地用户发来的连接中断请求CLOSING：等待远程TCP对连接中断的确认LAST-ACK：等待原来的发向远程TCP的连接中断请求的确认（不是什么好东西，此项出现，检查是否被攻击）TIME-WAIT：等待足够的时间以确保远程TCP接收到连接中断请求的确认CLOSED：没有任何连接状态 实例二：列出所有端口命令：netstat -a输出： 说明：显示一个所有的有效连接信息列表，包括已建立的连接（ESTABLISHED），也包括监听连接请（LISTENING）的那些连接。 实例三：显示当前UDP连接状况命令：netstat -nu输出： 实例四：显示UDP端口号的使用情况命令：netstat -apu输出： 实例五：显示网卡列表命令：netstat -i输出：123456[root@andy ~]# netstat -iKernel Interface tableIface MTU Met RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1500 0 151818887 0 0 0 198928403 0 0 0 BMRUlo 16436 0 107235 0 0 0 107235 0 0 0 LRU[root@andy ~]# 实例六：显示组播组的关系命令：netstat -g输出： 实例七：显示网络统计信息命令：netstat -s输出： 说明：按照各个协议分别显示其统计数据。如果我们的应用程序（如Web浏览器）运行速度比较慢，或者不能显示Web页之类的数据，那么我们就可以用本选项来查看一下所显示的信息。我们需要仔细查看统计数据的各行，找到出错的关键字，进而确定问题所在。 实例八：显示监听的套接口命令：netstat -l输出：12345678910111213[root@localhost ~]# netstat -lActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 localhost:smux *:* LISTENtcp 0 0 *:svn *:* LISTENtcp 0 0 *:ssh *:* LISTENudp 0 0 localhost:syslog *:*udp 0 0 *:snmp *:*Active UNIX domain sockets (only servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ACC ] STREAM LISTENING 708833 /tmp/ssh-yKnDB15725/agent.15725unix 2 [ ACC ] STREAM LISTENING 7296 /var/run/audispd_events[root@localhost ~]# 实例九：显示所有已建立的有效连接命令：netstat -n输出：12345678910111213[root@localhost ~]# netstat -nActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 268 192.168.120.204:22 10.2.0.68:62420 ESTABLISHEDActive UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 1491 @/org/kernel/udev/udevdunix 4 [ ] DGRAM 7337 /dev/logunix 2 [ ] DGRAM 708823unix 2 [ ] DGRAM 7539unix 3 [ ] STREAM CONNECTED 7287unix 3 [ ] STREAM CONNECTED 7286[root@localhost ~]# 实例十：显示关于以太网的统计数据命令：netstat -e输出：12345678910111213[root@localhost ~]# netstat -eActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address State User Inodetcp 0 248 192.168.120.204:ssh 10.2.0.68:62420 ESTABLISHED root 708795Active UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 1491 @/org/kernel/udev/udevdunix 4 [ ] DGRAM 7337 /dev/logunix 2 [ ] DGRAM 708823unix 2 [ ] DGRAM 7539unix 3 [ ] STREAM CONNECTED 7287unix 3 [ ] STREAM CONNECTED 7286[root@localhost ~]# 说明：用于显示关于以太网的统计数据。它列出的项目包括传送的数据报的总字节数、错误数、删除数、数据报的数量和广播的数量。这些统计数据既有发送的数据报数量，也有接收的数据报数量。这个选项可以用来统计一些基本的网络流量） 实例十一：显示关于路由表的信息命令：netstat -r输出：12345678[root@localhost ~]# netstat -rKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0default 192.168.120.240 0.0.0.0 UG 0 0 0 eth0[root@localhost ~]# 实例十二：列出所有 tcp 端口命令：netstat -at输出： 实例十三：统计机器中网络连接各个状态个数命令：netstat -a | awk ‘/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}’输出：1234[root@localhost ~]# netstat -a | awk &apos;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&apos;ESTABLISHED 1LISTEN 3[root@localhost ~]# 实例十四：把状态全都取出来后使用uniq -c统计后再进行排序命令：netstat -nat |awk ‘{print $6}’|sort|uniq -c输出： 实例十五：查看连接某服务端口最多的的IP地址命令：netstat -nat | grep “192.168.120.20:16067” |awk ‘{print $5}’|awk -F: ‘{print $4}’|sort|uniq -c|sort -nr|head -20 实例十六：找出程序运行的端口命令：netstat -ap | grep ssh 实例十七：在 netstat 输出中显示 PID 和进程名称命令：netstat -pt 说明：netstat -p 可以与其它开关一起使用，就可以添加 “PID/进程名称” 到 netstat 输出中，这样 debugging 的时候可以很方便的发现特定端口运行的程序。 实例十九：找出运行在指定端口的进程命令：netstat -anpt | grep ‘:16064’ 说明：运行在端口16064的进程id为24596，再通过ps命令就可以找到具体的应用程序了。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"netstat","slug":"netstat","permalink":"ly2513.github.com/tags/netstat/"}]},{"title":"每天掌握一个Linux命令(55) traceroute命令","date":"2018-01-13T14:49:36.000Z","path":"2018/01/13/每天掌握一个Linux命令-55-traceroute命令/","text":"通过traceroute我们可以知道信息从你的计算机到互联网另一端的主机是走的什么路径。当然每次数据包由某一同样的出发点（source）到达某一同样的目的地(destination)走的路径可能会不一样，但基本上来说大部分时候所走的路由是相同的。linux系统中，我们称之为traceroute,在MS Windows中为tracert。 traceroute通过发送小的数据包到目的设备直到其返回，来测量其需要多长时间。一条路径上的每个设备traceroute要测3次。输出结果中包括每次测试的时间(ms)和设备的名称（如有的话）及其IP地址。在大多数情况下，我们会在linux主机系统下，直接执行命令行：traceroute hostname而在Windows系统下是执行tracert的命令：tracert hostname 1.命令格式： traceroute[参数][主机] 2.命令功能： traceroute指令让你追踪网络数据包的路由途径，预设数据包大小是40Bytes，用户可另行设置。具体参数格式：traceroute [-dFlnrvx][-f&lt;存活数值&gt;][-g&lt;网关&gt;…][-i&lt;网络界面&gt;][-m&lt;存活数值&gt;][-p&lt;通信端口&gt;][-s&lt;来源地址&gt;][-t&lt;服务类型&gt;][-w&lt;超时秒数&gt;][主机名称或IP地址][数据包大小] 3.命令参数： -d 使用Socket层级的排错功能。-f 设置第一个检测数据包的存活数值TTL的大小。-F 设置勿离断位。-g 设置来源路由网关，最多可设置8个。-i 使用指定的网络界面送出数据包。-I 使用ICMP回应取代UDP资料信息。-m 设置检测数据包的最大存活数值TTL的大小。-n 直接使用IP地址而非主机名称。-p 设置UDP传输协议的通信端口。-r 忽略普通的Routing Table，直接将数据包送到远端主机上。-s 设置本地主机送出数据包的IP地址。-t 设置检测数据包的TOS数值。-v 详细显示指令的执行过程。-w 设置等待远端主机回报的时间。-x 开启或关闭数据包的正确性检验。 4.使用实例： 实例一：traceroute 用法简单、最常用的用法命令：1traceroute www.baidu.com 输出：12345678910111213[root@localhost ~]# traceroute www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packets 1 192.168.74.2 (192.168.74.2) 2.606 ms 2.771 ms 2.950 ms 2 211.151.56.57 (211.151.56.57) 0.596 ms 0.598 ms 0.591 ms 3 211.151.227.206 (211.151.227.206) 0.546 ms 0.544 ms 0.538 ms 4 210.77.139.145 (210.77.139.145) 0.710 ms 0.748 ms 0.801 ms 5 202.106.42.101 (202.106.42.101) 6.759 ms 6.945 ms 7.107 ms 6 61.148.154.97 (61.148.154.97) 718.908 ms * bt-228-025.bta.net.cn (202.106.228.25) 5.177 ms 7 124.65.58.213 (124.65.58.213) 4.343 ms 4.336 ms 4.367 ms 8 202.106.35.190 (202.106.35.190) 1.795 ms 61.148.156.138 (61.148.156.138) 1.899 ms 1.951 ms 9 * * *30 * * *[root@localhost ~]# 说明：记录按序列号从1开始，每个纪录就是一跳 ，每跳表示一个网关，我们看到每行有三个时间，单位是 ms，其实就是-q的默认参数。探测数据包向每个网关发送三个数据包后，网关响应后返回的时间；如果您用 traceroute -q 4 www.58.com ，表示向每个网关发送4个数据包。有时我们traceroute 一台主机时，会看到有一些行是以星号表示的。出现这样的情况，可能是防火墙封掉了ICMP的返回信息，所以我们得不到什么相关的数据包返回数据。有时我们在某一网关处延时比较长，有可能是某台网关比较阻塞，也可能是物理设备本身的原因。当然如果某台DNS出现问题时，不能解析主机名、域名时，也会 有延时长的现象；您可以加-n 参数来避免DNS解析，以IP格式输出数据。如果在局域网中的不同网段之间，我们可以通过traceroute 来排查问题所在，是主机的问题还是网关的问题。如果我们通过远程来访问某台服务器遇到问题时，我们用到traceroute 追踪数据包所经过的网关，提交IDC服务商，也有助于解决问题；但目前看来在国内解决这样的问题是比较困难的，就是我们发现问题所在，IDC服务商也不可能帮助我们解决。 实例二：跳数设置命令：1traceroute -m 10 www.baidu.com 输出：12345678910111213[root@localhost ~]# traceroute -m 10 www.baidu.comtraceroute to www.baidu.com (61.135.169.105), 10 hops max, 40 byte packets 1 192.168.74.2 (192.168.74.2) 1.534 ms 1.775 ms 1.961 ms 2 211.151.56.1 (211.151.56.1) 0.508 ms 0.514 ms 0.507 ms 3 211.151.227.206 (211.151.227.206) 0.571 ms 0.558 ms 0.550 ms 4 210.77.139.145 (210.77.139.145) 0.708 ms 0.729 ms 0.785 ms 5 202.106.42.101 (202.106.42.101) 7.978 ms 8.155 ms 8.311 ms 6 bt-228-037.bta.net.cn (202.106.228.37) 772.460 ms bt-228-025.bta.net.cn (202.106.228.25) 2.152 ms 61.148.154.97 (61.148.154.97) 772.107 ms 7 124.65.58.221 (124.65.58.221) 4.875 ms 61.148.146.29 (61.148.146.29) 2.124 ms 124.65.58.221 (124.65.58.221) 4.854 ms 8 123.126.6.198 (123.126.6.198) 2.944 ms 61.148.156.6 (61.148.156.6) 3.505 ms 123.126.6.198 (123.126.6.198) 2.885 ms 9 * * *10 * * *[root@localhost ~]# 实例三：显示IP地址，不查主机名命令：1traceroute -n www.baidu.com 输出：12345678910111213141516171819202122232425[root@localhost ~]# traceroute -n www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packets 1 211.151.74.2 5.430 ms 5.636 ms 5.802 ms 2 211.151.56.57 0.627 ms 0.625 ms 0.617 ms 3 211.151.227.206 0.575 ms 0.584 ms 0.576 ms 4 210.77.139.145 0.703 ms 0.754 ms 0.806 ms 5 202.106.42.101 23.683 ms 23.869 ms 23.998 ms 6 202.106.228.37 247.101 ms * * 7 61.148.146.29 5.256 ms 124.65.58.213 4.386 ms 4.373 ms 8 202.106.35.190 1.610 ms 61.148.156.138 1.786 ms 61.148.3.34 2.089 ms 9 * * *30 * * *[root@localhost ~]# traceroute www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packets 1 211.151.74.2 (211.151.74.2) 4.671 ms 4.865 ms 5.055 ms 2 211.151.56.57 (211.151.56.57) 0.619 ms 0.618 ms 0.612 ms 3 211.151.227.206 (211.151.227.206) 0.620 ms 0.642 ms 0.636 ms 4 210.77.139.145 (210.77.139.145) 0.720 ms 0.772 ms 0.816 ms 5 202.106.42.101 (202.106.42.101) 7.667 ms 7.910 ms 8.012 ms 6 bt-228-025.bta.net.cn (202.106.228.25) 2.965 ms 2.440 ms 61.148.154.97 (61.148.154.97) 431.337 ms 7 124.65.58.213 (124.65.58.213) 5.134 ms 5.124 ms 5.044 ms 8 202.106.35.190 (202.106.35.190) 1.917 ms 2.052 ms 2.059 ms 9 * * *30 * * *[root@localhost ~]# 实例四：探测包使用的基本UDP端口设置6888命令：1traceroute -p 6888 www.baidu.com 输出：123456789[root@localhost ~]# traceroute -p 6888 www.baidu.comtraceroute to www.baidu.com (220.181.111.147), 30 hops max, 40 byte packets 1 211.151.74.2 (211.151.74.2) 4.927 ms 5.121 ms 5.298 ms 2 211.151.56.1 (211.151.56.1) 0.500 ms 0.499 ms 0.509 ms 3 211.151.224.90 (211.151.224.90) 0.637 ms 0.631 ms 0.641 ms 4 * * * 5 220.181.70.98 (220.181.70.98) 5.050 ms 5.313 ms 5.596 ms 6 220.181.17.94 (220.181.17.94) 1.665 ms !X * *[root@localhost ~]# 实例五：把探测包的个数设置为值4命令：1traceroute -q 4 www.baidu.com 输出：12345678910111213[root@localhost ~]# traceroute -q 4 www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packets 1 211.151.74.2 (211.151.74.2) 40.633 ms 40.819 ms 41.004 ms 41.188 ms 2 211.151.56.57 (211.151.56.57) 0.637 ms 0.633 ms 0.627 ms 0.619 ms 3 211.151.227.206 (211.151.227.206) 0.505 ms 0.580 ms 0.571 ms 0.569 ms 4 210.77.139.145 (210.77.139.145) 0.753 ms 0.800 ms 0.853 ms 0.904 ms 5 202.106.42.101 (202.106.42.101) 7.449 ms 7.543 ms 7.738 ms 7.893 ms 6 61.148.154.97 (61.148.154.97) 316.817 ms bt-228-025.bta.net.cn (202.106.228.25) 3.695 ms 3.672 ms * 7 124.65.58.213 (124.65.58.213) 3.056 ms 2.993 ms 2.960 ms 61.148.146.29 (61.148.146.29) 2.837 ms 8 61.148.3.34 (61.148.3.34) 2.179 ms 2.295 ms 2.442 ms 202.106.35.190 (202.106.35.190) 7.136 ms 9 * * * *30 * * * *[root@localhost ~]# 实例六：绕过正常的路由表，直接发送到网络相连的主机命令：1traceroute -r www.baidu.com 输出：1234[root@localhost ~]# traceroute -r www.baidu.comtraceroute to www.baidu.com (61.135.169.125), 30 hops max, 40 byte packetsconnect: 网络不可达[root@localhost ~]# 实例七：把对外发探测包的等待响应时间设置为3秒命令：1traceroute -w 3 www.baidu.com 输出：12345678910111213[root@localhost ~]# traceroute -w 3 www.baidu.comtraceroute to www.baidu.com (61.135.169.105), 30 hops max, 40 byte packets 1 211.151.74.2 (211.151.74.2) 2.306 ms 2.469 ms 2.650 ms 2 211.151.56.1 (211.151.56.1) 0.621 ms 0.613 ms 0.603 ms 3 211.151.227.206 (211.151.227.206) 0.557 ms 0.560 ms 0.552 ms 4 210.77.139.145 (210.77.139.145) 0.708 ms 0.761 ms 0.817 ms 5 202.106.42.101 (202.106.42.101) 7.520 ms 7.774 ms 7.902 ms 6 bt-228-025.bta.net.cn (202.106.228.25) 2.890 ms 2.369 ms 61.148.154.97 (61.148.154.97) 471.961 ms 7 124.65.58.221 (124.65.58.221) 4.490 ms 4.483 ms 4.472 ms 8 123.126.6.198 (123.126.6.198) 2.948 ms 61.148.156.6 (61.148.156.6) 7.688 ms 7.756 ms 9 * * *30 * * *[root@localhost ~]# 说明：Traceroute的工作原理：Traceroute最简单的基本用法是：traceroute hostnameTraceroute程序的设计是利用ICMP及IP header的TTL（Time To Live）栏位（field）。首先，traceroute送出一个TTL是1的IP datagram（其实，每次送出的为3个40字节的包，包括源地址，目的地址和包发出的时间标签）到目的地，当路径上的第一个路由器（router）收到这个datagram时，它将TTL减1。此时，TTL变为0了，所以该路由器会将此datagram丢掉，并送回一个「ICMP time exceeded」消息（包括发IP包的源地址，IP包的所有内容及路由器的IP地址），traceroute 收到这个消息后，便知道这个路由器存在于这个路径上，接着traceroute 再送出另一个TTL是2 的datagram，发现第2 个路由器…… traceroute 每次将送出的datagram的TTL 加1来发现另一个路由器，这个重复的动作一直持续到某个datagram 抵达目的地。当datagram到达目的地后，该主机并不会送回ICMP time exceeded消息，因为它已是目的地了，那么traceroute如何得知目的地到达了呢？Traceroute在送出UDP datagrams到目的地时，它所选择送达的port number 是一个一般应用程序都不会用的号码（30000 以上），所以当此UDP datagram 到达目的地后该主机会送回一个「ICMP port unreachable」的消息，而当traceroute 收到这个消息时，便知道目的地已经到达了。所以traceroute 在Server端也是没有所谓的Daemon 程式。Traceroute提取发 ICMP TTL到期消息设备的IP地址并作域名解析。每次 ，Traceroute都打印出一系列数据,包括所经过的路由设备的域名及 IP地址,三个包每次来回所花时间。windows之tracert:格式： tracert [-d] [-h maximum_hops] [-j host-list] [-w timeout] target_name 参数说明： tracert [-d] [-h maximum_hops] [-j computer-list] [-w timeout] target_name 该诊断实用程序通过向目的地发送具有不同生存时间 (TL) 的 Internet 控制信息协议 (CMP) 回应报文，以确定至目的地的路由。路径上的每个路由器都要在转发该 ICMP 回应报文之前将其 TTL 值至少减 1，因此 TTL 是有效的跳转计数。当报文的 TTL 值减少到 0 时，路由器向源系统发回 ICMP 超时信息。通过发送 TTL 为 1 的第一个回应报文并且在随后的发送中每次将 TTL 值加 1，直到目标响应或达到最大 TTL 值，Tracert 可以确定路由。通过检查中间路由器发发回的 ICMP 超时 (ime Exceeded) 信息，可以确定路由器。注意，有些路由器“安静”地丢弃生存时间 (TLS) 过期的报文并且对 tracert 无效。参数：-d 指定不对计算机名解析地址。 -h maximum_hops 指定查找目标的跳转的最大数目。 -jcomputer-list 指定在 computer-list 中松散源路由。 -w timeout 等待由 timeout 对每个应答指定的毫秒数。 target_name 目标计算机的名称。 实例：1234567891011121314151617181920212223242526272829C:\\Users\\Administrator&gt;tracert www.58.comTracing route to www.58.com [221.187.111.30]over a maximum of 30 hops: 1 1 ms 1 ms 1 ms 10.58.156.1 2 1 ms &lt;1 ms &lt;1 ms 10.10.10.1 3 1 ms 1 ms 1 ms 211.103.193.129 4 2 ms 2 ms 2 ms 10.255.109.129 5 1 ms 1 ms 3 ms 124.205.98.205 6 2 ms 2 ms 2 ms 124.205.98.253 7 2 ms 6 ms 1 ms 202.99.1.125 8 5 ms 6 ms 5 ms 118.186.0.113 9 207 ms * * 118.186.0.106 10 8 ms 6 ms 11 ms 124.238.226.201 11 6 ms 7 ms 6 ms 219.148.19.177 12 12 ms 12 ms 16 ms 219.148.18.117 13 14 ms 17 ms 16 ms 219.148.19.125 14 13 ms 13 ms 12 ms 202.97.80.113 15 * * * Request timed out. 16 12 ms 12 ms 17 ms bj141-147-82.bjtelecom.net [219.141.147.82] 17 13 ms 13 ms 12 ms 202.97.48.2 18 * * * Request timed out. 19 14 ms 14 ms 12 ms 221.187.224.85 20 15 ms 13 ms 12 ms 221.187.104.2 21 * * * Request timed out. 22 15 ms 17 ms 18 ms 221.187.111.30Trace complete.","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"traceroute","slug":"traceroute","permalink":"ly2513.github.com/tags/traceroute/"}]},{"title":"每天掌握一个Linux命令(54) ping命令","date":"2018-01-10T13:40:02.000Z","path":"2018/01/10/每天掌握一个Linux命令-54-ping命令/","text":"Linux的系统的平命令是常用的网络命令，它通常用来测试与目标主机的连通性，我们经常会说的“ping一下某机器，看是不是开着”，不能打开网页时会说“你先平网络地址192.168.1.1试试“。它通过发送ICMP ECHO_REQUEST数据包到网络主机（发送ICMP ECHO_REQUEST到网络主机），并显示响应情况，这样我们就可以根据它输出的信息来确定目标主机是否可访问但这不是绝对的）。有些服务器为了防止通过平探测到，通过防火墙设置了禁止平或者在内核参数中禁止平，这样就不能通过平确定该主机是否还处于开启状态。的Linux下的平和窗下的平稍有区别，LINUX下平不会自动终止，需要按CTRL + C终止或者用参数-c指定要求完成的回应次数。 1.命令格式： ping [参数] [主机名或IP地址] 2.命令功能： 平命令用于：。确定网络和各外部主机的状态;跟踪和隔离硬件和软件问题;测试，评估和管理网络如果主机正在运行并连在网上，它就对回送信号进行响应每个回送信号请求包含一个网际协议（IP）和ICMP头，后面紧跟一个tim结构，以及来填写这个信息包的足够的字节。缺省情况是连续发送回送信号请求直到接收到中断信号（Ctrl -C）。ping命令每秒发送一个数据报并且为每个接收到的响应打印一行输出.ping命令计算信号往返时间和（信息）包丢失情况的统计信息，并且在完成之后显示一个简要总结.ping命令在程序超时或当接收到SIGINT信号时结束.Host参数或者是一个有效的主机名或者是因特网地址。 3.命令参数： -d使用Socket的SO_DEBUG功能。 -f极限检测。大量且快速地送网络封包给一台机器，看它的回应。 -n只输出数值。 -q不显示任何传送封包的信息，只显示最后的结果。 -r忽略普通的路由表，直接将数据包送到远端主机上。通常是查看本机的网络接口是否有问题。 -R记录路由过程。 -v详细显示指令的执行过程。 -c数据：在发送指定数据的包后停止。 -i秒数：设定间隔几秒送一个网络封包给一台机器，预设值是一秒送一次。 -I网络界面：使用指定的网络界面发送数据包。 -l前置载入：设置在送出要求信息之前，先行发出的数据包。 -p范本样式：设置填满数据包的范本样式。 -s字节数：指定发送的数据字节数，预设值是56，加上8字节的ICMP头，一共是64ICMP数据字节。 -t存活数值：设置存活数值TTL的大小。 4.使用实例： 实例一：平的通的情况命令：1ping 192.168.120.205 输出：123456789101112[root @ localhost〜]＃ping 192.168.120.205PING 192.168.120.205（192.168.120.205）56（84）字节的数据。来自192.168.120.205的64字节：icmp_seq = 1 ttl = 64时间=0.720ms来自192.168.120.205的64字节：icmp_seq = 2ttl = 64时间=0.181ms来自192.168.120.205的64字节：icmp_seq = 3tl = 64时间= 0.191 MS从192.168.120.205 64个字节：icmp_seq = 4 TTL = 64时间= 0.188毫秒从192.168.120.205 64个字节：icmp_seq = 5 TTL = 64时间= 0.189毫秒--- 192.168.120.205 ping统计---5个分组发送5收到0％丢包，时间4000msrtt min / avg / max / mdev = 0.181 / 0.293 / 0.720 / 0.214 ms[root @ localhost〜]＃ 实例二：平不通的情况命令：1ping 192.168.120.202 输出：1234567891011121314[root @ localhost〜]＃ping 192.168.120.202PING 192.168.120.202（192.168.120.202）56（84）字节的数据。从192.168.120.204 icmp_seq = 1目标主机不可达从192.168.120.204 icmp_seq = 2目标主机不可达从192.168.120.204 icmp_seq = 3目标主机不可达从192.168.120.204 icmp_seq = 4目标主机不可达从192.168.120.204 icmp_seq = 5目标主机不可达从192.168.120.204 icmp_seq = 6目的主机不可达--- 192.168.120.202 ping统计---8个报文发送，0个接收，+6个错误，100％丢包，时间7005ms，管道4个[root @ localhost〜]＃ 实例三：平网关命令：1ping -b 192.168.120.1 输出：123456789101112131415161718[root @ localhost〜]＃route 内核IP路由表目标网关掩码标志度量参考用法Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168 .120.1 255.0.0.0 UG 0 0 0 eth0默认192.168.120.240 0.0.0.0 UG 0 0 0 eth0[root @ localhost〜] ＃ping -b 192.168.120.1PING 192.168.120.1（192.168.120.1）56（84）bytes数据。来自192.168.120.1的64个字节：icmp_seq = 1 ttl = 255时间=2.02ms来自192.168.120.1的64字节：icmp_seq = 2 ttl = 255时间= 1.83ms来自192.168.120.1的64字节：icmp_seq = 3 ttl = 255时间=1.68ms来自192.168.120.1的64字节：icmp_seq = 4ttl = 255时间=1.98ms来自192.168.120.1的64字节：icmp_seq = 5ttl = 255时间= 1.88 ms--- 192.168.120.1 ping statistics ---发送5个数据包，接收到5个数据包，丢包0％，时间4000msrtt min / avg / max / mdev = 1.682 / 1.880 / 2.020 / 0.129 ms 实例四：平指定次数命令：1ping -c 10 192.168.120.206 输出：1234567891011121314151617[root @ localhost〜]＃ping -c 10 192.168.120.206PING 192.168.120.206（192.168.120.206）56（84）字节的数据。来自192.168.120.206的64字节：icmp_seq = 1 ttl = 64时间= 1.25ms来自192.168.120.206的64字节：icmp_seq = 2ttl = 64时间=0.260ms来自192.168.120.206的64字节：icmp_seq = 3tl = 64时间= 0.242 MS从192.168.120.206 64个字节：icmp_seq = 4 TTL = 64时间= 0.271毫秒从192.168.120.206 64个字节：icmp_seq = 5 TTL = 64时间= 0.274毫秒从192.168.120.206 64个字节：icmp_seq = 6 TTL = 64时间= 0.295 ms来自192.168.120.206的64个字节：icmp_seq = 7 ttl = 64时间= 0.269 ms来自192.168.120.206的64字节：icmp_seq = 8 ttl = 64时间=0.270ms来自192.168.120.206的64字节：icmp_seq = 9 ttl = 64时间= 0.253ms来自192.168.120.206的64个字节：icmp_seq = 10 ttl = 64时间= 0.289ms--- 192.168.120.206 ping统计信息---发送了10个分组，接收到10个分组，丢失0％，时间9000msrtt min / avg / max / mdev = 0.242 / 0.367 / 1.251 / 0.295 ms[root @ localhost〜]＃ 实例五：时间间隔和次数限制的平命令：1ping -c 10 -i 0.5 192.168.120.206 输出：123456789101112131415161718192021222324252627282930313233[root @ localhost〜]＃ping -c 10 -i 0.5 192.168.120.206PING 192.168.120.206（192.168.120.206）56（84）字节的数据。来自192.168.120.206的64字节：icmp_seq = 1 ttl = 64时间=1.24ms来自192.168.120.206的64字节：icmp_seq = 2ttl = 64时间=0.235ms来自192.168.120.206的64字节：icmp_seq = 3tl = 64时间= 0.244 MS从192.168.120.206 64个字节：icmp_seq = 4 TTL = 64时间= 0.300毫秒从192.168.120.206 64个字节：icmp_seq = 5 TTL = 64时间= 0.255毫秒从192.168.120.206 64个字节：icmp_seq = 6 TTL = 64时间= 0.264 ms来自192.168.120.206的64字节：icmp_seq = 7 ttl = 64时间=0.263 ms 来自192.168.120.206的64字节：icmp_seq = 8 ttl = 64时间= 0.331 ms来自192.168.120.206的64字节：icmp_seq = 9 ttl = 64时间= 0.247毫秒来自192.168.120.206的64个字节：icmp_seq = 10 ttl = 64时间= 0.244ms--- 192.168.120.206 ping统计信息---发送了10个包，接收到10个，包丢失0％，时间4499msrtt min / avg / max / mdev = 0.235 / 0.362 / 1.241 / 0.294 ms[root @ localhost〜]＃ping -c 10 -i 0.01 192.168.120.206PING 192.168.120.206（192.168.120.206）56（84）字节的数据。来自192.168.120.206的64字节：icmp_seq = 1 ttl = 64时间=0.244ms来自192.168.120.206的64字节：icmp_seq = 2ttl = 64时间=0.195ms来自192.168.120.206的64字节：icmp_seq = 3ttl = 64时间= 0.219 MS从192.168.120.206 64个字节：icmp_seq = 4 TTL = 64时间= 0.204毫秒从192.168.120.206 64个字节：icmp_seq = 5 TTL = 64时间= 3.56毫秒从192.168.120.206 64个字节：icmp_seq = 6 TTL = 64时间= 1.93毫秒来自192.168.120.206的64个字节：icmp_seq = 7ttl = 64时间= 0.193ms来自192.168.120.206的64字节：icmp_seq = 8ttl = 64时间= 0.193ms来自192.168.120.206的64字节：icmp_seq = 9ttl = 64时间= 0.202 ms64个字节从192.168.120.206：icmp_seq = 10 ttl = 64时间= 0.211 ms--- 192.168.120.206 ping统计数据---发送10个数据包，接收10个数据包，丢包0％，时间90msrtt min / avg / max / mdev = 0.193 / 0.716 / 3.564 / 1.080 ms[root @ localhost〜]＃ 实例六：通过域名平公网上的站点命令：1ping -c 5 www.58.com 输出：123456789101112peida-VirtualBox〜＃ping -c 5 www.58.comPING www.58.com（211.151.111.30）56（84）字节的数据。来自211.151.111.30的64字节：icmp_req = 1 ttl = 49时间=14.7ms来自211.151.111.30的64字节：icmp_req = 2ttl = 49时间=16.4ms来自211.151.111.30的64字节：icmp_req = 3ttl = 49时间= 15.2从211.151.111.30开始ms 64字节：icmp_req = 4 ttl = 49时间= 14.6 ms从211.151.111.30开始64字节：icmp_req = 5 ttl = 49时间= 19.9 ms--- www.58.com ping statistics ---5个数据包发送，5收到，0％丢包，时间20101msrtt min / avg / max / mdev = 14.618 / 16.192 / 19.917 / 1.965 mspeida-VirtualBox〜＃ 实例七：多参数使用命令：1ping -i 3 -s 1024 -t 255 192.168.120.206 输出：1234567891011121314151617181920212223[root @ localhost〜]＃ping -i 3 -s 1024 -t 255 192.168.120.206PING 192.168.120.206 (192.168.120.206) 1024(1052) bytes of data.1032 bytes from 192.168.120.206: icmp_seq=1 ttl=64 time=1.99 ms1032 bytes from 192.168.120.206: icmp_seq=2 ttl=64 time=0.694 ms1032 bytes from 192.168.120.206: icmp_seq=3 ttl=64 time=0.300 ms1032 bytes from 192.168.120.206: icmp_seq=4 ttl=64 time=0.481 ms1032 bytes from 192.168.120.206: icmp_seq=5 ttl=64 time=0.415 ms1032 bytes from 192.168.120.206: icmp_seq=6 ttl=64 time=0.600 ms1032 bytes from 192.168.120.206: icmp_seq=7 ttl=64 time=0.411 ms1032 bytes from 192.168.120.206: icmp_seq=8 ttl=64 time=0.281 ms1032 bytes from 192.168.120.206: icmp_seq=9 ttl=64 time=0.318 ms1032 bytes from 192.168.120.206: icmp_seq=10 ttl=64 time=0.362 ms来自192.168.120.206的1032个字节：icmp_seq = 11ttl = 64时间=0.408ms来自192.168.120.206的1032个字节：icmp_seq = 12ttl = 64时间=0.445ms来自192.168.120.206的1032个字节：icmp_seq = 13tl = 64时间= 0.397毫秒1032字节从192.168.120.206：icmp_seq = 14 ttl = 64时间= 0.406毫秒1032字节从192.168.120.206：icmp_seq = 15 ttl = 64时间= 0.458毫秒--- 192.168.120.206 ping统计---15个数据包传输， 15收到，0％丢包，时间41999msrtt min / avg / max / mdev = 0.281 / 0.531 / 1.993 / 0.404 ms[root @ localhost〜]＃ 说明：-i 3发送周期为3秒-s设置发送包的大小为1024 -t设置TTL值为255","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"ping","slug":"ping","permalink":"ly2513.github.com/tags/ping/"}]},{"title":"每天掌握一个Linux命令(53) route命令","date":"2018-01-09T14:50:13.000Z","path":"2018/01/09/每天掌握一个Linux命令-53-route命令/","text":"Linux系统的route命令用于显示和操作IP路由表（show / manipulate the IP routing table）。要实现两个不同的子网之间的通信，需要一台连接两个网络的路由器，或者同时位于两个网络的网关来实现。 在Linux系统中，设置路由通常是为了解决以下问题：该Linux系统在一个局域网中，局域网中有一个网关，能够让机器访问Internet，那么就需要将这台机器的IP地址设置为Linux机器的默认路由。要注意的是，直接在命令行下执行route命令来添加路由，不会永久保存，当网卡重启或者机器重启之后，该路由就失效了；可以在/etc/rc.local中添加route命令来保证该路由设置永久有效。 1．命令格式： 1route [-f] [-p] [Command [Destination] [mask Netmask] [Gateway] [metric Metric]] [if Interface]] 2．命令功能： Route命令是用于操作基于内核ip路由表，它的主要作用是创建一个静态路由让指定一个主机或者一个网络通过一个网络接口，如eth0。当使用”add”或者”del”参数时，路由表被修改，如果没有参数，则显示路由表当前的内容。 3．命令参数： -c 显示更多信息-n 不解析名字-v 显示详细的处理信息-F 显示发送信息-C 显示路由缓存-f 清除所有网关入口的路由表。-p 与 add 命令一起使用时使路由具有永久性。add:添加一条新路由。del:删除一条路由。-net:目标地址是一个网络。-host:目标地址是一个主机。netmask:当添加一个网络路由时，需要使用网络掩码。gw:路由数据包通过网关。注意，你指定的网关必须能够达到。metric：设置路由跳数。 Command 指定您想运行的命令 (Add/Change/Delete/Print)。 Destination 指定该路由的网络目标。 mask Netmask 指定与网络目标相关的网络掩码（也被称作子网掩码）。 Gateway 指定网络目标定义的地址集和子网掩码可以到达的前进或下一跃点 IP 地址。 metric Metric 为路由指定一个整数成本值标（从 1 至 9999），当在路由表(与转发的数据包目标地址最匹配)的多个路由中进行选择时可以使用。 if Interface 为可以访问目标的接口指定接口索引。若要获得一个接口列表和它们相应的接口索引，使用 route print 命令的显示功能。可以使用十进制或十六进制值进行接口索引。 4．使用实例： 实例一：显示当前路由命令： route route -n 输出：1234567891011121314[root@localhost ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0e192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0default 192.168.120.240 0.0.0.0 UG 0 0 0 eth0[root@localhost ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth00.0.0.0 192.168.120.240 0.0.0.0 UG 0 0 0 eth0 说明： 第一行表示主机所在网络的地址为192.168.120.0，若数据传送目标是在本局域网内通信，则可直接通过eth0转发数据包; 第四行表示数据传送目的是访问Internet，则由接口eth0，将数据包发送到网关192.168.120.240 其中Flags为路由标志，标记当前网络节点的状态。Flags标志说明： U Up表示此路由当前为启动状态H Host，表示此网关为一主机G Gateway，表示此网关为一路由器R Reinstate Route，使用动态路由重新初始化的路由D Dynamically,此路由是动态性地写入M Modified，此路由是由路由守护程序或导向器动态修改! 表示此路由当前为关闭状态备注：route -n (-n 表示不解析名字,列出速度会比route 快) 实例二：添加网关/设置网关命令：1route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0 输出：12345678910[root@localhost ~]# route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0[root@localhost ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0224.0.0.0 * 240.0.0.0 U 0 0 0 eth0default 192.168.120.240 0.0.0.0 UG 0 0 0 eth0[root@localhost ~]# 说明：增加一条 到达244.0.0.0的路由 实例三：屏蔽一条路由命令：1route add -net 224.0.0.0 netmask 240.0.0.0 reject 输出：12345678910[root@localhost ~]# route add -net 224.0.0.0 netmask 240.0.0.0 reject[root@localhost ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0224.0.0.0 - 240.0.0.0 ! 0 - 0 -224.0.0.0 * 240.0.0.0 U 0 0 0 eth0default 192.168.120.240 0.0.0.0 UG 0 0 0 eth0 说明：增加一条屏蔽的路由，目的地址为 224.x.x.x 将被拒绝 实例四：删除路由记录命令：123route del -net 224.0.0.0 netmask 240.0.0.0route del -net 224.0.0.0 netmask 240.0.0.0 reject 输出：12345678910111213141516171819202122232425262728[root@localhost ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0224.0.0.0 - 240.0.0.0 ! 0 - 0 -224.0.0.0 * 240.0.0.0 U 0 0 0 eth0default 192.168.120.240 0.0.0.0 UG 0 0 0 eth0[root@localhost ~]# route del -net 224.0.0.0 netmask 240.0.0.0[root@localhost ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0224.0.0.0 - 240.0.0.0 ! 0 - 0 -default 192.168.120.240 0.0.0.0 UG 0 0 0 eth0[root@localhost ~]# route del -net 224.0.0.0 netmask 240.0.0.0 reject[root@localhost ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0default 192.168.120.240 0.0.0.0 UG 0 0 0 eth0[root@localhost ~]# 实例五：删除和添加设置默认网关命令：123route del default gw 192.168.120.240route add default gw 192.168.120.240 输出：12345678910111213141516[root@localhost ~]# route del default gw 192.168.120.240[root@localhost ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0[root@localhost ~]# route add default gw 192.168.120.240[root@localhost ~]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.120.0 * 255.255.255.0 U 0 0 0 eth0192.168.0.0 192.168.120.1 255.255.0.0 UG 0 0 0 eth010.0.0.0 192.168.120.1 255.0.0.0 UG 0 0 0 eth0default 192.168.120.240 0.0.0.0 UG 0 0 0 eth0[root@localhost ~]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"route","slug":"route","permalink":"ly2513.github.com/tags/route/"}]},{"title":"每天掌握一个Linux命令(52) ifconfig命令","date":"2018-01-08T03:40:44.000Z","path":"2018/01/08/每天掌握一个Linux命令-52-ifconfig命令/","text":"许多windows非常熟悉ipconfig命令行工具，它被用来获取网络接口配置信息并对此进行修改。Linux系统拥有一个类似的工具，也就是ifconfig(interfaces config)。 通常需要以root身份登录或使用sudo以便在Linux机器上使用ifconfig工具。依赖于ifconfig命令中使用一些选项属性，ifconfig工具不仅可以被用来简单地获取网络接口配置信息，还可以修改这些配置。 1．命令格式： ifconfig [网络设备] [参数] 2．命令功能： ifconfig 命令用来查看和配置网络设备。当网络环境发生改变时可通过此命令对网络进行相应的配置。 3．命令参数： up 启动指定网络设备/网卡。down 关闭指定网络设备/网卡。该参数可以有效地阻止通过指定接口的IP信息流，如果想永久地关闭一个接口，我们还需要从核心路由表中将该接口的路由信息全部删除。arp 设置指定网卡是否支持ARP协议。-promisc 设置是否支持网卡的promiscuous模式，如果选择此参数，网卡将接收网络中发给它所有的数据包-allmulti 设置是否支持多播模式，如果选择此参数，网卡将接收网络中所有的多播数据包-a 显示全部接口信息-s 显示摘要信息（类似于 netstat -i）add 给指定网卡配置IPv6地址del 删除指定网卡的IPv6地址&lt;硬件地址&gt; 配置网卡最大的传输单元mtu&lt;字节数&gt; 设置网卡的最大传输单元 (bytes)netmask&lt;子网掩码&gt; 设置网卡的子网掩码。掩码可以是有前缀0x的32位十六进制数，也可以是用点分开的4个十进制数。如果不打算将网络分成子网，可以不管这一选项；如果要使用子网，那么请记住，网络中每一个系统必须有相同子网掩码。tunel 建立隧道dstaddr 设定一个远端地址，建立点对点通信-broadcast&lt;地址&gt; 为指定网卡设置广播协议-pointtopoint&lt;地址&gt; 为网卡设置点对点通讯协议multicast 为网卡设置组播标志address 为网卡设置IPv4地址txqueuelen&lt;长度&gt; 为网卡设置传输列队的长度 4．使用实例： 实例一：显示网络设备信息（激活状态的）命令：ifconfig输出：12345678910111213141516[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:20 inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0 TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:596390239 (568.7 MiB) TX bytes:2886956 (2.7 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:68 errors:0 dropped:0 overruns:0 frame:0 TX packets:68 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2856 (2.7 KiB) TX bytes:2856 (2.7 KiB) 说明： eth0 表示第一块网卡， 其中 HWaddr 表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是 00:50:56:BF:26:20 inet addr 用来表示网卡的IP地址，此网卡的 IP地址是 192.168.120.204，广播地址， Bcast:192.168.120.255，掩码地址Mask:255.255.255.0 lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 HTTPD服务器的指定到回坏地址，在浏览器输入 127.0.0.1 就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址） 第二行：网卡的IP地址、子网、掩码 第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节 第四、五行：接收、发送数据包情况统计 第七行：接收、发送数据字节数统计信息。 实例2：启动关闭指定网卡命令：123ifconfig eth0 upifconfig eth0 down 输出： 说明： ifconfig eth0 up 为启动网卡eth0 ；ifconfig eth0 down 为关闭网卡eth0。ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。 实例三：为网卡配置和删除IPv6地址命令：123ifconfig eth0 add 33ffe:3240:800:1005::2/64ifconfig eth0 del 33ffe:3240:800:1005::2/64 输出： 说明：123ifconfig eth0 add 33ffe:3240:800:1005::2/64 为网卡eth0配置IPv6地址；ifconfig eth0 add 33ffe:3240:800:1005::2/64 为网卡eth0删除IPv6地址； 练习的时候，ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。 实例四：用ifconfig修改MAC地址命令：1ifconfig eth0 hw ether 00:AA:BB:CC:DD:EE 输出：12345678910111213141516171819202122232425262728293031323334353637[root@localhost ~]# ifconfig eth0 down //关闭网卡[root@localhost ~]# ifconfig eth0 hw ether 00:AA:BB:CC:DD:EE //修改MAC地址[root@localhost ~]# ifconfig eth0 up //启动网卡[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:AA:BB:CC:DD:EE inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0 TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:596390239 (568.7 MiB) TX bytes:2886956 (2.7 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:68 errors:0 dropped:0 overruns:0 frame:0 TX packets:68 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2856 (2.7 KiB) TX bytes:2856 (2.7 KiB)[root@localhost ~]# ifconfig eth0 hw ether 00:50:56:BF:26:20 //关闭网卡并修改MAC地址[root@localhost ~]# ifconfig eth0 up //启动网卡[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:20 inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0 TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:596390239 (568.7 MiB) TX bytes:2886956 (2.7 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:68 errors:0 dropped:0 overruns:0 frame:0 TX packets:68 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2856 (2.7 KiB) TX bytes:2856 (2.7 KiB) 实例五：配置IP地址命令： 输出：123[root@localhost ~]# ifconfig eth0 192.168.120.56[root@localhost ~]# ifconfig eth0 192.168.120.56 netmask 255.255.255.0[root@localhost ~]# ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255 说明：ifconfig eth0 192.168.120.56给eth0网卡配置IP地：192.168.120.56 ifconfig eth0 192.168.120.56 netmask 255.255.255.0给eth0网卡配置IP地址：192.168.120.56 ，并加上子掩码：255.255.255.0 ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255给eth0网卡配置IP地址：192.168.120.56，加上子掩码：255.255.255.0，加上个广播地址： 192.168.120.255 实例六：启用和关闭ARP协议命令：12ifconfig eth0 arpifconfig eth0 -arp 输出：12[root@localhost ~]# ifconfig eth0 arp[root@localhost ~]# ifconfig eth0 -arp 说明： ifconfig eth0 arp 开启网卡eth0 的arp协议； ifconfig eth0 -arp 关闭网卡eth0 的arp协议； 实例七：设置最大传输单元命令：ifconfig eth0 mtu 1500输出：12345678910111213141516171819202122232425262728293031323334353637[root@localhost ~]# ifconfig eth0 mtu 1480[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:1F inet addr:192.168.120.203 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1480 Metric:1 RX packets:8712395 errors:0 dropped:0 overruns:0 frame:0 TX packets:36631 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:597062089 (569.4 MiB) TX bytes:2643973 (2.5 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:9973 errors:0 dropped:0 overruns:0 frame:0 TX packets:9973 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:518096 (505.9 KiB) TX bytes:518096 (505.9 KiB)[root@localhost ~]# ifconfig eth0 mtu 1500[root@localhost ~]# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:1F inet addr:192.168.120.203 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8712548 errors:0 dropped:0 overruns:0 frame:0 TX packets:36685 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:597072333 (569.4 MiB) TX bytes:2650581 (2.5 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:9973 errors:0 dropped:0 overruns:0 frame:0 TX packets:9973 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:518096 (505.9 KiB) TX bytes:518096 (505.9 KiB)[root@localhost ~]# 说明：设置能通过的最大数据包大小为 1500 bytes 备注： 用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"ifconfig","slug":"ifconfig","permalink":"ly2513.github.com/tags/ifconfig/"}]},{"title":"每天掌握一个Linux命令(51) lsof命令","date":"2018-01-07T13:36:00.000Z","path":"2018/01/07/每天掌握一个Linux命令-51-lsof命令/","text":"lsof（list open files）是一个列出当前系统打开文件的工具。在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。所以如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，无论这个文件的本质如何，该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口。因为应用程序打开文件的描述符列表提供了大量关于这个应用程序本身的信息，因此通过lsof工具能够查看这个列表对系统监测以及排错将是很有帮助的。 1．命令格式： lsof [参数][文件] 2．命令功能： 用于查看你进程开打的文件，打开文件的进程，进程打开的端口(TCP、UDP)。找回/恢复删除的文件。是十分方便的系统监视工具，因为lsof需要访问核心内存和各种文件，所以需要root用户执行。 lsof打开的文件可以是： 普通文件 目录 网络文件系统的文件 字符或设备文件 (函数)共享库 管道，命名管道 符号链接 网络文件（例如：NFS file、网络socket，unix域名socket） 还有其它类型的文件，等等 3．命令参数： 1234567891011121314151617181920212223-a 列出打开文件存在的进程-c&lt;进程名&gt; 列出指定进程所打开的文件-g 列出GID号进程详情-d&lt;文件号&gt; 列出占用该文件号的进程+d&lt;目录&gt; 列出目录下被打开的文件+D&lt;目录&gt; 递归列出目录下被打开的文件-n&lt;目录&gt; 列出使用NFS的文件-i&lt;条件&gt; 列出符合条件的进程。（4、6、协议、:端口、 @ip ）-p&lt;进程号&gt; 列出指定进程号所打开的文件-u 列出UID号进程详情-h 显示帮助信息-v 显示版本信息 4．使用实例： 实例一：无任何参数命令：1lsof 输出：12345678910111213141516171819202122232425262728[root@localhost ~]# lsofCOMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEinit 1 root cwd DIR 8,2 4096 2 /init 1 root rtd DIR 8,2 4096 2 /init 1 root txt REG 8,2 43496 6121706 /sbin/initinit 1 root mem REG 8,2 143600 7823908 /lib64/ld-2.5.soinit 1 root mem REG 8,2 1722304 7823915 /lib64/libc-2.5.soinit 1 root mem REG 8,2 23360 7823919 /lib64/libdl-2.5.soinit 1 root mem REG 8,2 95464 7824116 /lib64/libselinux.so.1init 1 root mem REG 8,2 247496 7823947 /lib64/libsepol.so.1init 1 root 10u FIFO 0,17 1233 /dev/initctlmigration 2 root cwd DIR 8,2 4096 2 /migration 2 root rtd DIR 8,2 4096 2 /migration 2 root txt unknown /proc/2/exeksoftirqd 3 root cwd DIR 8,2 4096 2 /ksoftirqd 3 root rtd DIR 8,2 4096 2 /ksoftirqd 3 root txt unknown /proc/3/exemigration 4 root cwd DIR 8,2 4096 2 /migration 4 root rtd DIR 8,2 4096 2 /migration 4 root txt unknown /proc/4/exeksoftirqd 5 root cwd DIR 8,2 4096 2 /ksoftirqd 5 root rtd DIR 8,2 4096 2 /ksoftirqd 5 root txt unknown /proc/5/exeevents/0 6 root cwd DIR 8,2 4096 2 /events/0 6 root rtd DIR 8,2 4096 2 /events/0 6 root txt unknown /proc/6/exeevents/1 7 root cwd DIR 8,2 4096 2 / 说明： lsof输出各列信息的意义如下： COMMAND：进程的名称 PID：进程标识符 PPID：父进程标识符（需要指定-R参数） USER：进程所有者 PGID：进程所属组 FD：文件描述符，应用程序通过文件描述符识别该文件。如cwd、txt等 （1）cwd：表示current work dirctory，即：应用程序的当前工作目录，这是该应用程序启动的目录，除非它本身对这个目录进行更改（2）txt ：该类型的文件是程序代码，如应用程序二进制文件本身或共享库，如上列表中显示的 /sbin/init 程序（3）lnn：library references (AIX);（4）er：FD information error (see NAME column);（5）jld：jail directory (FreeBSD);（6）ltx：shared library text (code and data);（7）mxx ：hex memory-mapped type number xx.（8）m86：DOS Merge mapped file;（9）mem：memory-mapped file;（10）mmap：memory-mapped device;（11）pd：parent directory;（12）rtd：root directory;（13）tr：kernel trace file (OpenBSD);（14）v86 VP/ix mapped file;（15）0：表示标准输出（16）1：表示标准输入（17）2：表示标准错误 一般在标准输出、标准错误、标准输入后还跟着文件状态模式：r、w、u等 （1）u：表示该文件被打开并处于读取/写入模式（2）r：表示该文件被打开并处于只读模式（3）w：表示该文件被打开并处于（4）空格：表示该文件的状态模式为unknow，且没有锁定（5）-：表示该文件的状态模式为unknow，且被锁定 同时在文件状态模式后面，还跟着相关的锁 （1）N：for a Solaris NFS lock of unknown type;（2）r：for read lock on part of the file;（3）R：for a read lock on the entire file;（4）w：for a write lock on part of the file;（文件的部分写锁）（5）W：for a write lock on the entire file;（整个文件的写锁）（6）u：for a read and write lock of any length;（7）U：for a lock of unknown type;（8）x：for an SCO OpenServer Xenix lock on part of the file;（9）X：for an SCO OpenServer Xenix lock on the entire file;（10）space：if there is no lock. TYPE：文件类型，如DIR、REG等，常见的文件类型 （1）DIR：表示目录（2）CHR：表示字符类型（3）BLK：块设备类型（4）UNIX： UNIX 域套接字（5）FIFO：先进先出 (FIFO) 队列（6）IPv4：网际协议 (IP) 套接字 DEVICE：指定磁盘的名称SIZE：文件的大小NODE：索引节点（文件在磁盘上的标识）NAME：打开文件的确切名称 实例二：查看谁正在使用某个文件，也就是说查找某个文件相关的进程命令：1lsof /bin/bash 输出： 123456[root@localhost ~]# lsof /bin/bashCOMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEbash 24159 root txt REG 8,2 801528 5368780 /bin/bashbash 24909 root txt REG 8,2 801528 5368780 /bin/bashbash 24941 root txt REG 8,2 801528 5368780 /bin/bash[root@localhost ~]# 说明： 实例三：递归查看某个目录的文件信息 命令：1lsof test/test3 输出：123456[root@localhost ~]# cd /opt/soft/[root@localhost soft]# lsof test/test3COMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEbash 24941 root cwd DIR 8,2 4096 2258872 test/test3vi 24976 root cwd DIR 8,2 4096 2258872 test/test3[root@localhost soft]# 说明：使用了+D，对应目录下的所有子目录和文件都会被列出 实例四：不使用+D选项，遍历查看某个目录的所有文件信息的方法命令：1lsof |grep &apos;test/test3&apos; 输出：123456[root@localhost soft]# lsof |grep &apos;test/test3&apos;bash 24941 root cwd DIR 8,2 4096 2258872 /opt/soft/test/test3vi 24976 root cwd DIR 8,2 4096 2258872 /opt/soft/test/test3vi 24976 root 4u REG 8,2 12288 2258882 /opt/soft/test/test3/.log2013.log.swp[root@localhost soft]# 实例五：列出某个用户打开的文件信息命令：1lsof -u username 说明:-u 选项，u其实是user的缩写 实例六：列出某个程序进程所打开的文件信息命令：1lsof -c mysql 说明: -c 选项将会列出所有以mysql这个进程开头的程序的文件，其实你也可以写成 lsof | grep mysql, 但是第一种方法明显比第二种方法要少打几个字符了 实例七：列出多个进程多个打开的文件信息命令：1lsof -c mysql -c apache 实例八：列出某个用户以及某个进程所打开的文件信息命令：1lsof -u test -c mysql 说明：用户与进程可相关，也可以不相关 实例九：列出除了某个用户外的被打开的文件信息命令：1lsof -u ^root 说明： ^这个符号在用户名之前，将会把是root用户打开的进程不让显示 实例十：通过某个进程号显示该进行打开的文件命令：1lsof -p 1 实例十一：列出多个进程号对应的文件信息命令：1lsof -p 1,2,3 实例十二：列出除了某个进程号，其他进程号所打开的文件信息命令：1lsof -p ^1 实例十三：列出所有的网络连接命令：1lsof -i 实例十四：列出所有tcp 网络连接信息命令：1lsof -i tcp 实例十五：列出所有udp网络连接信息命令：1lsof -i udp 实例16：列出谁在使用某个端口命令：1lsof -i :3306 实例十七：列出谁在使用某个特定的udp端口 命令：1lsof -i udp:55 或者：特定的tcp端口 命令：1lsof -i tcp:80 实例十八：列出某个用户的所有活跃的网络端口命令：1lsof -a -u test -i 实例十九：列出所有网络文件系统命令：1lsof -N 实例二十：域名socket文件命令：1lsof -u 实例二十一：某个用户组所打开的文件信息命令：1lsof -g 5555 实例二十二：根据文件描述列出对应的文件信息命令：1234567lsof -d description(like 2)例如：lsof -d txt例如：lsof -d 1例如：lsof -d 2 说明： 0表示标准输入，1表示标准输出，2表示标准错误，从而可知：所以大多数应用程序所打开的文件的 FD 都是从 3 开始 实例二十三：根据文件描述范围列出文件信息命令：1lsof -d 2-3 实例二十四：列出COMMAND列中包含字符串” sshd”，且文件描符的类型为txt的文件信息命令：1lsof -c sshd -a -d txt 输出：12345678[root@localhost soft]# lsof -c sshd -a -d txtCOMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEsshd 2756 root txt REG 8,2 409488 1027867 /usr/sbin/sshdsshd 24155 root txt REG 8,2 409488 1027867 /usr/sbin/sshdsshd 24905 root txt REG 8,2 409488 1027867 /usr/sbin/sshdsshd 24937 root txt REG 8,2 409488 1027867 /usr/sbin/sshd[root@localhost soft]#[root@localhost soft]# 实例二十五：列出被进程号为1234的进程所打开的所有IPV4 network files命令：1lsof -i 4 -a -p 1234 实例二十六：列出目前连接主机peida.linux上端口为：20，21，22，25，53，80相关的所有文件信息，且每隔3秒不断的执行lsof指令命令：1lsof -i @peida.linux:20,21,22,25,53,80 -r 3","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"lsof","slug":"lsof","permalink":"ly2513.github.com/tags/lsof/"}]},{"title":"快速上手Charles手机APP抓包工具","date":"2018-01-05T04:50:14.000Z","path":"2018/01/05/快速上手Charles手机APP抓包工具/","text":"一、英雄不问出处？ Charles是目前强大、流行的http抓包调试工具，Mac、Unix、Windows各个平台都支持，其功能强大到包括： 支持SSL代理，可以截取分析SSL的请求 支持流量控制。可以模拟慢速网络以及等待时间（latency）较长的请求。 支持AJAX调试。可以自动将json或xml数据格式化，方便查看。 支持AMF调试。可以将Flash Remoting 或 Flex Remoting信息格式化，方便查看。 支持重发网络请求，方便后端调试。 支持修改网络请求参数。 支持网络请求的截获并动态修改。 检查HTML，CSS和RSS内容是否符合W3C标准 So 特别是做APP开发，抓取APP网络请求、调试与服务端的通信，尤其真机测试网络请求时，Charles是必备工具。 Charles的工作原理很简单，本质是就是一个http抓包分析工具，在工作的时候需要先把charles设置成代理服务器，这样所有的网络请求都会经过charles了。在此借用网友的示例图（侵删）为大家详细展示一哈 1.普通http请求过程 2.加入了Charles的HTTP代理的请求与响应过程 Charles的安装的话可以去官网 http://www.charlesproxy.com/download/ 下载，当然正式版的是需要付费的，大家可以找度娘安装破解版的，方便又舒心 二、超神之路首先是如何用Charles抓取电脑模拟器的网络请求。电脑模拟器抓包相对比较简单，只需设置Charles代理即可。如图，打开Charles后，选择工具栏Proxy，确认勾选MAC OS X Proxy选项即可完成代理设置 这里需要为大家解释的是，Charles 主要提供两种查看封包的视图，分别名为 “Structure”和 “Sequence”。其中Structure 视图将网络请求按访问的域名分类；Sequence 视图将网络请求按访问的时间排序。大家可以根据具体的需要在这两种视图之前来回切换。Request的数据直接JSON格式化。如图 下面为大家详细介绍用Charles抓取手机APP网络请求的具体步骤。需要注意的是，抓取手机APP网络请求时，手机和电脑必须在一个局域网内，不一定非要是一个ip段，只要是同一个路由器下就可以了。 开启Charles http代理； 手机端Wifi添加代理； 开启Charles录制功能； 启动APP开始抓包； 开启Charles http代理a.设置Charles代理。因为只是要监控手机端APP网络请求，所以将此前设置的电脑代理勾去掉 a.激活http代理功能。进入Proxy-&gt;Proxy Setting，设置http proxy代理端口：8888（一般默认设置为这个） 手机端Wifi添加代理 点击你所连接的wifi - - 选择手动 – 输入代理服务器的IP与端口 IP即你的电脑IP地址（打开电脑的网络设置，里面就醒目的显示了IP地址，或者打开终端，使用ifconfig命令查看），端口就是前面一步设置Charles时所设置的端口(即8888)。 此处需要注意的是，当结束抓包时，要记得把手机WiFi代理恢复过来（选择自动即可），否则会影响手机上网 开启Charles录制功能当手机连接上代理后Charles会弹出相应的提示框，点击Allow即可。点击工具栏上的开始录制按钮，即启动了Charles的抓包功能了（录制按钮有的版本是默认开启的） 启动应用开始抓包此时只需启动你想抓包的APP就可以完成抓包，查看网络请求的数据了。因为现在大部分APP网络请求都做了加密处理，而Charles是不会进行自动解密的，所以会经常看到以下乱码。但是Charles作为测试时期的工具还是十分方便的。 三、获奖感言到此Charles简单的网络请求抓取已经完成。当然，如此强大的工具并不仅仅只有这一个功能，后续会陆续为大家带来Charles工具其他功能的使用讲解，敬请期待。","tags":[{"name":"Charles","slug":"Charles","permalink":"ly2513.github.com/tags/Charles/"},{"name":"APP","slug":"APP","permalink":"ly2513.github.com/tags/APP/"},{"name":"抓包","slug":"抓包","permalink":"ly2513.github.com/tags/抓包/"}]},{"title":"ELK6.0部署：Elasticsearch+Logstash+Kibana搭建分布式日志平台","date":"2018-01-04T14:45:24.000Z","path":"2018/01/04/ELK6-0部署：Elasticsearch-Logstash-Kibana搭建分布式日志平台/","text":"一、前言1、ELK简介 ELK是Elasticsearch + Logstash + Kibana的简称 ElasticSearch是一个基于Lucene的分布式全文搜索引擎，提供 RESTful API进行数据读写 Logstash是一个收集，处理和转发事件和日志消息的工具 Kibana是Elasticsearch的开源数据可视化插件，为查看存储在ElasticSearch提供了友好的Web界面，并提供了条形图，线条和散点图，饼图和地图等分析工具总的来说，ElasticSearch负责存储数据，Logstash负责收集日志，并将日志格式化后写入ElasticSearch，Kibana提供可视化访问ElasticSearch数据的功能。 2、ELK工作流 应用将日志按照约定的Key写入Redis，Logstash从Redis中读取日志信息写入ElasticSearch集群。Kibana读取ElasticSearch中的日志，并在Web页面中以表格/图表的形式展示。 二、准备工作1、服务器&amp;软件环境说明 服务器一共准备3台CentOS7 Server 服务器名 IP 说明 es1 192.168.1.31 部署ElasticSearch主节点 es2 192.168.1.32 部署ElasticSearch从节点 elk 192.168.1.21 部署Logstash + Kibana + Redis 这里为了节省，只部署2台Elasticsearch，并将Logstash + Kibana + Redis部署在了一台机器上。如果在生产环境部署，可以按照自己的需求调整。 软件环境 项 说明 Linux Server CentOS 7 Elasticsearch 6.0.0 Logstash 6.0.0 Kibana 6.0.0 Redis 4.0 JDK 1.8 2、ELK环境准备 由于Elasticsearch、Logstash、Kibana均不能以root账号运行。 但是Linux对非root账号可并发操作的文件、线程都有限制。 所以，部署ELK相关的机器都要调整： 修改文件限制 123456789# 修改系统文件vi /etc/security/limits.conf#增加的内容* soft nofile 65536* hard nofile 65536* soft nproc 2048* hard nproc 4096 调整进程数 123456#修改系统文件vi /etc/security/limits.d/20-nproc.conf#调整成以下配置* soft nproc 4096root soft nproc unlimited 调整虚拟内存&amp;最大并发连接 123456#修改系统文件vi /etc/sysctl.conf#增加的内容vm.max_map_count=655360fs.file-max=655360 保存之后执行 sysctl -p 生效 JDK8安装CentO安装JDK8：https://ken.io/note/centos-java-setup 创建ELK专用用户 1useradd elk 创建ELK相关目录并赋权 12345678#创建ELK APP目录mkdir /usr/elk#创建ELK 数据目录mkdir /elk#更改目录Ownerchown -R elk:elk /usr/elkchown -R elk:elk /elk 下载ELK包并解压https://www.elastic.co/downloads123456789101112#打开文件夹cd /home/download#下载wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.tar.gzwget https://artifacts.elastic.co/downloads/logstash/logstash-6.0.0.tar.gzwget wget https://artifacts.elastic.co/downloads/kibana/kibana-6.0.0-linux-x86_64.tar.gz#解压tar -zvxf elasticsearch-6.0.0.tar.gztar -zvxf logstash-6.0.0.tar.gztar -zvxf kibana-6.0.0-linux-x86_64.tar.gz 三、Elasticsearch 部署本次一共要部署两个Elasticsearch节点，所有文中没有指定机器的操作都表示每个Elasticsearch机器都要执行该操作 1、准备工作移动Elasticsearch到统一目录1234#移动目录mv /home/download/elasticsearch-6.0.0 /usr/elk#赋权chown -R elk:elk /usr/elk/elasticsearch-6.0.0/ 开放端口 123456#增加端口firewall-cmd --add-port=9200/tcp --permanentfirewall-cmd --add-port=9300/tcp --permanent#重新加载防火墙规则firewall-cmd --reload 切换账号 12#账号切换到 elksu - elk 数据&amp;日志目录 123456# 创建Elasticsearch主目录mkdir /elk/es#创建Elasticsearch数据目录mkdir /elk/es/data#创建Elasticsearch日志目录mkdir /elk/es/logs 2、Elasticsearch 配置 修改配置 12345#打开目录cd /usr/elk/elasticsearch-6.0.0#修改配置vi config/elasticsearch.yml 主节点配置（192.168.1.31） 1234567891011cluster.name: esnode.name: es1path.data: /elk/es/datapath.logs: /elk/es/logsnetwork.host: 192.168.1.31http.port: 9200transport.tcp.port: 9300node.master: truenode.data: truediscovery.zen.ping.unicast.hosts: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;]discovery.zen.minimum_master_nodes: 1 从节点配置（192.168.1.32） 1234567891011cluster.name: esnode.name: es2path.data: /elk/es/datapath.logs: /elk/es/logsnetwork.host: 192.168.1.32http.port: 9200transport.tcp.port: 9300node.master: falsenode.data: truediscovery.zen.ping.unicast.hosts: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;]discovery.zen.minimum_master_nodes: 1 配置项说明 项 说明 cluster.name 集群名 node.name 节点名 path.data 数据保存目录 path.logs 日志保存目录 network.host 节点host/ip http.port HTTP访问端口 transport.tcp.port TCP传输端口 node.master 是否允许作为主节点 node.data 是否保存数据 discovery.zen.ping.unicast.hosts 集群中的主节点的初始列表,当节点(主节点或者数据节点)启动时使用这个列表进行探测 discovery.zen.minimum_master_nodes 主节点个数 2、Elasticsearch启动&amp;健康检查 启动 1234#进入elasticsearch根目录cd /usr/elk/elasticsearch-6.0.0#启动./bin/elasticsearch 查看健康状态 1curl http://192.168.1.31:9200/_cluster/health 如果返回status=green表示正常1234567891011121314151617&#123; \"cluster_name\": \"esc\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 2, \"number_of_data_nodes\": 2, \"active_primary_shards\": 0, \"active_shards\": 0, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 100.0&#125; 四、Logstash 配置 1、准备工作 部署RedisRedis4 安装与配置：https://ken.io/note/centos7-redis4-setup由于本次核心是ELK搭建，所以ken.io偷懒，Redis没有部署集群，采用的单节点。 移动Logstash到统一目录 1234#移动目录mv /home/download/logstash-6.0.0 /usr/elk#赋权chown -R elk:elk /usr/elk/logstash-6.0.0/ 切换账号 12#账号切换到 elksu - elk 数据&amp;日志目录 123456#创建Logstash主目录mkdir /elk/logstash#创建Logstash数据目录mkdir /elk/logstash/data#创建Logstash日志目录mkdir /elk/logstash/logs 2、Logstash配置 配置数据&amp;日志目录 12345678#打开目录cd /usr/elk/logstash-6.0.0#修改配置vi config/logstash.yml#增加以下内容path.data: /elk/logstash/datapath.logs: /elk/logstash/logs 配置Redis&amp;Elasticsearch 12345678910111213141516171819202122232425vi config/input-output.conf#配置内容input &#123; redis &#123; data_type =&gt; &quot;list&quot; key =&gt; &quot;logstash&quot; host =&gt; &quot;192.168.1.21&quot; port =&gt; 6379 threads =&gt; 5 codec =&gt; &quot;json&quot; &#125;&#125;filter &#123;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;192.168.1.31:9200&quot;,&quot;192.168.1.32:9200&quot;] index =&gt; &quot;logstash-%&#123;type&#125;-%&#123;+YYYY.MM.dd&#125;&quot; document_type =&gt; &quot;%&#123;type&#125;&quot; &#125; stdout &#123; &#125;&#125; 该配置就是从redis中读取数据，然后写入指定的elasticsearch Redis核心配置项说明： 配置项 说明 data_type =&gt; “list” 数据类型为list key =&gt; “logstash” 缓存key为：logstash codec =&gt; “json” 数据格式为：json 启动1234#进入Logstash根目录cd /usr/elk/logstash-6.0.0#启动./bin/logstash -f config/input-output.conf 启动成功后，在启动输出的最后一行会看到如下信息：12[INFO ][logstash.pipeline ] Pipeline started &#123;&quot;pipeline.id&quot;=&gt;&quot;main&quot;&#125;[INFO ][logstash.agent ] Pipelines running &#123;:count=&gt;1, :pipelines=&gt;[&quot;main&quot;]&#125; 五、Kibana 配置 移动Kibana到统一目录 1234#移动目录mv /home/download/kibana-6.0.0-linux-x86_64 /usr/elk/kibana-6.0.0#赋权chown -R elk:elk /usr/elk/kibana-6.0.0/ 开放端口 12345#增加端口firewall-cmd --add-port=5601/tcp --permanent#重新加载防火墙规则firewall-cmd --reload 切换账号 12#账号切换到 elksu - elk 修改配置 123456789#进入kibana-6.0.0根目录cd /usr/elk/kibana-6.0.0#修改配置vi config/kibana.yml#增加以下内容server.port: 5601server.host: &quot;192.168.1.21&quot;elasticsearch.url: &quot;http://192.168.1.31:9200&quot; 启动 1234#进入kibana-6.0.0根目录cd /usr/elk/kibana-6.0.0#启动./bin/kibana 访问浏览器访问： 192.168.1.21:5601 警告提示：No default index pattern. You must select or create one to continue.错误提示：Unable to fetch mapping. do you have indices matching the pattern?不用担心，这是因为还没有写入日志 六、测试 1、日志写入日历写入的话，写入到logstash监听的redis即可。数据类型之前在/usr/elk/logstash-6.0.0/config/input-uput.conf中有配置 redis命令方式 12345678#启动redis客户端#执行以下命令lpush logstash &apos;&#123;&quot;host&quot;:&quot;127.0.0.1&quot;,&quot;type&quot;:&quot;logtest&quot;,&quot;message&quot;:&quot;hello&quot;&#125;&apos;Java代码批量写入（引入Jedis）Jedis jedis = new Jedis(&quot;192.168.1.21&quot;, 6379);for (int i = 0; i &lt; 1000; i++) &#123; jedis.lpush(&quot;logstash&quot;, &quot;&#123;\\&quot;host\\&quot;:\\&quot;127.0.0.1\\&quot;,\\&quot;type\\&quot;:\\&quot;logtest\\&quot;,\\&quot;message\\&quot;:\\&quot;&quot; + i + &quot;\\&quot;&#125;&quot;);&#125; 2、Kibana使用 浏览器访问：192.168.1.21:5601 此时会提示： Configure an index pattern 直接点击create即可 浏览器访问：192.168.1.21:5601/app/kibana#/discover 即可查看日志 大功告成！ 七、备注 1、Kibana使用教程https://segmentfault.com/a/1190000002972420 2、 ELK开机启动ELK开机启动，需要学习下以下知识 nohup命令使用：https://www.ibm.com/developerworks/cn/linux/l-cn-nohup/index.html 自定义系统服务，可以参考Redis的开机启动：https://ken.io/note/centos7-redis4-setup","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"ly2513.github.com/tags/elasticsearch/"},{"name":"logstash","slug":"logstash","permalink":"ly2513.github.com/tags/logstash/"},{"name":"kibana","slug":"kibana","permalink":"ly2513.github.com/tags/kibana/"},{"name":"分布式日志平台","slug":"分布式日志平台","permalink":"ly2513.github.com/tags/分布式日志平台/"}]},{"title":"每天掌握一个Linux命令(50) crontab命令","date":"2018-01-04T09:48:02.000Z","path":"2018/01/04/每天掌握一个Linux命令-50-crontab命令/","text":"前一天学习了 at 命令是针对仅运行一次的任务，循环运行的例行性计划任务，linux系统则是由 cron (crond) 这个系统服务来控制的。Linux 系统上面原本就有非常多的计划性工作，因此这个系统服务是默认启动的。另外, 由于使用者自己也可以设置计划任务，所以， Linux 系统也提供了使用者控制计划任务的命令 :crontab 命令。 一、crond简介crond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 Linux下的任务调度分为两类，系统任务调度和用户任务调度。 系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。 /etc/crontab文件包括下面几行： 12345678910[root@localhost ~]# cat /etc/crontabSHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=&quot;&quot;HOME=/# run-parts51 * * * * root run-parts /etc/cron.hourly24 7 * * * root run-parts /etc/cron.daily22 4 * * 0 root run-parts /etc/cron.weekly42 4 1 * * root run-parts /etc/cron.monthly[root@localhost ~]# 前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。第六至九行表示的含义将在下个小节详细讲述。这里不在多说。 用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用crontab工具来定制自己的计划任务。所有用户定义的crontab文件都被保存在/var/spool/cron目录中。其文件名与用户名一致。 使用者权限文件：文件：/etc/cron.deny说明：该文件中所列用户不允许使用crontab命令 文件：/etc/cron.allow说明：该文件中所列用户允许使用crontab命令文件：/var/spool/cron/说明：所有用户crontab文件存放的目录,以用户名命名 crontab文件的含义：用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： 1minute hour day month week command 其中：1234567891011minute： 表示分钟，可以是从0到59之间的任何整数。hour：表示小时，可以是从0到23之间的任何整数。day：表示日期，可以是从1到31之间的任何整数。month：表示月份，可以是从1到12之间的任何整数。week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符： 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 二、crond服务安装crontab：1yum install crontabs 服务操作说明：1234567/sbin/service crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置 查看crontab服务状态：1service crond status 手动启动crontab服务：1service crond start 查看crontab服务是否已设置为开机启动，执行命令：1ntsysv 加入开机自动启动：1chkconfig –level 35 crond on 三、crontab命令详解 1．命令格式：123crontab [-u user] filecrontab [-u user] [ -e | -l | -r ] 2．命令功能： 通过crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常设合周期性的日志分析或数据备份等工作。 3．命令参数：1234567891011-u user：用来设定某个用户的crontab服务，例如，“-u ixdba”表示设定ixdba用户的crontab服务，此参数一般有root用户来运行。file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。-r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。-i：在删除用户的crontab文件时给确认提示。 4．常用方法： 1). 创建一个新的crontab文件在考虑向cron进程提交一个crontab文件之前，首先要做的一件事情就是设置环境变量EDITOR。cron进程根据它来确定使用哪个编辑器编辑crontab文件。9 9 %的UNIX和LINUX用户都使用vi，如果你也是这样，那么你就编辑$ HOME目录下的.profile文件，在其中加入这样一行：1EDITOR=vi; export EDITOR 然后保存并退出。不妨创建一个名为&lt;user&gt; cron的文件，其中user&gt;是用户名，例如，davecron。在该文件中加入如下的内容。12345# (put your own initials here)echo the date to the console every# 15minutes between 6pm and 6am0,15,30,45 18-06 * * * /bin/echo &apos;date&apos; &gt; /dev/console 保存并退出。确信前面5个域用空格分隔。在上面的例子中，系统将每隔1 5分钟向控制台输出一次当前时间。如果系统崩溃或挂起，从最后所显示的时间就可以一眼看出系统是什么时间停止工作的。在有些系统中，用tty1来表示控制台，可以根据实际情况对上面的例子进行相应的修改。为了提交你刚刚创建的crontab文件，可以把这个新创建的文件作为cron命令的参数：1$ crontab davecron 现在该文件已经提交给cron进程，它将每隔1 5分钟运行一次。 同时，新创建文件的一个副本已经被放在/var/spool/cron目录中，文件名就是用户名(即dave)。 2). 列出crontab文件 为了列出crontab文件，可以用：123$ crontab -l0,15,30,45,18-06 * * * /bin/echo `date` &gt; dev/tty1 你将会看到和上面类似的内容。可以使用这种方法在$ H O M E目录中对crontab文件做一备份：1$ crontab -l &gt; $HOME/mycron 这样，一旦不小心误删了crontab文件，可以用上一节所讲述的方法迅速恢复。 3). 编辑crontab文件 如果希望添加、删除或编辑crontab文件中的条目，而EDITOR环境变量又设置为vi，那么就可以用vi来编辑crontab文件，相应的命令为：1$ crontab -e 可以像使用vi编辑其他任何文件那样修改crontab文件并退出。如果修改了某些条目或添加了新的条目，那么在保存该文件时，cron会对其进行必要的完整性检查。如果其中的某个域出现了超出允许范围的值，它会提示你。 我们在编辑crontab文件时，没准会加入新的条目。例如，加入下面的一条： 123# DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month30 3 1,7,14,21,26 * * /bin/find -name &quot;core&apos; -exec rm &#123;&#125; \\; 现在保存并退出。最好在crontab文件的每一个条目之上加入一条注释，这样就可以知道它的功能、运行时间，更为重要的是，知道这是哪位用户的作业。 现在让我们使用前面讲过的crontab -l命令列出它的全部信息：1234567891011$ crontab -l# (crondave installed on Tue May 4 13:07:43 1999)# DT:ech the date to the console every 30 minites0,15,30,45 18-06 * * * /bin/echo `date` &gt; /dev/tty1# DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month30 3 1,7,14,21,26 * * /bin/find -name &quot;core&apos; -exec rm &#123;&#125; \\; 4). 删除crontab文件 要删除crontab文件，可以用：1$ crontab -r 5). 恢复丢失的crontab文件 如果不小心误删了crontab文件，假设你在自己的$ HOME目录下还有一个备份，那么可以将其拷贝到/var/spool/cron/&lt;username&gt;，其中&lt;username&gt;是用户名。如果由于权限问题无法完成拷贝，可以用：1$ crontab &lt;filename&gt; 其中，&lt;filename&gt;是你在$ HOME目录中副本的文件名。 我建议你在自己的$ HOME目录中保存一个该文件的副本。我就有过类似的经历，有数次误删了crontab文件（因为r键紧挨在e键的右边）。这就是为什么有些系统文档建议不要直接编辑crontab文件，而是编辑该文件的一个副本，然后重新提交新的文件。 有些crontab的变体有些怪异，所以在使用crontab命令时要格外小心。如果遗漏了任何选项，crontab可能会打开一个空文件，或者看起来像是个空文件。这时敲delete键退出，不要按&lt;Ctrl-D&gt;，否则你将丢失crontab文件。 5．使用实例 实例一：每1分钟执行一次command命令：1* * * * * command 实例二：每小时的第3和第15分钟执行命令：13,15 * * * * command 实例三：在上午8点到11点的第3和第15分钟执行命令：13,15 8-11 * * * command 实例四：每隔两天的上午8点到11点的第3和第15分钟执行命令：13,15 8-11 */2 * * command 实例五：每个星期一的上午8点到11点的第3和第15分钟执行命令：13,15 8-11 * * 1 command 实例六：每晚的21:30重启smb命令：130 21 * * * /etc/init.d/smb restart 实例七：每月1、10、22日的4 : 45重启smb命令：145 4 1,10,22 * * /etc/init.d/smb restart 实例八：每周六、周日的1 : 10重启smb命令：110 1 * * 6,0 /etc/init.d/smb restart 实例九：每天18 : 00至23 : 00之间每隔30分钟重启smb命令：10,30 18-23 * * * /etc/init.d/smb restart 实例十：每星期六的晚上11 : 00 pm重启smb命令：10 23 * * 6 /etc/init.d/smb restart 实例十一：每一小时重启smb命令：1* */1 * * * /etc/init.d/smb restart 实例十二：晚上11点到早上7点之间，每隔一小时重启smb命令：1* 23-7/1 * * * /etc/init.d/smb restart 实例十三：每月的4号与每周一到周三的11点重启smb命令：10 11 4 * mon-wed /etc/init.d/smb restart 实例十四：一月一号的4点重启smb命令：10 4 1 jan * /etc/init.d/smb restart 实例十五：每小时执行/etc/cron.hourly目录内的脚本命令：101 * * * * root run-parts /etc/cron.hourly 说明： run-parts这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是目录名了 四、使用注意事项 1. 注意环境变量问题 有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。 在crontab文件中定义多个调度任务时，需要特别注意的一个问题就是环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shell脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点： 脚本中涉及文件路径时写全局路径； 脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如： 12345cat start_cbp.sh#!/bin/shsource /etc/profileexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; 当手动执行脚本OK，但是crontab死活不执行时。这时必须大胆怀疑是环境变量惹的祸，并可以尝试在crontab中直接引入环境变量解决问题。如： 10 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 2. 注意清理系统用户的邮件日志每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。例如，可以在crontab文件中设置如下形式，忽略日志输出：10 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 /dev/null 2&gt;&amp;1表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。 3. 系统级任务调度与用户级任务调度 系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行，root用户的任务调度操作可以通过crontab –uroot –e来设置，也可以将调度任务直接写入/etc/crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。 4. 其他注意事项 新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。 当crontab突然失效时，可以尝试/etc/init.d/crond restart解决问题。或者查看日志看某个job有没有执行/报错tail -f /var/log/cron。 千万别乱运行crontab -r。它从Crontab目录/var/spool/cron中删除用户的Crontab文件。删除了该用户的所有crontab都没了。 在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义\\%，如经常用的date ‘+%Y%m%d’在crontab里是不会执行的，应该换成date ‘+\\%Y\\%m\\%d’。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"crontab","slug":"crontab","permalink":"ly2513.github.com/tags/crontab/"}]},{"title":"每天掌握一个Linux命令(49) at命令","date":"2018-01-04T09:25:57.000Z","path":"2018/01/04/每天掌握一个Linux命令-49-at命令/","text":"在windows系统中，windows提供了计划任务这一功能，在控制面板 -&gt; 性能与维护 -&gt; 任务计划， 它的功能就是安排自动运行的任务。 通过’添加任务计划’的一步步引导，则可建立一个定时执行的任务。 在linux系统中你可能已经发现了为什么系统常常会自动的进行一些任务？这些任务到底是谁在支配他们工作的？在linux系统如果你想要让自己设计的备份程序可以自动在某个时间点开始在系统底下运行，而不需要手动来启动它，又该如何处置呢？ 这些例行的工作可能又分为一次性定时工作与循环定时工作，在系统内又是哪些服务在负责？ 还有，如果你想要每年在老婆的生日前一天就发出一封信件提醒自己不要忘记，linux系统下该怎么做呢？ 今天我们主要学习一下一次性定时计划任务的at命令的用法！ 1．命令格式： at[参数][时间] 2．命令功能： 在一个指定的时间执行一个指定任务，只能执行一次，且需要开启atd进程（ps -ef | grep atd查看，开启用/etc/init.d/atd start or restart； 开机即启动则需要运行chkconfig --level 2345 atd on）。 3．命令参数： 1234567891011121314151617-m 当指定的任务被完成之后，将给用户发送邮件，即使没有标准输出-I atq的别名-d atrm的别名-v 显示任务将被执行的时间-c 打印任务的内容到标准输出-V 显示版本信息-q&lt;列队&gt; 使用指定的列队-f&lt;文件&gt; 从指定文件读入任务而不是从标准输入读入-t&lt;时间参数&gt; 以时间参数的形式提交要运行的任务 at允许使用一套相当复杂的指定时间的方法。他能够接受在当天的hh:mm（小时:分钟）式的时间指定。假如该时间已过去，那么就放在第二天执行。当然也能够使用midnight（深夜），noon（中午），teatime（饮茶时间，一般是下午4点）等比较模糊的 词语来指定时间。用户还能够采用12小时计时制，即在时间后面加上AM（上午）或PM（下午）来说明是上午还是下午。 也能够指定命令执行的具体日期，指定格式为month day（月 日）或mm/dd/yy（月/日/年）或dd.mm.yy（日.月.年）。指定的日期必须跟在指定时间的后面。 上面介绍的都是绝对计时法，其实还能够使用相对计时法，这对于安排不久就要执行的命令是很有好处的。指定格式为：now + count time-units ，now就是当前时间，time-units是时间单位，这里能够是minutes（分钟）、hours（小时）、days（天）、weeks（星期）。count是时间的数量，究竟是几天，还是几小时，等等。 更有一种计时方法就是直接使用today（今天）、tomorrow（明天）来指定完成命令的时间。 TIME：时间格式，这里可以定义出什么时候要进行 at 这项任务的时间，格式有：12HH:MMex&gt; 04:00 在今日的 HH:MM 时刻进行，若该时刻已超过，则明天的 HH:MM 进行此任务。12HH:MM YYYY-MM-DDex&gt; 04:00 2009-03-17 强制规定在某年某月的某一天的特殊时刻进行该项任务12HH:MM[am|pm] [Month] [Date]ex&gt; 04pm March 17 也是一样，强制在某年某月某日的某时刻进行该项任务123HH:MM[am|pm] + number [minutes|hours|days|weeks]ex&gt; now + 5 minutesex&gt; 04pm + 3 days 就是说，在某个时间点再加几个时间后才进行该项任务。 4．使用实例： 实例一：三天后的下午 5 点锺执行 /bin/ls命令：1at 5pm+3 days 输出：12345[root@localhost ~]# at 5pm+3 daysat&gt; /bin/lsat&gt; &lt;EOT&gt;job 7 at 2013-01-08 17:00[root@localhost ~]# 实例二：明天17点钟，输出时间到指定文件内命令：1at 17:20 tomorrow 输出：12345[root@localhost ~]# at 17:20 tomorrowat&gt; date &gt;/root/2013.logat&gt; &lt;EOT&gt;job 8 at 2013-01-06 17:20[root@localhost ~]# 实例三：计划任务设定后，在没有执行之前我们可以用atq命令来查看系统没有执行工作任务命令：1atq 输出：1234[root@localhost ~]# atq8 2013-01-06 17:20 a root7 2013-01-08 17:00 a root[root@localhost ~]# 实例四：删除已经设置的任务命令：1atrm 7 输出：1234567[root@localhost ~]# atq8 2013-01-06 17:20 a root7 2013-01-08 17:00 a root[root@localhost ~]# atrm 7[root@localhost ~]# atq8 2013-01-06 17:20 a root[root@localhost ~]# 实例五：显示已经设置的任务内容命令：1at -c 8 输出：1234567[root@localhost ~]# at -c 8#!/bin/sh# atrun uid=0 gid=0# mail root 0umask 22此处省略n个字符date &gt;/root/2013.log[root@localhost ~]# 5．atd 的启动与 at 运行的方式： 5.1 atd 的启动 要使用一次性计划任务时，我们的 Linux 系统上面必须要有负责这个计划任务的服务，那就是 atd 服务。不过并非所有的 Linux distributions 都默认会把他打开的，所以某些时刻我们需要手动将atd服务激活才行。 激活的方法很简单，就是这样：命令：12/etc/init.d/atd start/etc/init.d/atd restart 输出：12345678910111213141516[root@localhost /]# /etc/init.d/atd start[root@localhost /]# /etc/init.d/atd用法：/etc/init.d/atd &#123;start|stop|restart|condrestart|status&#125;[root@localhost /]# /etc/init.d/atd stop停止 atd：[确定][root@localhost /]# ps -ef|grep atdroot 25062 24951 0 14:53 pts/0 00:00:00 grep atd[root@localhost /]# /etc/init.d/atd start[确定]td：[确定][root@localhost /]# ps -ef|grep atdroot 25068 1 0 14:53 ? 00:00:00 /usr/sbin/atdroot 25071 24951 0 14:53 pts/0 00:00:00 grep atd[root@localhost /]# /etc/init.d/atd restart停止 atd：[确定][确定]td：[确定][root@localhost /]# 说明：1234# 没有启动的时候，直接启动atd服务/etc/init.d/atd start# 服务已经启动后，重启atd服务/etc/init.d/atd restart 备注：配置一下启动时就启动这个服务，免得每次重新启动都得再来一次 命令：1chkconfig atd on 输出：12[root@localhost /]# chkconfig atd on[root@localhost /]# 5.2 at 的运行方式 既然是计划任务，那么应该会有任务执行的方式，并且将这些任务排进行程表中。那么产生计划任务的方式是怎么进行的? 事实上，我们使用at这个命令来产生所要运行的计划任务，并将这个计划任务以文字档的方式写入 /var/spool/at/ 目录内，该工作便能等待 atd 这个服务的取用与运行了。就这么简单。 不过，并不是所有的人都可以进行 at 计划任务。为什么? 因为系统安全的原因。很多主机被所谓的攻击破解后，最常发现的就是他们的系统当中多了很多的黑客程序， 这些程序非常可能运用一些计划任务来运行或搜集你的系统运行信息,并定时的发送给黑客。 所以，除非是你认可的帐号，否则先不要让他们使用 at 命令。那怎么达到使用 at 的可控呢? 我们可以利用 /etc/at.allow 与 /etc/at.deny 这两个文件来进行 at 的使用限制。加上这两个文件后， at 的工作情况是这样的： 先找寻 /etc/at.allow 这个文件，写在这个文件中的使用者才能使用 at ，没有在这个文件中的使用者则不能使用 at (即使没有写在 at.deny 当中); 如果 /etc/at.allow 不存在，就寻找 /etc/at.deny 这个文件，若写在这个 at.deny 的使用者则不能使用 at ，而没有在这个 at.deny 文件中的使用者，就可以使用 at 命令了。 如果两个文件都不存在，那么只有 root 可以使用 at 这个命令。 透过这个说明，我们知道 /etc/at.allow 是管理较为严格的方式，而 /etc/at.deny 则较为松散 (因为帐号没有在该文件中，就能够运行 at 了)。在一般的 distributions 当中，由于假设系统上的所有用户都是可信任的， 因此系统通常会保留一个空的 /etc/at.deny 文件，意思是允许所有人使用 at 命令的意思 (您可以自行检查一下该文件)。 不过，万一你不希望有某些使用者使用 at 的话，将那个使用者的帐号写入 /etc/at.deny 即可！一个帐号写一行。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"at","slug":"at","permalink":"ly2513.github.com/tags/at/"}]},{"title":"PHP中被忽略的性能优化利--生成器","date":"2018-01-03T03:30:59.000Z","path":"2018/01/03/PHP中被忽略的性能优化利--生成器/","text":"如果是做Python或者其他语言的小伙伴，对于生成器应该不陌生。但很多PHP开发者或许都不知道生成器这个功能，可能是因为生成器是PHP 5.5.0才引入的功能，也可以是生成器作用不是很明显。但是，生成器功能的确非常有用。 优点直接讲概念估计你听完还是一头雾水，所以我们先来说说优点，也许能勾起你的兴趣。那么生成器有哪些优点，如下： 生成器会对PHP应用的性能有非常大的影响 PHP代码运行时节省大量的内存 比较适合计算大量的数据 那么，这些神奇的功能究竟是如何做到的？我们先来举个例子。 概念引入首先，放下生成器概念的包袱，来看一个简单的PHP函数： 1234567function createRange($number)&#123; $data = []; for($i=0;$i&lt;$number;$i++)&#123; $data[] = time(); &#125; return $data;&#125; 这是一个非常常见的PHP函数，我们在处理一些数组的时候经常会使用。这里的代码也非常简单： 我们创建一个函数。 函数内包含一个for循环，我们循环的把当前时间放到$data里面 for循环执行完毕，把$data返回出去。 下面没完，我们继续。我们再写一个函数，把这个函数的返回值循环打印出来： 12345$result = createRange(10); // 这里调用上面我们创建的函数foreach($result as $value)&#123; sleep(1);//这里停顿1秒，我们后续有用 echo $value.'&lt;br /&gt;';&#125; 我们看一下运行结果： 这里非常完美，没有任何问题。（当然sleep(1)效果你们看不出来） 思考一个问题我们注意到，在调用函数createRange的时候给$number的传值是10，一个很小的数字。假设，现在传递一个值10000000（1000万）。 那么，在函数createRange里面，for循环就需要执行1000万次。且有1000万个值被放到$data里面，而$data数组在是被放在内存内。所以，在调用函数时候会占用大量内存。 这里，生成器就可以大显身手了。 创建生成器我们直接修改代码，你们注意观察： 12345function createRange($number)&#123; for($i=0;$i&lt;$number;$i++)&#123; yield time(); &#125;&#125; 看下这段和刚刚很像的代码，我们删除了数组$data，而且也没有返回任何内容，而是在time()之前使用了一个关键字yield 使用生成器我们再运行一下第二段代码： 12345$result = createRange(10); // 这里调用上面我们创建的函数foreach($result as $value)&#123; sleep(1); echo $value.'&lt;br /&gt;';&#125; 我们再看一下运行结果： 我们奇迹般的发现了，输出的值和第一次没有使用生成器的不一样。这里的值（时间戳）中间间隔了1秒。 这里的间隔一秒其实就是sleep(1)造成的后果。但是为什么第一次没有间隔？那是因为： 未使用生成器时：createRange函数内的for循环结果被很快放到$data中，并且立即返回。所以，foreach循环的是一个固定的数组。 使用生成器时：createRange的值不是一次性快速生成，而是依赖于foreach循环。foreach循环一次，for执行一次。 到这里，你应该对生成器有点儿头绪。 深入理解生成器代码剖析下面我们来对于刚刚的代码进行剖析。 1234567891011function createRange($number)&#123; for($i=0;$i&lt;$number;$i++)&#123; yield time(); &#125;&#125;$result = createRange(10); // 这里调用上面我们创建的函数foreach($result as $value)&#123; sleep(1); echo $value.'&lt;br /&gt;';&#125; 我们来还原一下代码执行过程。 首先调用createRange函数，传入参数10，但是for值执行了一次然后停止了，并且告诉foreach第一次循环可以用的值。 foreach开始对$result循环，进来首先sleep(1)，然后开始使用for给的一个值执行输出。 foreach准备第二次循环，开始第二次循环之前，它向for循环又请求了一次。 for循环于是又执行了一次，将生成的时间戳告诉foreach. foreach拿到第二个值，并且输出。由于foreach中sleep(1)，所以，for循环延迟了1秒生成当前时间 所以，整个代码执行中，始终只有一个记录值参与循环，内存中也只有一条信息。 无论开始传入的$number有多大，由于并不会立即生成所有结果集，所以内存始终是一条循环的值。 实际开发应用很多PHP开发者不了解生成器，其实主要是不了解应用领域。那么，生成器在实际开发中有哪些应用？ 读取超大文件PHP开发很多时候都要读取大文件，比如csv文件、text文件，或者一些日志文件。这些文件如果很大，比如5个G。这时，直接一次性把所有的内容读取到内存中计算不太现实。 这里生成器就可以派上用场啦。简单看个例子：读取text文件,以下是文件内容 12345678910111213第1行第2行第3行第4行第5行第6行第7行第8行第9行第10行第11行第12行第13行 我们创建一个text文本文档，并在其中输入几行文字，示范读取。 12345678910111213141516171819&lt;?phpheader(\"content-type:text/html;charset=utf-8\");function readTxt()&#123; # code... $handle = fopen(\"./test.txt\", 'rb'); while (feof($handle)===false) &#123; # code... yield fgets($handle); &#125; fclose($handle);&#125;foreach (readTxt() as $key =&gt; $value) &#123; # code... echo $value.'&lt;br /&gt;';&#125; 执行以上代码的结果如下图所示:","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"yield","slug":"yield","permalink":"ly2513.github.com/tags/yield/"}]},{"title":"PHP7下的协程实现","date":"2018-01-03T01:48:33.000Z","path":"2018/01/03/PHP7下的协程实现/","text":"写在最前面相信大家都听说过 协程 这个概念吧。 但是有些同学对这个概念似懂非懂，不知道怎么实现，怎么用，用在哪，甚至有些人认为 yield 就是协程！ 我始终相信，如果你无法准确地表达出一个知识点的话，我可以认为你就是不懂。 如果你之前了解过利用PHP实现协程的话，你肯定看过鸟哥的那篇文章：在PHP中使用协程实现多任务调度| 风雪之隅 鸟哥这篇文章是从国外的作者翻译来的，翻译的简洁明了，也给出了具体的例子了。 我写这篇文章的目的，是想对鸟哥文章做更加充足的补充，毕竟有部分同学的基础还是不够好，看得也是云头雾里的。 什么是协程先搞清楚，什么是协程。 你可能已经听过『进程』和『线程』这两个概念。 进程就是二进制可执行文件在计算机内存里的一个运行实例，就好比你的.exe文件是个类，进程就是new出来的那个实例。 进程是计算机系统进行资源分配和调度的基本单位（调度单位这里别纠结线程进程的），每个CPU下同一时刻只能处理一个进程。 所谓的并行，只不过是看起来并行，CPU事实上在用很快的速度切换不同的进程。 进程的切换需要进行系统调用，CPU要保存当前进程的各个信息，同时还会使CPUCache被废掉。 所以进程切换不到非不得已就不做。 那么怎么实现『进程切换不到非不得已就不做』呢？ 首先进程被切换的条件是：进程执行完毕、分配给进程的CPU时间片结束，系统发生中断需要处理，或者进程等待必要的资源（进程阻塞）等。你想下，前面几种情况自然没有什么话可说，但是如果是在阻塞等待，是不是就浪费了。 其实阻塞的话我们的程序还有其他可执行的地方可以执行，不一定要傻傻的等！ 所以就有了线程。 线程简单理解就是一个『微进程』，专门跑一个函数（逻辑流）。 所以我们就可以在编写程序的过程中将可以同时运行的函数用线程来体现了。 线程有两种类型，一种是由内核来管理和调度。 我们说，只要涉及需要内核参与管理调度的，代价都是很大的。这种线程其实也就解决了当一个进程中，某个正在执行的线程遇到阻塞，我们可以调度另外一个可运行的线程来跑，但是还是在同一个进程里，所以没有了进程切换。 还有另外一种线程，他的调度是由程序员自己写程序来管理的，对内核来说不可见。这种线程叫做 用户空间线程。 协程可以理解就是一种用户空间线程。 协程，有几个特点： 协同，因为是由程序员自己写的调度策略，其通过协作而不是抢占来进行切换 在用户态完成创建，切换和销毁 ⚠️ 从编程角度上看，协程的思想本质上就是控制流的主动让出（yield）和恢复（resume）机制 generator经常用来实现协程 说到这里，你应该明白协程的基本概念了吧？ PHP实现协程一步一步来，从解释概念说起！ 可迭代对象PHP5提供了一种定义对象的方法使其可以通过单元列表来遍历，例如用foreach语句。 你如果要实现一个可迭代对象，你就要实现Iterator接口： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?phpclass MyIterator implements Iterator&#123; private $var = array(); public function __construct($array) &#123; if (is_array($array)) &#123; $this-&gt;var = $array; &#125; &#125; public function rewind() &#123; echo \"rewinding\\n\"; reset($this-&gt;var); &#125; public function current() &#123; $var = current($this-&gt;var); echo \"current: $var\\n\"; return $var; &#125; public function key() &#123; $var = key($this-&gt;var); echo \"key: $var\\n\"; return $var; &#125; public function next() &#123; $var = next($this-&gt;var); echo \"next: $var\\n\"; return $var; &#125; public function valid() &#123; $var = $this-&gt;current() !== false; echo \"valid: &#123;$var&#125;\\n\"; return $var; &#125;&#125;$values = array(1,2,3);$it = new MyIterator($values);foreach ($it as $a =&gt; $b) &#123; print \"$a: $b\\n\";&#125; 生成器可以说之前为了拥有一个能够被foreach遍历的对象，你不得不去实现一堆的方法，yield关键字就是为了简化这个过程。 生成器提供了一种更容易的方法来实现简单的对象迭代，相比较定义类实现Iterator接口的方式，性能开销和复杂性大大降低。 12345678910&lt;?phpfunction xrange($start, $end, $step = 1) &#123; for ($i = $start; $i &lt;= $end; $i += $step) &#123; yield $i; &#125;&#125;foreach (xrange(1, 1000000) as $num) &#123; echo $num, \"\\n\";&#125; 记住，一个函数中如果用了yield，他就是一个生成器，直接调用他是没有用的，不能等同于一个函数那样去执行！ 所以，yield就是yield，下次谁再说yield是协程，我肯定把你xxxx。 PHP协程前面介绍协程的时候说了，协程需要程序员自己去编写调度机制，下面我们来看这个机制怎么写。 0）生成器正确使用既然生成器不能像函数一样直接调用，那么怎么才能调用呢？ 方法如下： foreach他 send($value) current / next… 1）Task实现Task就是一个任务的抽象，刚刚我们说了协程就是用户空间线程，线程可以理解就是跑一个函数。 所以Task的构造函数中就是接收一个闭包函数，我们命名为coroutine。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * Task任务类 */class Task&#123; protected $taskId; protected $coroutine; protected $beforeFirstYield = true; protected $sendValue; /** * Task constructor. * @param $taskId * @param Generator $coroutine */ public function __construct($taskId, Generator $coroutine) &#123; $this-&gt;taskId = $taskId; $this-&gt;coroutine = $coroutine; &#125; /** * 获取当前的Task的ID * * @return mixed */ public function getTaskId() &#123; return $this-&gt;taskId; &#125; /** * 判断Task执行完毕了没有 * * @return bool */ public function isFinished() &#123; return !$this-&gt;coroutine-&gt;valid(); &#125; /** * 设置下次要传给协程的值，比如 $id = (yield $xxxx)，这个值就给了$id了 * * @param $value */ public function setSendValue($value) &#123; $this-&gt;sendValue = $value; &#125; /** * 运行任务 * * @return mixed */ public function run() &#123; // 这里要注意，生成器的开始会reset，所以第一个值要用current获取 if ($this-&gt;beforeFirstYield) &#123; $this-&gt;beforeFirstYield = false; return $this-&gt;coroutine-&gt;current(); &#125; else &#123; // 我们说过了，用send去调用一个生成器 $retval = $this-&gt;coroutine-&gt;send($this-&gt;sendValue); $this-&gt;sendValue = null; return $retval; &#125; &#125;&#125; 2）Scheduler实现接下来就是Scheduler这个重点核心部分，他扮演着调度员的角色。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * Class Scheduler */Class Scheduler&#123; /** * @var SplQueue */ protected $taskQueue; /** * @var int */ protected $tid = 0; /** * Scheduler constructor. */ public function __construct() &#123; /* 原理就是维护了一个队列， * 前面说过，从编程角度上看，协程的思想本质上就是控制流的主动让出（yield）和恢复（resume）机制 * */ $this-&gt;taskQueue = new SplQueue(); &#125; /** * 增加一个任务 * * @param Generator $task * @return int */ public function addTask(Generator $task) &#123; $tid = $this-&gt;tid; $task = new Task($tid, $task); $this-&gt;taskQueue-&gt;enqueue($task); $this-&gt;tid++; return $tid; &#125; /** * 把任务进入队列 * * @param Task $task */ public function schedule(Task $task) &#123; $this-&gt;taskQueue-&gt;enqueue($task); &#125; /** * 运行调度器 */ public function run() &#123; while (!$this-&gt;taskQueue-&gt;isEmpty()) &#123; // 任务出队 $task = $this-&gt;taskQueue-&gt;dequeue(); $res = $task-&gt;run(); // 运行任务直到 yield if (!$task-&gt;isFinished()) &#123; $this-&gt;schedule($task); // 任务如果还没完全执行完毕，入队等下次执行 &#125; &#125; &#125;&#125; 这样我们基本就实现了一个协程调度器。 你可以使用下面的代码来测试： 12345678910111213141516171819&lt;?phpfunction task1() &#123; for ($i = 1; $i &lt;= 10; ++$i) &#123; echo \"This is task 1 iteration $i.\\n\"; yield; // 主动让出CPU的执行权 &#125;&#125;function task2() &#123; for ($i = 1; $i &lt;= 5; ++$i) &#123; echo \"This is task 2 iteration $i.\\n\"; yield; // 主动让出CPU的执行权 &#125;&#125;$scheduler = new Scheduler; // 实例化一个调度器$scheduler-&gt;addTask(task1()); // 添加不同的闭包函数作为任务$scheduler-&gt;addTask(task2());$scheduler-&gt;run(); 关键说下在哪里能用得到PHP协程。 123456789101112131415function task1() &#123; /* 这里有一个远程任务，需要耗时10s，可能是一个远程机器抓取分析远程网址的任务，我们只要提交最后去远程机器拿结果就行了 */ remote_task_commit(); // 这时候请求发出后，我们不要在这里等，主动让出CPU的执行权给task2运行，他不依赖这个结果 yield; yield (remote_task_receive()); ...&#125;function task2() &#123; for ($i = 1; $i &lt;= 5; ++$i) &#123; echo \"This is task 2 iteration $i.\\n\"; yield; // 主动让出CPU的执行权 &#125;&#125; 这样就提高了程序的执行效率。 关于『系统调用』的实现，鸟哥已经讲得很明白，我这里不再说明。 3）协程堆栈鸟哥文中还有一个协程堆栈的例子。 我们上面说过了，如果在函数中使用了yield，就不能当做函数使用。 所以你在一个协程函数中嵌套另外一个协程函数： 123456789101112131415161718&lt;?phpfunction echoTimes($msg, $max) &#123; for ($i = 1; $i &lt;= $max; ++$i) &#123; echo \"$msg iteration $i\\n\"; yield; &#125;&#125;function task() &#123; echoTimes('foo', 10); // print foo ten times echo \"---\\n\"; echoTimes('bar', 5); // print bar five times yield; // force it to be a coroutine&#125;$scheduler = new Scheduler;$scheduler-&gt;addTask(task());$scheduler-&gt;run(); 这里的echoTimes是执行不了的！所以就需要协程堆栈。 不过没关系，我们改一改我们刚刚的代码。 把Task中的初始化方法改下，因为我们在运行一个Task的时候，我们要分析出他包含了哪些子协程，然后将子协程用一个堆栈保存。（C语言学的好的同学自然能理解这里，不理解的同学我建议去了解下进程的内存模型是怎么处理函数调用） 123456789101112/** * Task constructor. * @param $taskId * @param Generator $coroutine */ public function __construct($taskId, Generator $coroutine) &#123; $this-&gt;taskId = $taskId; // $this-&gt;coroutine = $coroutine; // 换成这个，实际Task-&gt;run的就是stackedCoroutine这个函数，不是$coroutine保存的闭包函数了 $this-&gt;coroutine = stackedCoroutine($coroutine); &#125; 当Task-&gt;run()的时候，一个循环来分析：1234567891011121314151617181920212223242526272829303132333435/** * @param Generator $gen */function stackedCoroutine(Generator $gen)&#123; $stack = new SplStack; // 不断遍历这个传进来的生成器 for (; ;) &#123; // $gen可以理解为指向当前运行的协程闭包函数（生成器） $value = $gen-&gt;current(); // 获取中断点，也就是yield出来的值 if ($value instanceof Generator) &#123; // 如果是也是一个生成器，这就是子协程了，把当前运行的协程入栈保存 $stack-&gt;push($gen); $gen = $value; // 把子协程函数给gen，继续执行，注意接下来就是执行子协程的流程了 continue; &#125; // 我们对子协程返回的结果做了封装，下面讲 $isReturnValue = $value instanceof CoroutineReturnValue; // 子协程返回`$value`需要主协程帮忙处理 if (!$gen-&gt;valid() || $isReturnValue) &#123; if ($stack-&gt;isEmpty()) &#123; return; &#125; // 如果是gen已经执行完毕，或者遇到子协程需要返回值给主协程去处理 $gen = $stack-&gt;pop(); //出栈，得到之前入栈保存的主协程 $gen-&gt;send($isReturnValue ? $value-&gt;getValue() : NULL); // 调用主协程处理子协程的输出值 continue; &#125; $gen-&gt;send(yield $gen-&gt;key() =&gt; $value); // 继续执行子协程 &#125;&#125; 然后我们增加echoTime的结束标示：12345678910111213141516class CoroutineReturnValue &#123; protected $value; public function __construct($value) &#123; $this-&gt;value = $value; &#125; // 获取能把子协程的输出值给主协程，作为主协程的send参数 public function getValue() &#123; return $this-&gt;value; &#125;&#125;function retval($value) &#123; return new CoroutineReturnValue($value);&#125; 然后修改echoTimes： 1234567function echoTimes($msg, $max) &#123; for ($i = 1; $i &lt;= $max; ++$i) &#123; echo \"$msg iteration $i\\n\"; yield; &#125; yield retval(\"\"); // 增加这个作为结束标示&#125; Task变为： 1234function task1()&#123; yield echoTimes('bar', 5);&#125; 这样就实现了一个协程堆栈，现在你可以举一反三了。 4）PHP7中yield from关键字PHP7中增加了yield from，所以我们不需要自己实现携程堆栈，真是太好了。 把Task的构造函数改回去： 123456public function __construct($taskId, Generator $coroutine)&#123; $this-&gt;taskId = $taskId; $this-&gt;coroutine = $coroutine; // $this-&gt;coroutine = stackedCoroutine($coroutine); //不需要自己实现了，改回之前的&#125; echoTimes函数：123456function echoTimes($msg, $max) &#123; for ($i = 1; $i &lt;= $max; ++$i) &#123; echo \"$msg iteration $i\\n\"; yield; &#125;&#125; task1生成器： 1234function task1()&#123; yield from echoTimes('bar', 5);&#125; 这样，轻松调用子协程。 总结这下应该明白怎么实现PHP协程了吧？ 议不要使用PHP的Yield来实现协程，推荐使用swoole，2.0已经支持了协程，并附带了部分案例","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"协程","slug":"协程","permalink":"ly2513.github.com/tags/协程/"},{"name":"yield","slug":"yield","permalink":"ly2513.github.com/tags/yield/"},{"name":"Generator","slug":"Generator","permalink":"ly2513.github.com/tags/Generator/"}]},{"title":"每天掌握一个Linux命令(48) watch命令","date":"2018-01-02T15:57:56.000Z","path":"2018/01/02/每天掌握一个Linux命令-48-watch命令/","text":"watch是一个非常实用的命令，基本所有的Linux发行版都带有这个小工具，如同名字一样，watch可以帮你监测一个命令的运行结果，省得你一遍遍的手动运行。在Linux下，watch是周期性的执行下个程序，并全屏显示执行结果。你可以拿他来监测你想要的一切命令的结果变化，比如 tail 一个 log 文件，ls 监测某个文件的大小变化，看你的想象力了！ 1．命令格式： watch[参数][命令] 2．命令功能： 可以将命令的输出结果输出到标准输出设备，多用于周期性执行命令/定时执行命令 3．命令参数： -n或–interval watch缺省每2秒运行一下程序，可以用-n或-interval来指定间隔的时间。 -d或–differences 用-d或–differences 选项watch 会高亮显示变化的区域。 而-d=cumulative选项会把变动过的地方(不管最近的那次有没有变动)都高亮显示出来。 -t 或-no-title 会关闭watch命令在顶部的时间间隔,命令，当前时间的输出。 -h, –help 查看帮助文档 4．使用实例： 实例一：每隔一秒高亮显示网络链接数的变化情况命令：1watch -n 1 -d netstat -ant 说明： 其它操作：切换终端： Ctrl+x退出watch：Ctrl+g 实例二：每隔一秒高亮显示http链接数的变化情况命令：1watch -n 1 -d &apos;pstree|grep http&apos; 说明： 每隔一秒高亮显示http链接数的变化情况。 后面接的命令若带有管道符，需要加’’将命令区域归整。 实例三：实时查看模拟攻击客户机建立起来的连接数命令：1watch &apos;netstat -an | grep:21 | \\ grep&lt;模拟攻击客户机的IP&gt;| wc -l&apos; 实例四：监测当前目录中 scf’ 的文件的变化命令：1watch -d &apos;ls -l|grep scf&apos; 实例五：10秒一次输出系统的平均负载命令：1watch -n 10 &apos;cat /proc/loadavg&apos;","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"watch","slug":"watch","permalink":"ly2513.github.com/tags/watch/"}]},{"title":"每天掌握一个Linux命令(47) iostat命令","date":"2018-01-02T15:42:16.000Z","path":"2018/01/02/每天掌握一个Linux命令-47-iostat命令/","text":"Linux系统中的 iostat是I/O statistics（输入/输出统计）的缩写，iostat工具将对系统的磁盘操作活动进行监视。它的特点是汇报磁盘活动统计情况，同时也会汇报出CPU使用情况。同vmstat一样，iostat也有一个弱点，就是它不能对某个进程进行深入分析，仅对系统的整体情况进行分析。iostat属于sysstat软件包。可以用yum install sysstat 直接安装。 1．命令格式： iostat[参数][时间][次数] 2．命令功能： 通过iostat方便查看CPU、网卡、tty设备、磁盘、CD-ROM 等等设备的活动情况, 负载信息。 3．命令参数： 12345678910111213141516171819-C 显示CPU使用情况-d 显示磁盘使用情况-k 以 KB 为单位显示-m 以 M 为单位显示-N 显示磁盘阵列(LVM) 信息-n 显示NFS 使用情况-p[磁盘] 显示磁盘和分区的情况-t 显示终端和CPU的信息-x 显示详细信息-V 显示版本信息 4．使用实例： 实例一：显示所有设备负载情况命令：1iostat 输出：123456789101112131415[root@CT1186 ~]# iostatLinux 2.6.18-128.el5 (CT1186) 2012年12月28日avg-cpu: %user %nice %system %iowait %steal %idle 8.30 0.02 5.07 0.17 0.00 86.44Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 22.73 43.70 487.42 674035705 7517941952sda1 0.00 0.00 0.00 2658 536sda2 0.11 3.74 3.51 57721595 54202216sda3 0.98 0.61 17.51 9454172 270023368sda4 0.00 0.00 0.00 6 0sda5 6.95 0.12 108.73 1924834 1677123536sda6 2.20 0.18 31.22 2837260 481488056sda7 12.48 39.04 326.45 602094508 5035104240 说明： cpu属性值说明： %user：CPU处在用户模式下的时间百分比。 %nice：CPU处在带NICE值的用户模式下的时间百分比。 %system：CPU处在系统模式下的时间百分比。 %iowait：CPU等待输入输出完成时间的百分比。 %steal：管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。 %idle：CPU空闲时间百分比。 备注：如果%iowait的值过高，表示硬盘存在I/O瓶颈，%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。%idle值如果持续低于10，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。 disk属性值说明： rrqm/s: 每秒进行 merge 的读操作数目。即 rmerge/s wrqm/s: 每秒进行 merge 的写操作数目。即 wmerge/s r/s: 每秒完成的读 I/O 设备次数。即 rio/s w/s: 每秒完成的写 I/O 设备次数。即 wio/s rsec/s: 每秒读扇区数。即 rsect/s wsec/s: 每秒写扇区数。即 wsect/s rkB/s: 每秒读K字节数。是 rsect/s 的一半，因为每扇区大小为512字节。 wkB/s: 每秒写K字节数。是 wsect/s 的一半。 avgrq-sz: 平均每次设备I/O操作的数据大小 (扇区)。 avgqu-sz: 平均I/O队列长度。 await: 平均每次设备I/O操作的等待时间 (毫秒)。 svctm: 平均每次设备I/O操作的服务时间 (毫秒)。 %util: 一秒中有百分之多少的时间用于 I/O 操作，即被io消耗的cpu百分比 备注：如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明I/O 队列太长，io响应太慢，则需要进行必要优化。如果avgqu-sz比较大，也表示有当量io在等待。 实例二：定时显示所有信息命令：1iostat 2 3 输出：1234567891011121314151617181920212223242526272829303132333435363738394041[root@CT1186 ~]# iostat 2 3Linux 2.6.18-128.el5 (CT1186) 2012年12月28日avg-cpu: %user %nice %system %iowait %steal %idle 8.30 0.02 5.07 0.17 0.00 86.44Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 22.73 43.70 487.42 674035705 7517947296sda1 0.00 0.00 0.00 2658 536sda2 0.11 3.74 3.51 57721595 54202216sda3 0.98 0.61 17.51 9454172 270023608sda4 0.00 0.00 0.00 6 0sda5 6.95 0.12 108.73 1924834 1677125640sda6 2.20 0.18 31.22 2837260 481488152sda7 12.48 39.04 326.44 602094508 5035107144avg-cpu: %user %nice %system %iowait %steal %idle 8.88 0.00 7.94 0.19 0.00 83.00Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 6.00 0.00 124.00 0 248sda1 0.00 0.00 0.00 0 0sda2 0.00 0.00 0.00 0 0sda3 0.00 0.00 0.00 0 0sda4 0.00 0.00 0.00 0 0sda5 0.00 0.00 0.00 0 0sda6 0.00 0.00 0.00 0 0sda7 6.00 0.00 124.00 0 248avg-cpu: %user %nice %system %iowait %steal %idle 9.12 0.00 7.81 0.00 0.00 83.07Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda 4.00 0.00 84.00 0 168sda1 0.00 0.00 0.00 0 0sda2 0.00 0.00 0.00 0 0sda3 0.00 0.00 0.00 0 0sda4 0.00 0.00 0.00 0 0sda5 0.00 0.00 0.00 0 0sda6 4.00 0.00 84.00 0 168sda7 0.00 0.00 0.00 0 0 说明： 每隔 2秒刷新显示，且显示3次 实例三：显示指定磁盘信息命令：1iostat -d sda1 输出：12345[root@CT1186 ~]# iostat -d sda1Linux 2.6.18-128.el5 (CT1186) 2012年12月28日Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnsda1 0.00 0.00 0.00 2658 536 说明： 实例4：显示tty和Cpu信息命令：1iostat -t 输出： 实例五：以M为单位显示所有信息命令：1iostat -m 输出： 说明： 实例六：查看TPS和吞吐量信息命令：1iostat -d -k 1 1 输出： 说明： tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。“一次传输”意思是“一次I/O请求”。多个逻辑请求可能会被合并为“一次I/O请求”。“一次传输”请求的大小是未知的。 kB_read/s：每秒从设备（drive expressed）读取的数据量； kB_wrtn/s：每秒向设备（drive expressed）写入的数据量； kB_read：读取的总数据量；kB_wrtn：写入的总数量数据量； 这些单位都为Kilobytes。 上面的例子中，我们可以看到磁盘sda以及它的各个分区的统计数据，当时统计的磁盘总TPS是22.73，下面是各个分区的TPS。（因为是瞬间值，所以总TPS并不严格等于各个分区TPS的总和） 实例七：查看设备使用率（%util）、响应时间（await）命令：1iostat -d -x -k 1 1 输出： 说明： rrqm/s： 每秒进行 merge 的读操作数目.即 delta(rmerge)/s wrqm/s： 每秒进行 merge 的写操作数目.即 delta(wmerge)/s r/s： 每秒完成的读 I/O 设备次数.即 delta(rio)/s w/s： 每秒完成的写 I/O 设备次数.即 delta(wio)/s rsec/s： 每秒读扇区数.即 delta(rsect)/s wsec/s： 每秒写扇区数.即 delta(wsect)/s rkB/s： 每秒读K字节数.是 rsect/s 的一半,因为每扇区大小为512字节.(需要计算) wkB/s： 每秒写K字节数.是 wsect/s 的一半.(需要计算) avgrq-sz：平均每次设备I/O操作的数据大小 (扇区).delta(rsect+wsect)/delta(rio+wio) avgqu-sz：平均I/O队列长度.即 delta(aveq)/s/1000 (因为aveq的单位为毫秒). await： 平均每次设备I/O操作的等待时间 (毫秒).即 delta(ruse+wuse)/delta(rio+wio) svctm： 平均每次设备I/O操作的服务时间 (毫秒).即 delta(use)/delta(rio+wio) %util： 一秒中有百分之多少的时间用于 I/O 操作,或者说一秒中有多少时间 I/O 队列是非空的，即 delta(use)/s/1000 (因为use的单位为毫秒) 如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。 idle小于70% IO压力就较大了，一般读取速度有较多的wait。 同时可以结合vmstat 查看查看b参数(等待资源的进程数)和wa参数(IO等待所占用的CPU时间的百分比，高过30%时IO压力高)。 另外 await 的参数也要多和 svctm 来参考。差的过高就一定有 IO 的问题。 avgqu-sz 也是个做 IO 调优时需要注意的地方，这个就是直接每次操作的数据的大小，如果次数多，但数据拿的小的话，其实 IO 也会很小。如果数据拿的大，才IO 的数据会高。也可以通过 avgqu-sz × ( r/s or w/s ) = rsec/s or wsec/s。也就是讲，读定速度是这个来决定的。 svctm 一般要小于 await (因为同时等待的请求的等待时间被重复计算了)，svctm 的大小一般和磁盘性能有关，CPU/内存的负荷也会对其有影响，请求过多也会间接导致 svctm 的增加。await 的大小一般取决于服务时间(svctm) 以及 I/O 队列的长度和 I/O 请求的发出模式。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明 I/O 队列太长，应用得到的响应时间变慢，如果响应时间超过了用户可以容许的范围，这时可以考虑更换更快的磁盘，调整内核 elevator 算法，优化应用，或者升级 CPU。 队列长度(avgqu-sz)也可作为衡量系统 I/O 负荷的指标，但由于 avgqu-sz 是按照单位时间的平均值，所以不能反映瞬间的 I/O 洪水。 形象的比喻： r/s+w/s 类似于交款人的总数 平均队列长度(avgqu-sz)类似于单位时间里平均排队人的个数 平均服务时间(svctm)类似于收银员的收款速度 平均等待时间(await)类似于平均每人的等待时间 平均I/O数据(avgrq-sz)类似于平均每人所买的东西多少 I/O 操作率 (%util)类似于收款台前有人排队的时间比例 设备IO操作:总IO(io)/s = r/s(读) +w/s(写) =1.46 + 25.28=26.74 平均每次设备I/O操作只需要0.36毫秒完成,现在却需要10.57毫秒完成，因为发出的 请求太多(每秒26.74个)，假如请求时同时发出的，可以这样计算平均等待时间: 平均等待时间=单个I/O服务器时间*(1+2+…+请求总数-1)/请求总数 每秒发出的I/0请求很多,但是平均队列就4,表示这些请求比较均匀,大部分处理还是比较及时。 实例八：查看cpu状态命令：1iostat -c 1 3 输出：","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"iostat","slug":"iostat","permalink":"ly2513.github.com/tags/iostat/"}]},{"title":"每天掌握一个Linux命令(46) vmstat命令","date":"2018-01-02T15:22:09.000Z","path":"2018/01/02/每天掌握一个Linux命令-46-vmstat命令/","text":"vmstat是Virtual Meomory Statistics（虚拟内存统计）的缩写，可对操作系统的虚拟内存、进程、CPU活动进行监控。他是对系统的整体情况进行统计，不足之处是无法对某个进程进行深入分析。vmstat 工具提供了一种低开销的系统性能观察方式。因为 vmstat 本身就是低开销工具，在非常高负荷的服务器上，你需要查看并监控系统的健康情况,在控制窗口还是能够使用vmstat 输出结果。在学习vmstat命令前，我们先了解一下Linux系统中关于物理内存和虚拟内存相关信息。 物理内存和虚拟内存区别： 我们知道，直接从物理内存读写数据要比从硬盘读写数据要快的多，因此，我们希望所有数据的读取和写入都在内存完成，而内存是有限的，这样就引出了物理内存与虚拟内存的概念。物理内存就是系统硬件提供的内存大小，是真正的内存，相对于物理内存，在linux下还有一个虚拟内存的概念，虚拟内存就是为了满足物理内存的不足而提出的策略，它是利用磁盘空间虚拟出的一块逻辑内存，用作虚拟内存的磁盘空间被称为交换空间（Swap Space）。作为物理内存的扩展，linux会在物理内存不足时，使用交换分区的虚拟内存，更详细的说，就是内核会将暂时不用的内存块信息写到交换空间，这样以来，物理内存得到了释放，这块内存就可以用于其它目的，当需要用到原始的内容时，这些信息会被重新从交换空间读入物理内存。linux的内存管理采取的是分页存取机制，为了保证物理内存能得到充分的利用，内核会在适当的时候将物理内存中不经常使用的数据块自动交换到虚拟内存中，而将经常使用的信息保留到物理内存。要深入了解linux内存运行机制，需要知道下面提到的几个方面： 首先，Linux系统会不时的进行页面交换操作，以保持尽可能多的空闲物理内存，即使并没有什么事情需要内存，Linux也会交换出暂时不用的内存页面。这可以避免等待交换所需的时间。 其次，linux进行页面交换是有条件的，不是所有页面在不用时都交换到虚拟内存，linux内核根据”最近最经常使用“算法，仅仅将一些不经常使用的页面文件交换到虚拟内存，有时我们会看到这么一个现象：linux物理内存还有很多，但是交换空间也使用了很多。其实，这并不奇怪，例如，一个占用很大内存的进程运行时，需要耗费很多内存资源，此时就会有一些不常用页面文件被交换到虚拟内存中，但后来这个占用很多内存资源的进程结束并释放了很多内存时，刚才被交换出去的页面文件并不会自动的交换进物理内存，除非有这个必要，那么此刻系统物理内存就会空闲很多，同时交换空间也在被使用，就出现了刚才所说的现象了。关于这点，不用担心什么，只要知道是怎么一回事就可以了。 最后，交换空间的页面在使用时会首先被交换到物理内存，如果此时没有足够的物理内存来容纳这些页面，它们又会被马上交换出去，如此以来，虚拟内存中可能没有足够空间来存储这些交换页面，最终会导致linux出现假死机、服务异常等问题，linux虽然可以在一段时间内自行恢复，但是恢复后的系统已经基本不可用了。 因此，合理规划和设计linux内存的使用，是非常重要的。 虚拟内存原理： 在系统中运行的每个进程都需要使用到内存，但不是每个进程都需要每时每刻使用系统分配的内存空间。当系统运行所需内存超过实际的物理内存，内核会释放某些进程所占用但未使用的部分或所有物理内存，将这部分资料存储在磁盘上直到进程下一次调用，并将释放出的内存提供给有需要的进程使用。 在Linux内存管理中，主要是通过“调页Paging”和“交换Swapping”来完成上述的内存调度。调页算法是将内存中最近不常使用的页面换到磁盘上，把活动页面保留在内存中供进程使用。交换技术是将整个进程，而不是部分页面，全部交换到磁盘上。 分页(Page)写入磁盘的过程被称作Page-Out，分页(Page)从磁盘重新回到内存的过程被称作Page-In。当内核需要一个分页时，但发现此分页不在物理内存中(因为已经被Page-Out了)，此时就发生了分页错误（Page Fault）。 当系统内核发现可运行内存变少时，就会通过Page-Out来释放一部分物理内存。经管Page-Out不是经常发生，但是如果Page-out频繁不断的发生，直到当内核管理分页的时间超过运行程式的时间时，系统效能会急剧下降。这时的系统已经运行非常慢或进入暂停状态，这种状态亦被称作thrashing(颠簸)。 1．命令格式： 12345678910111213vmstat [-a] [-n] [-S unit] [delay [ count]]vmstat [-s] [-n] [-S unit]vmstat [-m] [-n] [delay [ count]]vmstat [-d] [-n] [delay [ count]]vmstat [-p disk partition] [-n] [delay [ count]]vmstat [-f]vmstat [-V] 2．命令功能： 用来显示虚拟内存的信息 3．命令参数： 123456789101112131415161718192021-a：显示活跃和非活跃内存-f：显示从系统启动至今的fork数量 。-m：显示slabinfo-n：只在开始时显示一次各字段名称。-s：显示内存相关统计信息及多种系统活动数量。delay：刷新时间间隔。如果不指定，只显示一条结果。count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷。-d：显示磁盘相关统计信息。-p：显示指定磁盘分区统计信息-S：使用指定单位显示。参数有 k 、K 、m 、M ，分别代表1000、1024、1000000、1048576字节（byte）。默认单位为K（1024 bytes）-V：显示vmstat版本信息。 4．使用实例： 实例一：显示虚拟内存使用情况命令：1vmstat 输出：12345678910[root@localhost ~]# vmstat 5 6procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 3029876 199616 690980 0 0 0 2 3 2 0 0 100 0 0 0 0 0 3029752 199616 690980 0 0 0 41 1009 39 0 0 100 0 0 0 0 0 3029752 199616 690980 0 0 0 3 1004 36 0 0 100 0 0 0 0 0 3029752 199616 690980 0 0 0 4 1004 36 0 0 100 0 0 0 0 0 3029752 199616 690980 0 0 0 6 1003 33 0 0 100 0 0 0 0 0 3029752 199616 690980 0 0 0 5 1003 33 0 0 100 0 0 说明： 字段说明： Procs（进程）： r: 运行队列中进程数量 b: 等待IO的进程数量 Memory（内存）： swpd: 使用虚拟内存大小 free: 可用内存大小 buff: 用作缓冲的内存大小 cache: 用作缓存的内存大小 Swap： si: 每秒从交换区写到内存的大小 so: 每秒写入交换区的内存大小 IO：（现在的Linux版本块的大小为1024bytes） bi: 每秒读取的块数 bo: 每秒写入的块数 系统： in: 每秒中断数，包括时钟中断。 cs: 每秒上下文切换数。 CPU（以百分比表示）： us: 用户进程执行时间(user time) sy: 系统进程执行时间(system time) id: 空闲时间(包括IO等待时间),中央处理器的空闲时间 。以百分比表示。 wa: 等待IO时间 备注： 如果 r经常大于 4 ，且id经常少于40，表示cpu的负荷很重。如果pi，po 长期不等于0，表示内存不足。如果disk 经常不等于0， 且在 b中的队列 大于3， 表示 io性能不好。Linux在具有高稳定性、可靠性的同时，具有很好的可伸缩性和扩展性，能够针对不同的应用和硬件环境调整，优化出满足当前应用需要的最佳性能。因此企业在维护Linux系统、进行系统调优时，了解系统性能分析工具是至关重要的。 命令：1vmstat 5 5 表示在5秒时间内进行5次采样。将得到一个数据汇总他能够反映真正的系统情况。 实例二：显示活跃和非活跃内存命令：1vmstat -a 2 5 输出：12345678910[root@localhost ~]# vmstat -a 2 5procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free inact active si so bi bo in cs us sy id wa st 0 0 0 3029752 387728 513008 0 0 0 2 3 2 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1005 34 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 22 1004 36 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1004 33 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1003 32 0 0 100 0 0[root@localhost ~]# 说明： 使用-a选项显示活跃和非活跃内存时，所显示的内容除增加inact和active外，其他显示内容与例子1相同。 字段说明： Memory（内存）： inact: 非活跃内存大小（当使用-a选项时显示） active: 活跃的内存大小（当使用-a选项时显示） 实例三：查看系统已经fork了多少次命令：1vmstat -f 输出：12345[root@SCF1129 ~]# vmstat -f 12744849 forks[root@SCF1129 ~]# 说明： 这个数据是从/proc/stat中的processes字段里取得的 实例四：查看内存使用的详细信息命令：1vmstat -s 输出：123456789101112131415161718192021222324252627[root@localhost ~]# vmstat -s 4043760 total memory 1013884 used memory 513012 active memory 387728 inactive memory 3029876 free memory 199616 buffer memory 690980 swap cache 6096656 total swap 0 used swap 6096656 free swap 83587 non-nice user cpu ticks 132 nice user cpu ticks 278599 system cpu ticks 913344692 idle cpu ticks 814550 IO-wait cpu ticks 10547 IRQ cpu ticks 21261 softirq cpu ticks 0 stolen cpu ticks 310215 pages paged in 14254652 pages paged out 0 pages swapped in 0 pages swapped out 288374745 interrupts 146680577 CPU context switches 1351868832 boot time 367291 forks 说明： 这些信息的分别来自于/proc/meminfo,/proc/stat和/proc/vmstat。 实例五：查看磁盘的读/写命令：1vmstat -d 输出： 说明： 这些信息主要来自于/proc/diskstats. merged:表示一次来自于合并的写/读请求,一般系统会把多个连接/邻近的读/写请求合并到一起来操作. 实例六：查看/dev/sda1磁盘的读/写命令： 输出：1234567891011121314151617[root@SCF1129 ~]# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda3 1119336548 27642068 1034835500 3% /tmpfs 32978376 0 32978376 0% /dev/shm/dev/sda1 1032088 59604 920056 7% /boot[root@SCF1129 ~]# vmstat -p /dev/sda1sda1 reads read sectors writes requested writes 18607 4249978 6 48[root@SCF1129 ~]# vmstat -p /dev/sda3sda3 reads read sectors writes requested writes 429350 35176268 28998789 980301488[root@SCF1129 ~]# 说明： 这些信息主要来自于/proc/diskstats。 reads:来自于这个分区的读的次数。 read sectors:来自于这个分区的读扇区的次数。 writes:来自于这个分区的写的次数。 requested writes:来自于这个分区的写请求次数。 实例七：查看系统的slab信息命令：1vmstat -m 输出： 这组信息来自于/proc/slabinfo。 slab:由于内核会有许多小对象，这些对象构造销毁十分频繁，比如i-node，dentry，这些对象如果每次构建的时候就向内存要一个页(4kb)，而其实只有几个字节，这样就会非常浪费，为了解决这个问题，就引入了一种新的机制来处理在同一个页框中如何分配小存储区，而slab可以对小对象进行分配,这样就不用为每一个对象分配页框，从而节省了空间，内核对一些小对象创建析构很频繁，slab对这些小对象进行缓冲,可以重复利用,减少内存分配次数。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"vmstat","slug":"vmstat","permalink":"ly2513.github.com/tags/vmstat/"}]},{"title":"每天掌握一个Linux命令(45) free 命令","date":"2018-01-02T14:55:35.000Z","path":"2018/01/02/每天掌握一个Linux命令-45-free命令/","text":"free命令可以显示Linux系统中空闲的、已用的物理内存及swap内存,及被内核使用的buffer。在Linux系统监控的工具中，free命令是最经常使用的命令之一。 1．命令格式： free [参数] 2．命令功能： free 命令显示系统使用和空闲的内存情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。共享内存将被忽略 3．命令参数： 123456789101112131415-b 以Byte为单位显示内存使用情况。-k 以KB为单位显示内存使用情况。-m 以MB为单位显示内存使用情况。-g 以GB为单位显示内存使用情况。-o 不显示缓冲区调节列。-s&lt;间隔秒数&gt; 持续观察内存使用状况。-t 显示内存总和列。-V 显示版本信息。 4．使用实例： 实例一：显示内存使用情况命令：12345freefree -gfree -m 输出： 1234567891011121314151617181920212223242526272829[root@SF1150 service]# free total used free shared buffers cachedMem: 32940112 30841684 2098428 0 4545340 11363424-/+ buffers/cache: 14932920 18007192Swap: 32764556 1944984 30819572[root@SF1150 service]# free -g total used free shared buffers cachedMem: 31 29 2 0 4 10-/+ buffers/cache: 14 17Swap: 31 1 29[root@SF1150 service]# free -m total used free shared buffers cachedMem: 32168 30119 2048 0 4438 11097-/+ buffers/cache: 14583 17584Swap: 31996 1899 30097 说明： 下面是对这些数值的解释： total:总计物理内存的大小。 used:已使用多大。 free:可用有多少。 Shared:多个进程共享的内存总额。 Buffers/cached:磁盘缓存的大小。 第三行(-/+ buffers/cached): used:已使用多大。 free:可用有多少。 第四行是交换分区SWAP的，也就是我们通常所说的虚拟内存。 区别：第二行(mem)的used/free与第三行(-/+ buffers/cache) used/free的区别。 这两个的区别在于使用的角度来看，第一行是从OS的角度来看，因为对于OS，buffers/cached 都是属于被使用，所以他的可用内存是2098428KB,已用内存是30841684KB,其中包括，内核（OS）使用+Application(X, oracle,etc)使用的+buffers+cached. 第三行所指的是从应用程序角度来看，对于应用程序来说，buffers/cached 是等于可用的，因为buffer/cached是为了提高文件读取的性能，当应用程序需在用到内存的时候，buffer/cached会很快地被回收。 所以从应用程序的角度来说，可用内存=系统free memory+buffers+cached。 如本机情况的可用内存为： 118007156=2098428KB+4545340KB+11363424KB 接下来解释什么时候内存会被交换，以及按什么方交换。 当可用内存少于额定值的时候，就会开会进行交换.如何看额定值： 命令：1cat /proc/meminfo 输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@SF1150 service]# cat /proc/meminfoMemTotal: 32940112 kBMemFree: 2096700 kBBuffers: 4545340 kBCached: 11364056 kBSwapCached: 1896080 kBActive: 22739776 kBInactive: 7427836 kBHighTotal: 0 kBHighFree: 0 kBLowTotal: 32940112 kBLowFree: 2096700 kBSwapTotal: 32764556 kBSwapFree: 30819572 kBDirty: 164 kBWriteback: 0 kBAnonPages: 14153592 kBMapped: 20748 kBSlab: 590232 kBPageTables: 34200 kBNFS_Unstable: 0 kBBounce: 0 kBCommitLimit: 49234612 kBCommitted_AS: 23247544 kBVmallocTotal: 34359738367 kBVmallocUsed: 278840 kBVmallocChunk: 34359459371 kBHugePages_Total: 0HugePages_Free: 0HugePages_Rsvd: 0Hugepagesize: 2048 kB 交换将通过三个途径来减少系统中使用的物理页面的个数： 1.减少缓冲与页面cache的大小， 2.将系统V类型的内存页面交换出去， 3.换出或者丢弃页面。(Application 占用的内存页，也就是物理内存不足）。 事实上，少量地使用swap是不是影响到系统性能的。 那buffers和cached都是缓存，两者有什么区别呢？ 为了提高磁盘存取效率, Linux做了一些精心的设计, 除了对dentry进行缓存(用于VFS,加速文件路径名到inode的转换), 还采取了两种主要Cache方式： Buffer Cache和Page Cache。前者针对磁盘块的读写，后者针对文件inode的读写。这些Cache有效缩短了 I/O系统调用(比如read,write,getdents)的时间。 磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。 Page cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。文件的逻辑层需要映射到实际的物理磁盘，这种映射关系由文件系统来完成。当page cache的数据需要刷新时，page cache中的数据交给buffer cache，因为Buffer Cache就是缓存磁盘块的。但是这种处理在2.6版本的内核之后就变的很简单了，没有真正意义上的cache操作。 Buffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中，例如，文件系统的元数据都会缓存到buffer cache中。 简单说来，page cache用来缓存文件数据，buffer cache用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。 所以我们看linux,只要不用swap的交换空间,就不用担心自己的内存太少.如果常常swap用很多,可能你就要考虑加物理内存了.这也是linux看内存是否够用的标准. 如果是应用服务器的话，一般只看第二行，+buffers/cache,即对应用程序来说free的内存太少了，也是该考虑优化程序或加内存了。 实例二：以总和的形式显示内存的使用信息命令：1free -t 输出：1234567[root@SF1150 service]# free -t total used free shared buffers cachedMem: 32940112 30845024 2095088 0 4545340 11364324-/+ buffers/cache: 14935360 18004752Swap: 32764556 1944984 30819572Total: 65704668 32790008 32914660[root@SF1150 service]# 实例三：周期性的查询内存使用信息 命令：1free -s 10 输出：12345678910111213[root@SF1150 service]# free -s 10 total used free shared buffers cachedMem: 32940112 30844528 2095584 0 4545340 11364380-/+ buffers/cache: 14934808 18005304Swap: 32764556 1944984 30819572 total used free shared buffers cachedMem: 32940112 30843932 2096180 0 4545340 11364388-/+ buffers/cache: 14934204 18005908Swap: 32764556 1944984 30819572 说明： 每10s 执行一次命令","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"free","slug":"free","permalink":"ly2513.github.com/tags/free/"}]},{"title":"Mysql数据库30条规范","date":"2018-01-02T09:29:52.000Z","path":"2018/01/02/Mysql数据库30条规范/","text":"这些规范适用场景：并发量大、数据量大的互联网业务 规范：介绍内容 解读：讲解原因，解读比规范更重要 一、基础规范 （1）必须使用InnoDB存储引擎 解读：支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 （2）必须使用UTF8字符集 解读：万国码，无需转码，无乱码风险，节省空间 （3）数据表、数据字段必须加入中文注释 解读：N年后谁tm知道这个r1,r2,r3字段是干嘛的 （4）禁止使用存储过程、视图、触发器、Event 解读：高并发大数据的互联网业务，架构设计思路是“解放数据库CPU，将计算转移到服务层”，并发量大的情况下，这些功能很可能将数据库拖死，业务逻辑放到服务层具备更好的扩展性，能够轻易实现“增机器就加性能”。数据库擅长存储与索引，CPU计算还是上移吧 （5）禁止存储大文件或者大照片 解读：为何要让数据库做它不擅长的事情？大文件和照片存储在文件系统，数据库里存URI多好 二、命名规范 （1）只允许使用内网域名，而不是ip连接数据库 （2）线上环境、开发环境、测试环境数据库内网域名遵循命名规范 业务名称：xxx 线上环境：dj.xxx.db 开发环境：dj.xxx.rdb 测试环境：dj.xxx.tdb 从库在名称后加-s标识，备库在名称后加-ss标识 线上从库：dj.xxx-s.db 线上备库：dj.xxx-sss.db （3）库名、表名、字段名：小写，下划线风格，不超过32个字符，必须见名知意，禁止拼音英文混用 （4）表名t_xxx，非唯一索引名idx_xxx，唯一索引名uniq_xxx 三、表设计规范 （1）单实例表数目必须小于500 （2）单表列数目必须小于30 （3）表必须有主键，例如自增主键 解读： a）主键递增，数据行写入可以提高插入性能，可以避免page分裂，减少表碎片提升空间和内存的使用 b）主键要选择较短的数据类型， Innodb引擎普通索引都会保存主键的值，较短的数据类型可以有效的减少索引的磁盘空间，提高索引的缓存效率 c） 无主键的表删除，在row模式的主从架构，会导致备库夯住 （4）禁止使用外键，如果有外键完整性约束，需要应用程序控制 解读：外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景数据库使用以性能优先 四、字段设计规范 （1）必须把字段定义为NOT NULL并且提供默认值 解读： a）null的列使索引/索引统计/值比较都更加复杂，对MySQL来说更难优化 b）null 这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条件下，表中有较多空字段的时候，数据库的处理性能会降低很多 c）null值需要更多的存储空，无论是表还是索引中每行中的null的列都需要额外的空间来标识 d）对null 的处理时候，只能采用is null或is not null，而不能采用=、in、&lt;、&lt;&gt;、!=、not in这些操作符号。如：where name!=’shenjian’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录 （2）禁止使用TEXT、BLOB类型 解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能 （3）禁止使用小数存储货币 解读：使用整数吧，小数容易导致钱对不上 （4）必须使用varchar(20)存储手机号 解读： a）涉及到区号或者国家代号，可能出现+-() b）手机号会去做数学运算么？ c）varchar可以支持模糊查询，例如：like“138%” （5）禁止使用ENUM，可使用TINYINT代替 解读： a）增加新的ENUM值要做DDL操作 b）ENUM的内部实际存储就是整数，你以为自己定义的是字符串？ 五、索引设计规范 （1）单表索引建议控制在5个以内 （2）单索引字段数不允许超过5个 解读：字段超过5个时，实际已经起不到有效过滤数据的作用了 （3）禁止在更新十分频繁、区分度不高的属性上建立索引 解读： a）更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能 b）“性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性能与全表扫描类似 （4）建立组合索引，必须把区分度高的字段放在前面 解读：能够更加有效的过滤数据 六、SQL使用规范 （1）禁止使用SELECT *，只获取必要的字段，需要显示说明列属性 解读： a）读取不需要的列会增加CPU、IO、NET消耗 b）不能有效的利用覆盖索引 c）使用SELECT *容易在增加或者删除字段后出现程序BUG （2）禁止使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性 解读：容易在增加或者删除字段后出现程序BUG （3）禁止使用属性隐式转换 解读：SELECT uid FROM t_user WHERE phone=13812345678 会导致全表扫描，而不能命中phone索引，猜猜为什么？（这个线上问题不止出现过一次） （4）禁止在WHERE条件的属性上使用函数或者表达式 解读：SELECT uid FROM t_user WHERE from_unixtime(day)&gt;=’2017-02-15’ 会导致全表扫描 正确的写法是：SELECT uid FROM t_user WHERE day&gt;= unix_timestamp(‘2017-02-15 00:00:00’) （5）禁止负向查询，以及%开头的模糊查询 解读： a）负向查询条件：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描 b）%开头的模糊查询，会导致全表扫描 （6）禁止大表使用JOIN查询，禁止大表使用子查询 解读：会产生临时表，消耗较多内存与CPU，极大影响数据库性能 （7）禁止使用OR条件，必须改为IN查询 解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费更多的CPU帮助实施查询优化呢？ （8）应用程序必须捕获SQL异常，并有相应处理 总结：大数据量高并发的互联网业务，极大影响数据库性能的都不让用，不让用哟。","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"mysql","slug":"mysql","permalink":"ly2513.github.com/tags/mysql/"}]},{"title":"Redis性能问题排查解决手册","date":"2017-11-07T04:51:18.000Z","path":"2017/11/07/Redis性能问题排查解决手册/","text":"性能相关的数据指标 通过Redis-cli命令行界面访问到Redis服务器，然后使用info命令获取所有与Redis服务相关的信息。通过这些信息来分析文章后面提到的一些性能指标。 info命令输出的数据可分为10个类别，分别是： server clients memory persistence stats replication cpu commandstats cluster keyspace这篇主要介绍比较重要的2部分性能指标memory和stats。 需要注意的是info命令返回的信息，并没有命令响应延迟相关的数据信息，所以后面会详细介绍怎么获取与延迟相关的数据指标。 倘若你觉得info输出的信息太多并且杂乱无章，可以指定info命令的参数来获取单个分类下的数据。比如输入info memory命令，会只返回与内存相关的数据。 为了快速定位并解决性能问题，这里选择5个关键性的数据指标，它包含了大多数人在使用Redis上会经常碰到的性能问题。 内存使用率used_memory 上图中used_memory 字段数据表示的是：由Redis分配器分配的内存总量，以字节（byte）为单位。 其中used_memory_human上的数据和used_memory是一样的值，它以M为单位显示，仅为了方便阅读。 used_memory是Redis使用的内存总量，它包含了实际缓存占用的内存和Redis自身运行所占用的内存(如元数据、lua)。它是由Redis使用内存分配器分配的内存，所以这个数据并没有把内存碎片浪费掉的内存给统计进去。 其他字段代表的含义，都以字节为单位： used_memory_rss：从操作系统上显示已经分配的内存总量。mem_fragmentation_ratio： 内存碎片率。used_memory_lua： Lua脚本引擎所使用的内存大小。mem_allocator： 在编译时指定的Redis使用的内存分配器，可以是libc、jemalloc、tcmalloc。 因内存交换引起的性能问题 内存使用率是Redis服务最关键的一部分。如果一个Redis实例的内存使用率超过可用最大内存 (used_memory &gt; 可用最大内存)，那么操作系统开始进行内存与swap空间交换，把内存中旧的或不再使用的内容写入硬盘上（硬盘上的这块空间叫Swap分区），以便腾出新的物理内存给新页或活动页(page)使用。在硬盘上进行读写操作要比在内存上进行读写操作，时间上慢了近5个数量级，内存是0.1μs单位、而硬盘是10ms。如果Redis进程上发生内存交换，那么Redis和依赖Redis上数据的应用会受到严重的性能影响。 通过查看used_memory指标可知道Redis正在使用的内存情况，如果used_memory&gt;可用最大内存，那就说明Redis实例正在进行内存交换或者已经内存交换完毕。管理员根据这个情况，执行相对应的应急措施。 跟踪内存使用率 若是在使用Redis期间没有开启rdb快照或aof持久化策略，那么缓存数据在Redis崩溃时就有丢失的危险。因为当Redis内存使用率超过可用内存的95%时，部分数据开始在内存与swap空间来回交换，这时就可能有丢失数据的危险。当开启并触发快照功能时，Redis会fork一个子进程把当前内存中的数据完全复制一份写入到硬盘上。因此若是当前使用内存超过可用内存的45%时触发快照功能，那么此时进行的内存交换会变的非常危险(可能会丢失数据)。 倘若在这个时候实例上有大量频繁的更新操作，问题会变得更加严重。 通过减少Redis的内存占用率，来避免这样的问题，或者使用下面的技巧来避免内存交换发生： 假如缓存数据小于4GB，就使用32位的Redis实例。因为32位实例上的指针大小只有64位的一半，它的内存空间占用空间会更少些。 这有一个坏处就是，假设物理内存超过4GB，那么32位实例能使用的内存仍然会被限制在4GB以下。 要是实例同时也共享给其他一些应用使用的话，那可能需要更高效的64位Redis实例，这种情况下切换到32位是不可取的。 不管使用哪种方式，Redis的dump文件在32位和64位之间是互相兼容的， 因此倘若有减少占用内存空间的需求，可以尝试先使用32位，后面再切换到64位上。 尽可能的使用Hash数据结构。因为Redis在储存小于100个字段的Hash结构上，其存储效率是非常高的。所以在不需要集合(set)操作或list的push/pop操作的时候，尽可能的使用Hash结构。比如，在一个web应用程序中，需要存储一个对象表示用户信息，使用单个key表示一个用户，其每个属性存储在Hash的字段里，这样要比给每个属性单独设置一个key-value要高效的多。 通常情况下倘若有数据使用string结构，用多个key存储时，那么应该转换成单key多字段的Hash结构。 如上述例子中介绍的Hash结构应包含，单个对象的属性或者单个用户各种各样的资料。Hash结构的操作命令是HSET(key, fields, value)和HGET(key, field)，使用它可以存储或从Hash中取出指定的字段。 设置key的过期时间。一个减少内存使用率的简单方法就是，每当存储对象时确保设置key的过期时间。倘若key在明确的时间周期内使用或者旧key不大可能被使用时，就可以用Redis过期时间命令(expire,expireat, pexpire, pexpireat)去设置过期时间，这样Redis会在key过期时自动删除key。 假如你知道每秒钟有多少个新key-value被创建，那可以调整key的存活时间，并指定阀值去限制Redis使用的最大内存。 回收key。在Redis配置文件中(一般叫Redis.conf)，通过设置“maxmemory”属性的值可以限制Redis最大使用的内存，修改后重启实例生效。 也可以使用客户端命令config set maxmemory 去修改值，这个命令是立即生效的，但会在重启后会失效，需要使用config rewrite命令去刷新配置文件。 若是启用了Redis快照功能，应该设置“maxmemory”值为系统可使用内存的45%，因为快照时需要一倍的内存来复制整个数据集，也就是说如果当前已使用45%，在快照期间会变成95%(45%+45%+5%)，其中5%是预留给其他的开销。 如果没开启快照功能，maxmemory最高能设置为系统可用内存的95%。 当内存使用达到设置的最大阀值时，需要选择一种key的回收策略，可在Redis.conf配置文件中修改“maxmemory-policy”属性值。 若是Redis数据集中的key都设置了过期时间，那么“volatile-ttl”策略是比较好的选择。但如果key在达到最大内存限制时没能够迅速过期，或者根本没有设置过期时间。那么设置为“allkeys-lru”值比较合适，它允许Redis从整个数据集中挑选最近最少使用的key进行删除(LRU淘汰算法)。Redis还提供了一些其他淘汰策略，如下： volatile-lru：使用LRU算法从已设置过期时间的数据集合中淘汰数据。volatile-ttl：从已设置过期时间的数据集合中挑选即将过期的数据淘汰。volatile-random：从已设置过期时间的数据集合中随机挑选数据淘汰。allkeys-lru：使用LRU算法从所有数据集合中淘汰数据。allkeys-random：从数据集合中任意选择数据淘汰no-enviction：禁止淘汰数据。通过设置maxmemory为系统可用内存的45%或95%(取决于持久化策略)和设置“maxmemory-policy”为“volatile-ttl”或“allkeys-lru”(取决于过期设置)，可以比较准确的限制Redis最大内存使用率，在绝大多数场景下使用这2种方式可确保Redis不会进行内存交换。倘若你担心由于限制了内存使用率导致丢失数据的话，可以设置noneviction值禁止淘汰数据。 命令处理数total_commands_processed 在info信息里的total_commands_processed字段显示了Redis服务处理命令的总数，其命令都是从一个或多个Redis客户端请求过来的。Redis每时每刻都在处理从客户端请求过来的命令，它可以是Redis提供的140种命令的任意一个。 total_commands_processed字段的值是递增的，比如Redis服务分别处理了client_x请求过来的2个命令和client_y请求过来的3个命令，那么命令处理总数(total_commands_processed)就会加上5。 分析命令处理总数，诊断响应延迟。 在Redis实例中，跟踪命令处理总数是解决响应延迟问题最关键的部分，因为Redis是个单线程模型，客户端过来的命令是按照顺序执行的。比较常见的延迟是带宽，通过千兆网卡的延迟大约有200μs。倘若明显看到命令的响应时间变慢，延迟高于200μs，那可能是Redis命令队列里等待处理的命令数量比较多。 如上所述，延迟时间增加导致响应时间变慢可能是由于一个或多个慢命令引起的，这时可以看到每秒命令处理数在明显下降，甚至于后面的命令完全被阻塞，导致Redis性能降低。要分析解决这个性能问题，需要跟踪命令处理数的数量和延迟时间。比如可以写个脚本，定期记录total_commands_processed的值。当客户端明显发现响应时间过慢时，可以通过记录的total_commands_processed历史数据值来判断命理处理总数是上升趋势还是下降趋势，以便排查问题。 使用命令处理总数解决延迟时间增加。 通过与记录的历史数据比较得知，命令处理总数确实是处于上升或下降状态，那么可能是有2个原因引起的: 命令队列里的命令数量过多，后面命令一直在等待中。 几个慢命令阻塞Redis。 下面有三个办法可以解决，因上面2条原因引起的响应延迟问题。 使用多参数命令：若是客户端在很短的时间内发送大量的命令过来，会发现响应时间明显变慢，这由于后面命令一直在等待队列中前面大量命令执行完毕。有个方法可以改善延迟问题，就是通过单命令多参数的形式取代多命令单参数的形式。举例来说，循环使用LSET命令去添加1000个元素到list结构中，是性能比较差的一种方式，更好的做法是在客户端创建一个1000元素的列表，用单个命令LPUSH或RPUSH，通过多参数构造形式一次性把1000个元素发送的Redis服务上。下面的表格是Redis的一些操作命令，有单个参数命令和支持多个参数的命令，通过这些命令可尽量减少使用多命令的次数。管道命令：另一个减少多命令的方法是使用管道(pipeline)，把几个命令合并一起执行，从而减少因网络开销引起的延迟问题。因为10个命令单独发送到服务端会引起10次网络延迟开销，使用管道会一次性把执行结果返回，仅需要一次网络延迟开销。Redis本身支持管道命令，大多数客户端也支持，倘若当前实例延迟很明显，那么使用管道去降低延迟是非常有效的。 避免操作大集合的慢命令：如果命令处理频率过低导致延迟时间增加，这可能是因为使用了高时间复杂度的命令操作导致，这意味着每个命令从集合中获取数据的时间增大。 所以减少使用高时间复杂的命令，能显著的提高的Redis的性能。下面的表格是高时间复杂度命令的列表，其详细描述了命令的属性，有这助于高效合理的、最优化的使用这些命令(如果不得不使用的话)，以提高Redis性能。 延迟时间Redis的延迟数据是无法从info信息中获取的。倘若想要查看延迟时间，可以用 Redis-cli工具加–latency参数运行，如: 1redis-cli --latency -h 127.0.0.1 -p 6379 其host和port是Redis实例的ip及端口。由于当前服务器不同的运行情况，延迟时间可能有所误差，通常1G网卡的延迟时间是200μs。 以毫秒为单位测量Redis的响应延迟时间，楼主本机的延迟是300μs： 跟踪Redis延迟性能 Redis之所以这么流行的主要原因之一就是低延迟特性带来的高性能，所以说解决延迟问题是提高Redis性能最直接的办法。拿1G带宽来说，若是延迟时间远高于200μs，那明显是出现了性能问题。 虽然在服务器上会有一些慢的IO操作，但Redis是单核接受所有客户端的请求，所有请求是按良好的顺序排队执行。因此若是一个客户端发过来的命令是个慢操作，那么其他所有请求必须等待它完成后才能继续执行。 使用延迟命令提高性能 一旦确定延迟时间是个性能问题后，这里有几个办法可以用来分析解决性能问题。 使用slowlog查出引发延迟的慢命令： Redis中的slowlog命令可以让我们快速定位到那些超出指定执行时间的慢命令，默认情况下命令若是执行时间超过10ms就会被记录到日志。slowlog只会记录其命令执行的时间，不包含io往返操作，也不记录单由网络延迟引起的响应慢。通常1gb带宽的网络延迟，预期在200μs左右，倘若一个命令仅执行时间就超过10ms，那比网络延迟慢了近50倍。 想要查看所有执行时间比较慢的命令，可以通过使用Redis-cli工具，输入slowlog get命令查看，返回结果的第三个字段以微妙位单位显示命令的执行时间。假如只需要查看最后10个慢命令，输入slowlog get 10即可。 关于怎么定位到是由慢命令引起的延迟问题，可查看total_commands_processed介绍章节。 图中字段分别意思是： 1=日志的唯一标识符2=被记录命令的执行时间点，以 UNIX 时间戳格式表示3=查询执行时间，以微秒为单位。例子中命令使用54毫秒。4= 执行的命令，以数组的形式排列。完整命令是config get *。倘若你想自定义慢命令的标准，可以调整触发日志记录慢命令的阀值。若是很少或没有命令超过10ms，想降低记录的阀值，比如5毫秒，可在Redis-cli工具中输入下面的命令配置： 1config set slowlog-log-slower-than 5000 也可以在Redis.config配置文件中设置，以微妙位单位。 监控客户端的连接：因为Redis是单线程模型(只能使用单核)，来处理所有客户端的请求， 但由于客户端连接数的增长，处理请求的线程资源开始降低分配给单个客户端连接的处理时间，这时每个客户端需要花费更多的时间去等待Redis共享服务的响应。这种情况下监控客户端连接数是非常重要的，因为客户端创建连接数的数量可能超出预期的数量，也可能是客户端端没有有效的释放连接。在Redis-cli工具中输入info clients可以查看到当前实例的所有客户端连接信息。如下图，第一个字段(connected_clients)显示当前实例客户端连接的总数： Redis默认允许客户端连接的最大数量是10000。若是看到连接数超过5000以上，那可能会影响Redis的性能。倘若一些或大部分客户端发送大量的命令过来，这个数字会低的多。 限制客户端连接数：自Redis2.6以后，允许使用者在配置文件(Redis.conf)maxclients属性上修改客户端连接的最大数，也可以通过在Redis-cli工具上输入config set maxclients 去设置最大连接数。根据连接数负载的情况，这个数字应该设置为预期连接数峰值的110%到150之间，若是连接数超出这个数字后，Redis会拒绝并立刻关闭新来的连接。通过设置最大连接数来限制非预期数量的连接数增长，是非常重要的。另外，新连接尝试失败会返回一个错误消息，这可以让客户端知道，Redis此时有非预期数量的连接数，以便执行对应的处理措施。 上述二种做法对控制连接数的数量和持续保持Redis的性能最优是非常重要的， 加强内存管理：较少的内存会引起Redis延迟时间增加。如果Redis占用内存超出系统可用内存，操作系统会把Redis进程的一部分数据，从物理内存交换到硬盘上，内存交换会明显的增加延迟时间。关于怎么监控和减少内存使用，可查看used_memory介绍章节。 性能数据指标： 分析解决Redis性能问题，通常需要把延迟时间的数据变化与其他性能指标的变化相关联起来。命令处理总数下降的发生可能是由慢命令阻塞了整个系统，但如果命令处理总数的增加，同时内存使用率也增加，那么就可能是由于内存交换引起的性能问题。对于这种性能指标相关联的分析，需要从历史数据上来观察到数据指标的重要变化，此外还可以观察到单个性能指标相关联的所有其他性能指标信息。这些数据可以在Redis上收集，周期性的调用内容为Redis info的脚本，然后分析输出的信息，记录到日志文件中。当延迟发生变化时，用日志文件配合其他数据指标，把数据串联起来排查定位问题。 内存碎片率 info信息中的mem_fragmentation_ratio给出了内存碎片率的数据指标，它是由操系统分配的内存除以Redis分配的内存得出： used_memory和used_memory_rss数字都包含的内存分配有： 用户定义的数据：内存被用来存储key-value值。 内部开销： 存储内部Redis信息用来表示不同的数据类型。 used_memory_rss的rss是Resident Set Size的缩写，表示该进程所占物理内存的大小，是操作系统分配给Redis实例的内存大小。除了用户定义的数据和内部开销以外，used_memory_rss指标还包含了内存碎片的开销，内存碎片是由操作系统低效的分配/回收物理内存导致的。操作系统负责分配物理内存给各个应用进程，Redis使用的内存与物理内存的映射是由操作系统上虚拟内存管理分配器完成的。举个例子来说，Redis需要分配连续内存块来存储1G的数据集，这样的话更有利，但可能物理内存上没有超过1G的连续内存块，那操作系统就不得不使用多个不连续的小内存块来分配并存储这1G数据，也就导致内存碎片的产生。内存分配器另一个复杂的层面是，它经常会预先分配一些内存块给引用，这样做会使加快应用程序的运行。 理解资源性能 跟踪内存碎片率对理解Redis实例的资源性能是非常重要的。内存碎片率稍大于1是合理的，这个值表示内存碎片率比较低，也说明redis没有发生内存交换。但如果内存碎片率超过1.5，那就说明Redis消耗了实际需要物理内存的150%，其中50%是内存碎片率。若是内存碎片率低于1的话，说明Redis内存分配超出了物理内存，操作系统正在进行内存交换。内存交换会引起非常明显的响应延迟，可查看used_memory介绍章节。 上图中的0.99即99%。 用内存碎片率预测性能问题 倘若内存碎片率超过了1.5，那可能是操作系统或Redis实例中内存管理变差的表现。下面有3种方法解决内存管理变差的问题，并提高Redis性能： 重启Redis服务器：如果内存碎片率超过1.5，重启Redis服务器可以让额外产生的内存碎片失效并重新作为新内存来使用，使操作系统恢复高效的内存管理。额外碎片的产生是由于Redis释放了内存块，但内存分配器并没有返回内存给操作系统，这个内存分配器是在编译时指定的，可以是libc、jemalloc或者tcmalloc。 通过比较used_memory_peak, used_memory_rss和used_memory_metrics的数据指标值可以检查额外内存碎片的占用。从名字上可以看出，used_memory_peak是过去Redis内存使用的峰值，而不是当前使用内存的值。如果used_memory_peak和used_memory_rss的值大致上相等，而且二者明显超过了used_memory值，这说明额外的内存碎片正在产生。 在Redis-cli工具上输入info memory可以查看上面三个指标的信息： 在重启服务器之前，需要在Redis-cli工具上输入shutdown save命令，意思是强制让Redis数据库执行保存操作并关闭Redis服务，这样做能保证在执行Redis关闭时不丢失任何数据。 在重启后，Redis会从硬盘上加载持久化的文件，以确保数据集持续可用。 限制内存交换：如果内存碎片率低于1，Redis实例可能会把部分数据交换到硬盘上。内存交换会严重影响Redis的性能，所以应该增加可用物理内存或减少实Redis内存占用。 可查看used_memory章节的优化建议。 修改内存分配器：Redis支持glibc’s malloc、jemalloc11、tcmalloc几种不同的内存分配器，每个分配器在内存分配和碎片上都有不同的实现。不建议普通管理员修改Redis默认内存分配器，因为这需要完全理解这几种内存分配器的差异，也要重新编译Redis。这个方法更多的是让其了解Redis内存分配器所做的工作，当然也是改善内存碎片问题的一种办法。 回收key info信息中的evicted_keys字段显示的是，因为maxmemory限制导致key被回收删除的数量。关于maxmemory的介绍见前面章节，回收key的情况只会发生在设置maxmemory值后，不设置会发生内存交换。 当Redis由于内存压力需要回收一个key时，Redis首先考虑的不是回收最旧的数据，而是在最近最少使用的key或即将过期的key中随机选择一个key，从数据集中删除。 这可以在配置文件中设置maxmemory-policy值为“volatile-lru”或“volatile-ttl”，来确定Redis是使用lru策略还是过期时间策略。 倘若所有的key都有明确的过期时间，那过期时间回收策略是比较合适的。若是没有设置key的过期时间或者说没有足够的过期key，那设置lru策略是比较合理的，这可以回收key而不用考虑其过期状态。 根据key回收定位性能问题 跟踪key回收是非常重要的，因为通过回收key，可以保证合理分配Redis有限的内存资源。如果evicted_keys值经常超过0，那应该会看到客户端命令响应延迟时间增加，因为Redis不但要处理客户端过来的命令请求，还要频繁的回收满足条件的key。需要注意的是，回收key对性能的影响远没有内存交换严重，若是在强制内存交换和设置回收策略做一个选择的话，选择设置回收策略是比较合理的，因为把内存数据交换到硬盘上对性能影响非常大(见前面章节)。 减少回收key以提升性能 减少回收key的数量是提升Redis性能的直接办法，下面有2种方法可以减少回收key的数量： 增加内存限制：倘若开启快照功能，maxmemory需要设置成物理内存的45%，这几乎不会有引发内存交换的危险。若是没有开启快照功能，设置系统可用内存的95%是比较合理的，具体参考前面的快照和maxmemory限制章节。如果maxmemory的设置是低于45%或95%(视持久化策略)，通过增加maxmemory的值能让Redis在内存中存储更多的key，这能显著减少回收key的数量。 若是maxmemory已经设置为推荐的阀值后，增加maxmemory限制不但无法提升性能，反而会引发内存交换，导致延迟增加、性能降低。 maxmemory的值可以在Redis-cli工具上输入config set maxmemory命令来设置。需要注意的是，这个设置是立即生效的，但重启后丢失，需要永久化保存的话，再输入config rewrite命令会把内存中的新配置刷新到配置文件中。 对实例进行分片：分片是把数据分割成合适大小，分别存放在不同的Redis实例上，每一个实例都包含整个数据集的一部分。通过分片可以把很多服务器联合起来存储数据，相当于增加总的物理内存，使其在没有内存交换和回收key的策略下也能存储更多的key。假如有一个非常大的数据集，maxmemory已经设置，实际内存使用也已经超过了推荐设置的阀值，那通过数据分片能明显减少key的回收，从而提高Redis的性能。 分片的实现有很多种方法，下面是Redis实现分片的几种常见方式： a. Hash分片：一个比较简单的方法实现，通过Hash函数计算出key的Hash值，然后值所在范围对应特定的Redis实例。 b. 代理分片：客户端把请求发送到代理上，代理通过分片配置表选择对应的Redis实例。 如Twitter的Twemproxy，豌豆荚的codis。 c. 一致性Hash分片： 参见前面博客《一致性Hash分片详解》 d. 虚拟桶分片：参见前面博客《虚拟桶分详解》 总结 对于开发者来说，Redis是个速度非常快的key-value内存数据库，并提供了方便的API接口。为了最好最优的使用Redis，需要理解哪些因素能影响到Redis性能，哪些数据指标能帮助我们避免性能陷阱。 通过本篇，能理解Redis中的重要性能指标，怎么查看，更重要的是怎么利用这些数据排查解决Redis性能问题。","tags":[{"name":"redis","slug":"redis","permalink":"ly2513.github.com/tags/redis/"}]},{"title":"mac系统 PHP7版本下安装并使用xhprof性能分析工具","date":"2017-11-05T16:51:32.000Z","path":"2017/11/06/mac系统-PHP7版本下安装并使用xhprof性能分析工具/","text":"有段时间没更新博客了,今天要分享的这篇是关于一个PHP的性能测试工具– xhprof也许有人对此工具不是很了解,博主在此简单介绍下该工具: Xhprof—-facebook开源的，轻量级的PHP性能分析工具。它报告函数级别的请求次数和各种指标，包括阻塞时间，CPU时间和内存使用情况。一个函数的开销，可细分成调用者和被调用者的开销，XHProf数据收集阶段，它记录调用次数的追踪和包容性的指标弧在动态callgraph的一个程序。它独有的数据计算的报告/后处理阶段。在数据收集时，XHProfd通过检测循环来处理递归的函数调用，并通过给递归调用中每个深度的调用一个有用的命名来避开死循环。XHProf分析报告有助于理解被执行的代码的结构，它有一个简单的HTML的用户界面（ PHP写成的）。基于浏览器的性能分析用户界面能更容易查看，或是与同行们分享成果。也能绘制调用关系图。简单说就是个PHP性能分析利器,PHPer们值得拥有。接下来进入本博客文章的主题,怎么在 mac 系统 PHP7版本下安装并使用该利器。 第一步,在GitHub上搜索xhprof,一般找第一个项目将其克隆道你本地,进行编译 1git clone git@github.com:phacility/xhprof.git 然后计入到 其项目的extension目录中 1cd xhprof/extension/ 如果你本地有多个PHP版本,请选择你当前使用的PHP版本,博主本人使用的是PHP7.1,找到PHP7.1的phpize工具,执行 1/usr/local/Cellar/php71/7.1.8_20/bin/phpize 出现如下类似的结果,表示没问题 1234Configuring for:PHP Api Version: 20160303Zend Module Api No: 20160303Zend Extension Api No: 320160303 然后进行编译 1./configure --with-php-config=/usr/local/Cellar/php71/7.1.8_20/bin/php-config --enable-xhprof 执行完上面命令后执行安装 12makemake install 安装完后会出现 1Installing shared extensions: /usr/local/Cellar/php71/7.1.8_20/lib/php/extensions/no-debug-non-zts-20160303/ 代表编译成功 第二步,编译好扩展后,修改php配置文件,在/etc/php.ini最后一行增加如下配置 12[xhprof]extension=xhprof.so 保存好之后，重启php-fpm 1kill -USR2 `cat /opt/7.1.8_20/var/run/php-fpm.pid` 将相关文件移动到项目中//切换到下载的 xhprof 目录12cp -r xhprof/xhprof_html ROOT_PATH/cp -r xhprof/xhprof_lib ROOT_PATH/ 怎么使用xhprof首先将如下代码写入到你需要测试代码中 1234567891011xhprof_enable();//你需要分析的代码$xhprof_data = xhprof_disable();include_once ROOT_PATH.&apos;/xhprof_lib/utils/xhprof_lib.php&apos;;include_once ROOT_PATH . &apos;/xhprof_lib/utils/xhprof_runs.php&apos;;$xhprof_runs = new XHProfRuns_Default();$run_id = $xhprof_runs-&gt;save_run($xhprof_data, &quot;xhprof_test&quot;); //将run_id保存起来或者随代码一起输出 查看数据 访问$host_url/xhpfrof_html/index.php?run=58d3b28b521f6&amp;source=xhprof_test来查看结果 图形化结果 点击[View Full Callgraph]可以看图形化结果 如果出现如下报错 1failed to execute cmd：&quot; dot -Tpng&quot;. stderr：sh： dot：command not found。 说明没安装graphviz图表组件 解决方案 12345# macbrew install graphviz# centosyum install graphviz","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"xhprof","slug":"xhprof","permalink":"ly2513.github.com/tags/xhprof/"}]},{"title":"Redis Cluster深入与实践","date":"2017-10-27T02:16:09.000Z","path":"2017/10/27/Redis-Cluster深入与实践/","text":"1. Redis介绍www.redis.ioredis是一个基于内存的K-V存储数据库。支持存储的类型有string,list,set,zset(sorted set),hash等。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。redis支持各种不同方式的排序。保证效率的情况下，数据缓存在内存中。同时redis提供了持久化策略，不同的策略触发同步到磁盘或者把修改操作写入追加的记录文件，在此基础上实现了master-slave。 它是一个高性能的存储系统，能支持超过 100K+ 每秒的读写频率。同时还支持消息的发布/订阅，从而让你在构建高性能消息队列系统时多了另一种选择。 Redis支持主从同步。数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器。这使得Redis可执行单层树复制。存盘可以有意无意的对数据进行写操作。由于完全实现了发布/订阅机制，使得从数据库在任何地方同步树时，可订阅一个频道并接收主服务器完整的消息发布记录。同步对读取操作的可扩展性和数据冗余很有帮助。 2. 主从Redis支持master-slave模式，一主多从，redis server可以设置另外多个redis server为slave，从机同步主机的数据。配置后，读写分离，主机负责读写服务，从机只负责读。减轻主机的压力。redis实现的是最终会一致性，具体选择强一致性还是弱一致性，取决于业务场景。 Redis 主从同步有两种方式（或者所两个阶段）：全同步和部分同步。 主从刚刚连接的时候，进行全同步；全同步结束后，进行部分同步。当然，如果有需要，slave 在任何时候都可以发起全同步。redis 策略是，无论如何，首先会尝试进行部分同步，如不成功，要求从机进行全同步，并启动 BGSAVE……BGSAVE 结束后，传输 RDB 文件；如果成功，允许从机进行部分同步，并传输积压空间中的数据。简单来说，主从同步就是 RDB 文件的上传下载；主机有小部分的数据修改，就把修改记录传播给每个从机。 3. Redis集群主从模式存在的问题是，master宕机之后，从机只能读，不可写，不能保证高可用。redis集群技术是构建高性能网站架构的重要手段，试想在网站承受高并发访问压力的同时，还需要从海量数据中查询出满足条件的数据，并快速响应，我们必然想到的是将数据进行切片，把数据根据某种规则放入多个不同的服务器节点，来降低单节点服务器的压力。 Redis Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。节点之间使用gossip协议传播信息以及发现新节点。 Redis 集群是一个分布式（distributed）、容错（fault-tolerant）的 Redis 实现，集群可以使用的功能是普通单机 Redis 所能使用的功能的一个子集（subset）。 Redis 集群中不存在中心（central）节点或者代理（proxy）节点，集群的其中一个主要设计目标是达到线性可扩展性（linear scalability）。 Redis 集群为了保证一致性（consistency）而牺牲了一部分容错性：系统会在保证对网络断线（net split）和节点失效（node failure）具有有限（limited）抵抗力的前提下，尽可能地保持数据的一致性。 4. 安装部署Redis安装较为简单，官网下载压缩包解压。集群模式需要ruby的编译环境，集群最小的配置为3台master，小于3则启动集群报错。 Redis版本：3.2.4 4.1 主从模式拓扑图 主从模式拓扑图 主从模式采用一主三从，主从都配置auth认证，读写分离。 主要实验的动作： 1）多个app 同时写，测定写速率；2）多个app 同时写，同时有读的进程，测定读写速率；3）master主机宕机，app依然进行读写。 4.2 Cluster拓扑图如下 cluster 集群模式采用四主四从，也是采用读写分离。 主要实验的动作： 1）有一个master宕机，观察日志，新的slave成为master；2）master宕机后，重新启动，master成为slave；3）集群全部宕机，redis主机重启，数据未丢失。 5. 原理5.1 一致性 filesnapshot:默认redis是会以快照的形式将数据持久化到磁盘,在配置文件中的格式是：save N M表示在N秒之内，redis至少发生M次修改则redis抓快照到磁盘。 工作原理：当Redis需要做持久化时，Redis会fork一个子进程；子进程将数据写到磁盘上一个临时RDB文件中；当子进程完成写临时文件后，将原来的RDB替换掉，这样的好处就是可以copy-on-write。 Append-only：filesnapshotting方法在redis异常死掉时， 最近的数据会丢失（丢失数据的多少视你save策略的配置），所以这是它最大的缺点，当业务量很大时，丢失的数据是很多的。Append-only方法可 以做到全部数据不丢失，但redis的性能就要差些。AOF就可以做到全程持久化，只需要在配置文件中开启（默认是no），appendonly yes开启AOF之后，redis每执行一个修改数据的命令，都会把它添加到aof文件中，当redis重启时，将会读取AOF文件进行“重放”以恢复到 redis关闭前的最后时刻。 AOF文件刷新的方式，有三种，参考配置参数appendfsync ： appendfsync always每提交一个修改命令都调用fsync刷新到AOF文件，非常非常慢，但也非常安全；appendfsync everysec每秒钟都调用fsync刷新到AOF文件，很快，但可能会丢失一秒以内的数据；appendfsync no依靠OS进行刷新，redis不主动刷新AOF，这样最快，但安全性就差。默认并推荐每秒刷新，这样在速度和安全上都做到了兼顾。 Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。因此我们可以将Redis的Replication架构视为图结构。 Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。 Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据。 为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成。即便如此，系统的伸缩性还是得到了很大的提高。 Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。 Redis在master是非阻塞模式，也就是说在slave执行数据同步的时候，master是可以接受客户端的请求的，并不影响同步数据的一致性，然而在slave端是阻塞模式的，slave在同步master数据时，并不能够响应客户端的查询。 5.2 Replication的工作原理 (1)Slave服务器连接到Master服务器。(2)Slave服务器发送SYCN命令。(3)Master服务器备份数据库到.rdb文件。(4)Master服务器把.rdb文件传输给Slave服务器。(5)Slave服务器把.rdb文件数据导入到数据库中。 在Slave启动并连接到Master之后，它将主动发送一个SYNC命令。此后Master将启动后台存盘进程，同时收集所有接收到的用于修改数据集 的命令，在后台进程执行完毕后，Master将传送整个数据库文件到Slave，以完成一次完全同步。而Slave服务器在接收到数据库文件数据之后将其 存盘并加载到内存中。此后，Master继续将所有已经收集到的修改命令，和新的修改命令依次传送给Slaves，Slave将在本次执行这些数据修改命令，从而达到最终的数据同步。如果Master和Slave之间的链接出现断连现象，Slave可以自动重连Master，但是在连接成功之后，一次完全同步将被自动执行。 5.3 一致性哈希 集群要实现的目的是要将不同的 key 分散放置到不同的 redis 节点，这里我们需要一个规则或者算法，通常的做法是获取 key 的哈希值，然后根据节点数来求模，但这种做法有其明显的弊端，当我们需要增加或减少一个节点时，会造成大量的 key 无法命中，这种比例是相当高的，所以就有人提出了一致性哈希的概念。 一致性哈希有四个重要特征： 均衡性：也有人把它定义为平衡性，是指哈希的结果能够尽可能分布到所有的节点中去，这样可以有效的利用每个节点上的资源。单调性：对于单调性有很多翻译让我非常的不解，而我想要的是当节点数量变化时哈希的结果应尽可能的保护已分配的内容不会被重新分派到新的节点。分散性和负载：这两个其实是差不多的意思，就是要求一致性哈希算法对 key 哈希应尽可能的避免重复。 Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。 使用哈希槽的好处就在于可以方便的添加或移除节点。 当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了； 当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了； 当设置了主从关系后，slave 在第一次连接或者重新连接 master 时，slave 都会发送一条同步指令给 master； master 接到指令后，开始启动后台保存进程保存数据，接着收集所有的数据修改指令。后台保存完了，master 就把这份数据发送给 slave，slave 先把数据保存到磁盘，然后把它加载到内存中，master 接着就把收集的数据修改指令一行一行的发给 slave，slave 接收到之后重新执行该指令，这样就实现了数据同步。 slave 在与 master 失去联系后，自动的重新连接。如果 master 收到了多个 slave 的同步请求，它会执行单个后台保存来为所有的 slave 服务。 5.4 节点失效检测 以下是节点失效检查的实现方法： 当一个节点向另一个节点发送 PING 命令，但是目标节点未能在给定的时限内返回 PING 命令的回复时，那么发送命令的节点会将目标节点标记为PFAIL （possible failure，可能已失效）。 等待 PING 命令回复的时限称为“节点超时时限（node timeout）”，是一个节点选项（node-wise setting）。 每次当节点对其他节点发送 PING 命令的时候，它都会随机地广播三个它所知道的节点的信息，这些信息里面的其中一项就是说明节点是否已经被标记为 PFAIL 或者 FAIL 。 当节点接收到其他节点发来的信息时，它会记下那些被其他节点标记为失效的节点。这称为失效报告（failure report）。 如果节点已经将某个节点标记为 PFAIL ，并且根据节点所收到的失效报告显式，集群中的大部分其他主节点也认为那个节点进入了失效状态，那么节点会将那个失效节点的状态标记为 FAIL 。 一旦某个节点被标记为 FAIL ，关于这个节点已失效的信息就会被广播到整个集群，所有接收到这条信息的节点都会将失效节点标记为 FAIL 。 简单来说，一个节点要将另一个节点标记为失效，必须先询问其他节点的意见，并且得到大部分主节点的同意才行。因为过期的失效报告会被移除，所以主节点要将某个节点标记为 FAIL 的话，必须以最近接收到的失效报告作为根据。 在以下两种情况中，节点的 FAIL 状态会被移除： 如果被标记为 FAIL 的是从节点，那么当这个节点重新上线时， FAIL 标记就会被移除。保持（retaning）从节点的 FAIL 状态是没有意义的，因为它不处理任何槽，一个从节点是否处于 FAIL 状态，决定了这个从节点在有需要时能否被提升为主节点。 如果一个主节点被打上 FAIL 标记之后，经过了节点超时时限的四倍时间，再加上十秒钟之后，针对这个主节点的槽的故障转移操作仍未完成，并且这个主节点已经重新上线的话，那么移除对这个节点的 FAIL 标记。 在第二种情况中，如果故障转移未能顺利完成，并且主节点重新上线，那么集群就继续使用原来的主节点，从而免去管理员介入的必要。 5.5 从节点选举 一旦某个主节点进入 FAIL 状态，如果这个主节点有一个或多个从节点存在，那么其中一个从节点会被升级为新的主节点，而其他从节点则会开始对这个新的主节点进行复制。 新的主节点由已下线主节点属下的所有从节点中自行选举产生，以下是选举的条件： 这个节点是已下线主节点的从节点。 已下线主节点负责处理的槽数量非空。 从节点的数据被认为是可靠的，也即是，主从节点之间的复制连接（replication link）的断线时长不能超过节点超时时限（node timeout）乘以REDIS_CLUSTER_SLAVE_VALIDITY_MULT 常量得出的积。 如果一个从节点满足了以上的所有条件，那么这个从节点将向集群中的其他主节点发送授权请求，询问它们，是否允许自己（从节点）升级为新的主节点。 如果发送授权请求的从节点满足以下属性，那么主节点将向从节点返回 FAILOVER_AUTH_GRANTED 授权，同意从节点的升级要求： 发送授权请求的是一个从节点，并且它所属的主节点处于 FAIL 状态。 在已下线主节点的所有从节点中，这个从节点的节点 ID 在排序中是最小的。 从节点处于正常的运行状态：它没有被标记为 FAIL 状态，也没有被标记为 PFAIL 状态。 一旦某个从节点在给定的时限内得到大部分主节点的授权，它就会开始执行以下故障转移操作： 通过 PONG 数据包（packet）告知其他节点，这个节点现在是主节点了。 通过 PONG 数据包告知其他节点，这个节点是一个已升级的从节点（promoted slave）。 接管（claiming）所有由已下线主节点负责处理的哈希槽。 显式地向所有节点广播一个 PONG 数据包，加速其他节点识别这个节点的进度，而不是等待定时的 PING / PONG 数据包。 所有其他节点都会根据新的主节点对配置进行相应的更新，特别地：a. 所有被新的主节点接管的槽会被更新。b. 已下线主节点的所有从节点会察觉到 PROMOTED 标志，并开始对新的主节点进行复制。c.如果已下线的主节点重新回到上线状态，那么它会察觉到 PROMOTED 标志，并将自身调整为现任主节点的从节点。 在集群的生命周期中，如果一个带有 PROMOTED 标识的主节点因为某些原因转变成了从节点，那么该节点将丢失它所带有的 PROMOTED 标识。 6. 总结Redis集群具有高可用，易于迁移，存取速度快等特点。也可以作为消息队列使用，支持pub/sub模式，具体优缺点总结如下： 首先优点： Redis 在主节点下线后，从节点会自动提升为主节点，提供服务 redis 宕机节点恢复后，自动会添加到集群中，变成从节点 动态扩展和删除节点，rehash slot的分配，基于桶的数据分布方式大大降低了迁移成本，只需将数据桶从一个Redis Node迁移到另一个Redis Node即可完成迁移。 Redis Cluster使用异步复制。 其缺点为： 由于 Redis 的复制使用异步机制，在自动故障转移的过程中，集群可能会丢失写命令。然而 Redis 几乎是同时执行(将命令恢复发送给客户端，以及将命令复制到从节点)这两个操作，所以实际中，命令丢失的窗口非常小。 普通的主从模式支持auth加密认证，虽然比较弱，但写或者读都要通过密码验证，cluster对密码支持不太友好，如果对集群设置密码，那么requirepass和masterauth都需要设置，否则发生主从切换时，就会遇到授权问题，可以模拟并观察日志。 参考资料： www.redis.io redis-cluster研究和使用（http://hot66hot.iteye.com/blog/2050676） Redis Cluster 3.0搭建与使用（http://www.cnblogs.com/gomysql/p/4395504.html）","tags":[{"name":"redis","slug":"redis","permalink":"ly2513.github.com/tags/redis/"},{"name":"集群","slug":"集群","permalink":"ly2513.github.com/tags/集群/"}]},{"title":"优雅的在一台vps(云主机)上面部署vue+mongodb+express项目","date":"2017-10-26T06:54:28.000Z","path":"2017/10/26/优雅的在一台vps-云主机-上面部署vue-mongodb-express项目/","text":"项目： vue + express + mongodb项目前后分离部署在一台服务器上面 express端口：3000mongodb端口：27017vue端口：本地是8080 服务端是：80 本地开发配置本地开发基于vue cli 端口是 8080如果请求api的时候在前缀加上localhost:3000会提示跨域问题，我们可以使用下面方式来解决这个问题在vue项目路径找到这个文件 /vue-item/config/index.js 找到这行代码： proxyTable: {}添加如下配置demo: 12345678proxyTable: &#123; '/v1/**':&#123; target: 'http://localhost:3000/', pathRewrite: &#123; '^/v1': '/' &#125; &#125;&#125; v1 是我给api自动添加的前缀这个前缀可以使用 axios 配置添加在main.js 主入口文件添加如下 12345678import apiConfig from '../config/api.config'// import axiosimport Axios from 'axios'import VueAxios from 'vue-axios'Vue.use(VueAxios, Axios)// Axios.defaults.baseURL = apiConfig.baseUrl;Axios.defaults.baseURL = 'v1/' 这样也ok的api.config 判断是开发模式还是本地模式，其实不需要这么麻烦 直接 12345const isProdMode = Object.is(process.env.NODE_ENV, 'production')module.exports = &#123; baseUrl: isProdMode ? 'api.shudong.wang/v1/' : 'v1/'&#125; 如果把axios 配置了自动前缀每次访问的时候 1234567891011121314151617181920212223data()&#123; return &#123; articleList:Object &#125;&#125;,mounted: function()&#123; this.getArticleList()&#125;,methods:&#123; getArticleList()&#123; console.log(111111111) this.$http.get(\"/article/list\") // this.$http axios使用的一种方式 .then((response)=&gt;&#123; console.log(response.data) let res = response.data; this.articleList = res.data; &#125;) .catch((error) =&gt;&#123; console.log(error) &#125;) &#125;&#125;, 上面请求的例子中相当于访问： localhost:8080/v1/article/list这样就可以解决跨域问题其实最终访问的是 localhost:3000/article/list express的api这个v1只是api版本的标识，如果想带着，并且api是可以v1版本方式访问的，把代理的路径重新规则去掉就可以操作如下： 1234567891011121314proxyTable: &#123; '/v1/**':&#123; target: 'http://localhost:3000/', //pathRewrite: &#123; //这个规则去掉 // '^/v1': '/' //&#125; &#125;, '/goods/*':&#123; target:'http://localhost:3000' &#125;, '/users/**':&#123; target:'http://localhost:3000' &#125;&#125; 服务端部署本地可以使用proxyTable 解决跨域问题，那么服务端怎么解决跨域问题呢？answer：使用nginx反向代理 nginx配置： 仔细分析一下看看是否适合自己的业务场景 12345678910111213141516171819202122232425262728293031323334353637383940server&#123; listen 80; #listen [::]:80; server_name zhenfan.shudong.wang ; # 你的域名不需要加http index index.html index.htm index.php default.html default.htm default.php; root /home/wwwroot/zhenfan/dist; include none.conf; #error_page 404 /404.html; # Deny access to PHP files in specific directory #location ~ /(wp-content|uploads|wp-includes|images)/.*\\.php$ &#123; deny all; &#125; include enable-php.conf; location /v1 &#123; proxy_pass http://127.0.0.1:3000/; # 当访问v1的时候默认转发到 3000端口 &#125; location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf)$ &#123; expires 30d; &#125; location ~ .*\\.(js|css)?$ &#123; expires 12h; &#125; location ~ /.well-known &#123; allow all; &#125; location ~ /\\. &#123; deny all; &#125; access_log off;&#125; 关于express链接mongodb可以直接填写端口号，不存在跨域问题，直接 127.0.0.1：27017就ok，怎么在服务器上面搭建可以参考上篇 mongodb篇 关于有什么问题，可以在下面留言，希望你是来讨论技术的。上次写完一篇，一个小朋友，来到这里咬文嚼字，针对 部署这个词，说用的不当，还口口声声说是来讨论技术，把注意力放在这个上面上真没意义。希望本篇文章能帮到你，解决你的问题。","tags":[{"name":"js","slug":"js","permalink":"ly2513.github.com/tags/js/"},{"name":"vue","slug":"vue","permalink":"ly2513.github.com/tags/vue/"},{"name":"mongodb","slug":"mongodb","permalink":"ly2513.github.com/tags/mongodb/"},{"name":"express","slug":"express","permalink":"ly2513.github.com/tags/express/"}]},{"title":"如何在vue里面优雅解决跨域, 路由冲突问题","date":"2017-10-26T06:38:55.000Z","path":"2017/10/26/如何在vue里面优雅解决跨域-路由冲突问题/","text":"当我们在路由里面配置成以下代理可以解决跨域问题12345678proxyTable: &#123; '/goods/*': &#123; target: 'http://localhost:3000' &#125;, '/users/*': &#123; target: 'http://localhost:3000' &#125;&#125;, 这种配置方式在一定程度上解决了跨域问题，但是会带来一些问题，比如我们的vue 路由 也命名为 goods，这时候就会产生了冲突，如果项目中接口很多，都在这里配置是很麻烦的，也容易产生路由冲突。 正确的姿势如果把所有的接口，统一规范为一个入口，在一定程度上会解决冲突把以上配置统一前面加上 /api/ 12345proxyTable: &#123; '/api/**': &#123; target: 'http://localhost:3000' &#125;,&#125;, 如果我们配置成这种方式,在使用http请求的时候就会发生变化，会在请求前面加上一个api，相对路由也会发生变化，也会在接口前面加上api，这样也会很麻烦,我们可以使用以下方式来解决这个问题12345678proxyTable: &#123; '/api/**': &#123; target: 'http://localhost:3000', pathRewrite:&#123; '^/api':'/' &#125; &#125;,&#125;, 上面这个代码，就是把咱们虚拟的这个api接口，去掉，此时真正去后端请求的时候，不会加上api这个前缀了，那么这样我们前台http请求的时候，还必须加上api前缀才能匹配到这个代理,代码如下：1234567891011121314logout()&#123; axios.post('/api/users/logout').then(result=&gt;&#123; let res = result.data; this.nickName = ''; console.log(res); &#125;)&#125;,getGoods()&#123; axios.post('/api/goods/list').then(result=&gt;&#123; let res = result.data; this.nickName = ''; console.log(res); &#125;)&#125; 我们可以利用axios的baseUrl直接默认值是 api，这样我们每次访问的时候，自动补上这个api前缀，就不需要我们自己手工在每个接口上面写这个前缀了在入口文件里面配置如下： 1234567import Axios from 'axios'import VueAxios from 'vue-axios'Vue.use(VueAxios, Axios)# 如果这配置 'api/' 会默认读取本地的域Axios.defaults.baseURL = 'api' 上面这样配置的话，不会区分生产和开发环境在config 文件夹里面新建一个 api.config.js 配置文件 12345const isPro = Object.is(process.env.NODE_ENV, 'production')module.exports = &#123; baseUrl: isPro ? 'http://www.vnshop.cn/api/' : 'api/'&#125; 然后在main.js 里面引入,这样可以保证动态的匹配生产和开发的定义前缀1234567import apiConfig from '../config/api.config'import Axios from 'axios'import VueAxios from 'vue-axios'Vue.use(VueAxios, Axios)Axios.defaults.baseURL = apiConfig.baseUrl 经过上面配置后，在dom里面可以这样轻松的访问,也不需要在任何组件里面引入axios模块了。1234567891011121314logout()&#123; this.$http.post('/users/logout').then(result=&gt;&#123; let res = result.data; this.nickName = ''; console.log(res); &#125;)&#125;,getGoods()&#123; this.$http.post('/goods/list').then(result=&gt;&#123; let res = result.data; this.nickName = ''; console.log(res); &#125;)&#125; ##最终代码 在代理里面配置12345678proxyTable: &#123; '/api/**': &#123; target: 'http://localhost:3000', pathRewrite:&#123; '^/api':'/' &#125; &#125;,&#125;, 在config里面的api.config.js 配置在config 文件夹里面新建一个 api.config.js 配置文件 12345const isPro = Object.is(process.env.NODE_ENV, 'production')module.exports = &#123; baseUrl: isPro ? 'http://www.vnshop.cn/api/' : 'api/'&#125; 关于生产和开发配置不太了解可以去 dev-server.js 里面看配置代码 123const webpackConfig = (process.env.NODE_ENV === 'testing' || process.env.NODE_ENV === 'production') ? require('./webpack.prod.conf') : require('./webpack.dev.conf') 在main.js 入口文件里面配置1234567import apiConfig from '../config/api.config'import Axios from 'axios'import VueAxios from 'vue-axios'Vue.use(VueAxios, Axios)Axios.defaults.baseURL = apiConfig.baseUrl 在dom里面请求api的姿势1234567891011121314logout()&#123; this.$http.post('/users/logout').then(result=&gt;&#123; let res = result.data; this.nickName = ''; console.log(res); &#125;)&#125;,getGoods()&#123; this.$http.post('/goods/list').then(result=&gt;&#123; let res = result.data; this.nickName = ''; console.log(res); &#125;)&#125;","tags":[{"name":"js","slug":"js","permalink":"ly2513.github.com/tags/js/"},{"name":"vue","slug":"vue","permalink":"ly2513.github.com/tags/vue/"}]},{"title":"Redis Cluster 3.0搭建与使用","date":"2017-10-26T01:46:38.000Z","path":"2017/10/26/Redis-Cluster-3-0搭建与使用/","text":"Redis Cluster终于出了Stable，这让人很是激动，等Stable很久了，所以还是先玩玩。 一、 集群简单概念。 Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施（installation）。 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis 集群提供了以下两个好处： 将数据自动切分（split）到多个节点的能力。当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个哈希槽， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。节点 B 负责处理 5501 号至 11000 号哈希槽。节点 C 负责处理 11001 号至 16384 号哈希槽。这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说： 如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。 为了使得集群在一部分节点下线或者无法与集群的大多数（majority）节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： 集群中的每个节点都有 1 个至 N 个复制品（replica）， 其中一个复制品为主节点（master）， 而其余的 N-1 个复制品为从节点（slave）。 在之前列举的节点 A 、B 、C 的例子中， 如果节点 B 下线了， 那么集群将无法正常运行， 因为集群找不到节点来处理 5501 号至 11000号的哈希槽。 另一方面， 假如在创建集群的时候（或者至少在节点 B 下线之前）， 我们为主节点 B 添加了从节点 B1 ， 那么当主节点 B 下线的时候， 集群就会将 B1 设置为新的主节点， 并让它代替下线的主节点 B ， 继续处理 5501 号至 11000 号的哈希槽， 这样集群就不会因为主节点 B 的下线而无法正常运作了。 不过如果节点 B 和 B1 都下线的话， Redis 集群还是会停止运作。 Redis-cluster 架构图如下： 架构细节: (1)所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽. (2)节点的fail是通过集群中超过半数的节点检测失效时才生效. (3)客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可 (4)redis-cluster把所有的物理节点映射到[0-16383]slot上,cluster 负责维护node&lt;-&gt;slot&lt;-&gt;value 二、 Redis Cluster搭建使用 要让集群正常工作至少需要3个主节点，在这里我们要创建6个redis节点，其中三个为主节点，三个为从节点，对应的redis节点的ip和端口对应关系如下（为了简单演示都在同一台机器上面） 127.0.0.1:7000127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 1、下载最新版redis。 1wget http://download.redis.io/releases/redis-3.0.0.tar.gz 2、解压，安装 123tar xf redis-3.0.0.tar.gzcd redis-3.0.0make &amp;&amp; make install 3、创建存放多个实例的目录 123mkdir /data/cluster -pcd /data/clustermkdir 7000 7001 7002 7003 7004 7005 4、修改配置文件 1cp redis-3.0.0/redis.conf /data/cluster/7000/ 修改配置文件中下面选项 1234567891011port 7000daemonize yescluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yes 文件中的 cluster-enabled 选项用于开实例的集群模式， 而 cluster-conf-file 选项则设定了保存节点配置文件的路径， 默认值为nodes.conf 。其他参数相信童鞋们都知道。节点配置文件无须人为修改， 它由 Redis 集群在启动时创建， 并在有需要时自动进行更新。 修改完成后，把修改完成的redis.conf复制到7001-7005目录下，并且端口修改成和文件夹对应。 5、分别启动6个redis实例。 123456789101112cd /data/cluster/7000redis-server redis.confcd /data/cluster/7001redis-server redis.confcd /data/cluster/7002redis-server redis.confcd /data/cluster/7003redis-server redis.confcd /data/cluster/7004redis-server redis.confcd /data/cluster/7005redis-server redis.conf 查看进程否存在。 12345678[root@redis-server 7005]# ps -ef | grep redisroot 4168 1 0 11:49 ? 00:00:00 redis-server *:7000 [cluster]root 4176 1 0 11:49 ? 00:00:00 redis-server *:7001 [cluster]root 4186 1 0 11:50 ? 00:00:00 redis-server *:7002 [cluster]root 4194 1 0 11:50 ? 00:00:00 redis-server *:7003 [cluster]root 4202 1 0 11:50 ? 00:00:00 redis-server *:7004 [cluster]root 4210 1 0 11:50 ? 00:00:00 redis-server *:7005 [cluster]root 4219 4075 0 11:50 pts/2 00:00:00 grep redis 6、执行命令创建集群，首先安装依赖，否则创建集群失败。 yum install ruby rubygems -y安装gem-redis 下载地址： 1gem install -l redis-3.0.0.gem 复制集群管理程序到/usr/local/bin 1cp redis-3.0.0/src/redis-trib.rb /usr/local/bin/redis-trib 创建集群： 1redis-trib create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 命令的意义如下： 给定 redis-trib.rb 程序的命令是 create ， 这表示我们希望创建一个新的集群。选项 –replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。之后跟着的其他参数则是实例的地址列表， 我们希望程序使用这些地址所指示的实例来创建新集群。简单来说， 以上命令的意思就是让 redis-trib 程序创建一个包含三个主节点和三个从节点的集群。 接着， redis-trib 会打印出一份预想中的配置给你看， 如果你觉得没问题的话， 就可以输入 yes ， redis-trib 就会将这份配置应用到集群当中： 12345678910111213141516171819202122232425262728&gt;&gt;&gt; Creating clusterConnecting to node 127.0.0.1:7000: OKConnecting to node 127.0.0.1:7001: OKConnecting to node 127.0.0.1:7002: OKConnecting to node 127.0.0.1:7003: OKConnecting to node 127.0.0.1:7004: OKConnecting to node 127.0.0.1:7005: OK&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:7000127.0.0.1:7001127.0.0.1:7002Adding replica 127.0.0.1:7003 to 127.0.0.1:7000Adding replica 127.0.0.1:7004 to 127.0.0.1:7001Adding replica 127.0.0.1:7005 to 127.0.0.1:7002M: 2774f156af482b4f76a5c0bda8ec561a8a1719c2 127.0.0.1:7000 slots:0-5460 (5461 slots) masterM: 2d03b862083ee1b1785dba5db2987739cf3a80eb 127.0.0.1:7001 slots:5461-10922 (5462 slots) masterM: 0456869a2c2359c3e06e065a09de86df2e3135ac 127.0.0.1:7002 slots:10923-16383 (5461 slots) masterS: 37b251500385929d5c54a005809377681b95ca90 127.0.0.1:7003 replicates 2774f156af482b4f76a5c0bda8ec561a8a1719c2S: e2e2e692c40fc34f700762d1fe3a8df94816a062 127.0.0.1:7004 replicates 2d03b862083ee1b1785dba5db2987739cf3a80ebS: 9923235f8f2b2587407350b1d8b887a7a59de8db 127.0.0.1:7005 replicates 0456869a2c2359c3e06e065a09de86df2e3135acCan I set the above configuration? (type 'yes' to accept): 输入 yes 并按下回车确认之后， 集群就会将配置应用到各个节点， 并连接起（join）各个节点 —— 也即是， 让各个节点开始互相通讯： 12345678910111213141516171819202122232425Can I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join......&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7000)M: 2774f156af482b4f76a5c0bda8ec561a8a1719c2 127.0.0.1:7000 slots:0-5460 (5461 slots) masterM: 2d03b862083ee1b1785dba5db2987739cf3a80eb 127.0.0.1:7001 slots:5461-10922 (5462 slots) masterM: 0456869a2c2359c3e06e065a09de86df2e3135ac 127.0.0.1:7002 slots:10923-16383 (5461 slots) masterM: 37b251500385929d5c54a005809377681b95ca90 127.0.0.1:7003 slots: (0 slots) master replicates 2774f156af482b4f76a5c0bda8ec561a8a1719c2M: e2e2e692c40fc34f700762d1fe3a8df94816a062 127.0.0.1:7004 slots: (0 slots) master replicates 2d03b862083ee1b1785dba5db2987739cf3a80ebM: 9923235f8f2b2587407350b1d8b887a7a59de8db 127.0.0.1:7005 slots: (0 slots) master replicates 0456869a2c2359c3e06e065a09de86df2e3135ac[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 一切正常输出以下信息： 1234[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 集群的客户端 Redis 集群现阶段的一个问题是客户端实现很少。 以下是一些我知道的实现： redis-rb-cluster 是我（@antirez）编写的 Ruby 实现， 用于作为其他实现的参考。 该实现是对 redis-rb 的一个简单包装， 高效地实现了与集群进行通讯所需的最少语义（semantic）。redis-py-cluster 看上去是 redis-rb-cluster 的一个 Python 版本， 这个项目有一段时间没有更新了（最后一次提交是在六个月之前）， 不过可以将这个项目用作学习集群的起点。流行的 Predis 曾经对早期的 Redis 集群有过一定的支持， 但我不确定它对集群的支持是否完整， 也不清楚它是否和最新版本的 Redis 集群兼容 （因为新版的 Redis 集群将槽的数量从 4k 改为 16k 了）。Redis unstable 分支中的 redis-cli 程序实现了非常基本的集群支持， 可以使用命令 redis-cli -c 来启动。测试 Redis 集群比较简单的办法就是使用 redis-rb-cluster 或者 redis-cli ， 接下来我们将使用 redis-cli 为例来进行演示： 123456[root@redis-server ~]# redis-cli -c -p 7001127.0.0.1:7001&gt; set name yayunOK127.0.0.1:7001&gt; get name\"yayun\"127.0.0.1:7001&gt; 我们可以看看还有哪些命令可以用： 123456789101112131415161718192021222324[root@redis-server ~]# redis-trib helpUsage: redis-trib &lt;command&gt; &lt;options&gt; &lt;arguments ...&gt; set-timeout host:port milliseconds add-node new_host:new_port existing_host:existing_port --master-id &lt;arg&gt; --slave fix host:port help (show this help) del-node host:port node_id import host:port --from &lt;arg&gt; check host:port call host:port command arg arg .. arg create host1:port1 ... hostN:portN --replicas &lt;arg&gt; reshard host:port --yes --to &lt;arg&gt; --from &lt;arg&gt; --slots &lt;arg&gt;For check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster.[root@redis-server ~]# 可以看见有add-node，不用想了，肯定是添加节点。那么del-node就是删除节点。还有check肯定就是检查状态了。 12345678[root@redis-server ~]# redis-cli -p 7000 cluster nodes2d03b862083ee1b1785dba5db2987739cf3a80eb 127.0.0.1:7001 master - 0 1428293673322 2 connected 5461-1092237b251500385929d5c54a005809377681b95ca90 127.0.0.1:7003 slave 2774f156af482b4f76a5c0bda8ec561a8a1719c2 0 1428293672305 4 connectede2e2e692c40fc34f700762d1fe3a8df94816a062 127.0.0.1:7004 slave 2d03b862083ee1b1785dba5db2987739cf3a80eb 0 1428293674340 5 connected0456869a2c2359c3e06e065a09de86df2e3135ac 127.0.0.1:7002 master - 0 1428293670262 3 connected 10923-163832774f156af482b4f76a5c0bda8ec561a8a1719c2 127.0.0.1:7000 myself,master - 0 0 1 connected 0-54609923235f8f2b2587407350b1d8b887a7a59de8db 127.0.0.1:7005 slave 0456869a2c2359c3e06e065a09de86df2e3135ac 0 1428293675362 6 connected[root@redis-server ~]# 可以看到7000-7002是master，7003-7005是slave。 故障转移测试： 12345127.0.0.1:7001&gt; KEYS *1) \"name\"127.0.0.1:7001&gt; get name\"yayun\"127.0.0.1:7001&gt; 可以看见7001是正常的，并且获取到了key，value，现在kill掉7000实例，再进行查询。 12345678910[root@redis-server ~]# ps -ef | grep 7000root 4168 1 0 11:49 ? 00:00:03 redis-server *:7000 [cluster]root 4385 4361 0 12:39 pts/3 00:00:00 grep 7000[root@redis-server ~]# kill 4168[root@redis-server ~]# ps -ef | grep 7000root 4387 4361 0 12:39 pts/3 00:00:00 grep 7000[root@redis-server ~]# redis-cli -c -p 7001127.0.0.1:7001&gt; get name\"yayun\"127.0.0.1:7001&gt; 可以正常获取到value，现在看看状态。 12345678[root@redis-server ~]# redis-cli -c -p 7001 cluster nodes2d03b862083ee1b1785dba5db2987739cf3a80eb 127.0.0.1:7001 myself,master - 0 0 2 connected 5461-109220456869a2c2359c3e06e065a09de86df2e3135ac 127.0.0.1:7002 master - 0 1428295271619 3 connected 10923-1638337b251500385929d5c54a005809377681b95ca90 127.0.0.1:7003 master - 0 1428295270603 7 connected 0-5460e2e2e692c40fc34f700762d1fe3a8df94816a062 127.0.0.1:7004 slave 2d03b862083ee1b1785dba5db2987739cf3a80eb 0 1428295272642 5 connected2774f156af482b4f76a5c0bda8ec561a8a1719c2 127.0.0.1:7000 master,fail - 1428295159553 1428295157205 1 disconnected9923235f8f2b2587407350b1d8b887a7a59de8db 127.0.0.1:7005 slave 0456869a2c2359c3e06e065a09de86df2e3135ac 0 1428295269587 6 connected[root@redis-server ~]# 原来的7000端口实例已经显示fail，原来的7003是slave，现在自动提升为master。 关于更多的在线添加节点，删除节点，以及对集群进行重新分片请参考官方文档。 总结： redis-cluster是个好东西，只是stable才出来不久，肯定坑略多，而且现在使用的人比较少，前期了解学习一下是可以的，生产环境肯定要慎重考虑。且需要进行严格的测试。生产环境中redis的集群可以考虑使用Twitter开源的twemproxy，以及豌豆荚开源的codis，这两个项目都比较成熟，现在使用的公司很多。已经向业界朋友得到证实。后面也会写博客介绍twemproxy和codis。","tags":[{"name":"redis","slug":"redis","permalink":"ly2513.github.com/tags/redis/"}]},{"title":"每天掌握一个Linux命令(44) : top命令","date":"2017-10-25T02:23:09.000Z","path":"2017/10/25/每天掌握一个Linux命令-44-top命令/","text":"top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。 下面详细介绍它的使用方法。top是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止.比较准确的说,top命令提供了实时的对系统处理器的状态监视.它将显示系统中CPU最“敏感”的任务列表.该命令可以按CPU使用.内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定. 1．命令格式： top [参数] 2．命令功能： 显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 3．命令参数： -b 批处理-c 显示完整的治命令-I 忽略失效过程-s 保密模式-S 累积模式-i&lt;时间&gt; 设置间隔时间-u&lt;用户名&gt; 指定用户名-p&lt;进程号&gt; 指定进程-n&lt;次数&gt; 循环显示的次数 4．使用实例： 实例一：显示进程信息命令：top 输出： 说明： 统计信息区： 前五行是当前系统情况整体的统计信息区。下面我们看每一行信息的具体意义。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下：14:06:23 — 当前系统时间 up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！） 2 users — 当前有2个用户登录系统 load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。 load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下： 系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行，cpu状态信息，具体属性说明如下： 5.9%us — 用户空间占用CPU的百分比。3.4% sy — 内核空间占用CPU的百分比。0.0% ni — 改变过优先级的进程占用CPU的百分比90.4% id — 空闲CPU百分比0.0% wa — IO等待占用CPU的百分比0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比0.2% si — 软中断（Software Interrupts）占用CPU的百分比 备注：在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！ 第四行,内存状态，具体信息如下： 32949016k total — 物理内存总量（32GB）14411180k used — 使用中的内存总量（14GB）18537836k free — 空闲内存总量（18GB）169884k buffers — 缓存的内存量 （169M） 第五行，swap交换分区信息，具体信息说明如下： 32764556k total — 交换区总量（32GB）0k used — 使用的交换区总量（0K）32764556k free — 空闲交换区总量（32GB）3612636k cached — 缓冲的交换区总量（3.6GB） 备注： 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 如果出于习惯去计算可用内存数，这里有个近似的计算公式：第四行的free + 第四行的buffers + 第五行的cached，按这个公式此台服务器的可用内存：18537836k +169884k +3612636k = 22GB左右。 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。 第六行，空行。 第七行以下：各进程（任务）的状态监控，项目列信息说明如下： PID — 进程idUSER — 进程所有者PR — 进程优先级NI — nice值。负值表示高优先级，正值表示低优先级VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RESRES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATASHR — 共享内存大小，单位kbS — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程%CPU — 上次更新到现在的CPU时间占用百分比%MEM — 进程使用的物理内存百分比TIME+ — 进程使用的CPU时间总计，单位1/100秒COMMAND — 进程名称（命令名/命令行） 其他使用技巧： 1.多U多核CPU监控 在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况： 观察上图，服务器有16个逻辑CPU，实际上是4个物理CPU。再按数字键1，就会返回到top基本视图界面。 2.高亮显示当前运行进程 敲击键盘“b”（打开/关闭加亮效果），top的视图变化如下： 我们发现进程id为2570的“top”进程被加亮了，top进程就是视图第二行显示的唯一的运行态（runing）的那个进程，可以通过敲击“y”键关闭或打开运行态进程的加亮效果。 3.进程字段排序 默认进入top时，各进程是按照CPU的占用量来排序的，在下图中进程ID为28894的java进程排在第一（cpu占用142%），进程ID为574的java进程排在第二（cpu占用16%）。 敲击键盘“x”（打开/关闭排序列的加亮效果），top的视图变化如下： 可以看到，top默认的排序列是“%CPU”。 4. 通过”shift + &gt;”或”shift + &lt;”可以向右或左改变排序列 下图是按一次”shift + &gt;”的效果图,视图现在已经按照%MEM来排序。 实例二：显示 完整命令命令：top -c 输出： 实例三：以批处理模式显示程序信息命令：top -b 实例四：以累积模式显示程序信息命令：top -S 实例五：设置信息更新次数命令：top -n 2 输出： 说明：表示更新两次后终止更新显示 实例六：设置信息更新时间命令：top -d 3 输出： 说明：表示更新周期为3秒 实例七：显示指定的进程信息命令：top -p 574 输出： 说明： 5.top交互命令 在top 命令执行过程中可以使用的一些交互命令。这些命令都是单字母的，如果在命令行中使用了s 选项， 其中一些命令可能会被屏蔽。 h 显示帮助画面，给出一些简短的命令总结说明k 终止一个进程。i 忽略闲置和僵死进程。这是一个开关式命令。q 退出程序r 重新安排一个进程的优先级别S 切换到累计模式s 改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 sf或者F 从当前显示中添加或者删除项目o或者O 改变显示项目的顺序l 切换显示平均负载和启动时间信息m 切换显示内存信息t 切换显示进程和CPU状态信息c 切换显示命令名称和完整命令行M 根据驻留内存大小进行排序P 根据CPU使用百分比大小进行排序T 根据时间/累计时间进行排序W 将当前设置写入~/.toprc文件中","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"top","slug":"top","permalink":"ly2513.github.com/tags/top/"}]},{"title":"每天掌握一个Linux命令(43) : killall命令","date":"2017-10-25T02:17:57.000Z","path":"2017/10/25/每天掌握一个Linux命令-43-killall命令/","text":"Linux系统中的killall命令用于杀死指定名字的进程（kill processes by name）。我们可以使用kill命令杀死指定进程PID的进程，如果要找到我们需要杀死的进程，我们还需要在之前使用ps等命令再配合grep来查找进程，而killall把这两个过程合二为一，是一个很好用的命令。 1．命令格式： killall[参数][进程名] 2．命令功能： 用来结束同名的的所有进程 3．命令参数： -Z 只杀死拥有scontext 的进程-e 要求匹配进程名称-I 忽略小写-g 杀死进程组而不是进程-i 交互模式，杀死进程前先询问用户-l 列出所有的已知信号名称-q 不输出警告信息-s 发送指定的信号-v 报告信号是否成功发送-w 等待进程死亡–help 显示帮助信息–version 显示版本显示 4．使用实例： 实例一：杀死所有同名进程命令：killall vi 输出： 12345678910[root@localhost ~]# ps -ef|grep viroot 17581 17398 0 17:51 pts/0 00:00:00 vi test.txtroot 17611 17582 0 17:51 pts/1 00:00:00 grep vi[root@localhost ~]# ps -ef|grep viroot 17581 17398 0 17:51 pts/0 00:00:00 vi test.txtroot 17640 17612 0 17:51 pts/2 00:00:00 vi test.logroot 17642 17582 0 17:51 pts/1 00:00:00 grep vi[root@localhost ~]# killall vi[root@localhost ~]# ps -ef|grep viroot 17645 17582 0 17:52 pts/1 00:00:00 grep vi 实例二：向进程发送指定信号命令：后台运行程序：vi &amp; 杀死 vi进程：killall -TERM vi 或者 killall -KILL vi 输出： 123456789101112131415161718192021[root@localhost ~]# vi &amp;[1] 17646[root@localhost ~]# killall -TERM vi[1]+ Stopped vi[root@localhost ~]# vi &amp;[2] 17648[root@localhost ~]# ps -ef|grep viroot 17646 17582 0 17:54 pts/1 00:00:00 viroot 17648 17582 0 17:54 pts/1 00:00:00 viroot 17650 17582 0 17:55 pts/1 00:00:00 grep vi[2]+ Stopped vi[root@localhost ~]# killall -TERM vi[root@localhost ~]# ps -ef|grep viroot 17646 17582 0 17:54 pts/1 00:00:00 viroot 17648 17582 0 17:54 pts/1 00:00:00 viroot 17653 17582 0 17:55 pts/1 00:00:00 grep vi[root@localhost ~]# killall -KILL vi[1]- 已杀死 vi[2]+ 已杀死 vi[root@localhost ~]# ps -ef|grep viroot 17656 17582 0 17:56 pts/1 00:00:00 grep vi[root@localhost ~]# 实例三：把所有的登录后的shell给杀掉命令：killall -9 bash 输出： 12345678910[root@localhost ~]# w 18:01:03 up 41 days, 18:53, 3 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 10.2.0.68 14:58 9:52 0.10s 0.10s -bashroot pts/1 10.2.0.68 17:51 0.00s 0.02s 0.00s wroot pts/2 10.2.0.68 17:51 9:24 0.01s 0.01s -bash[root@localhost ~]# killall -9 bash[root@localhost ~]# w 18:01:48 up 41 days, 18:54, 1 user, load average: 0.07, 0.02, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 10.2.0.68 18:01 0.00s 0.01s 0.00s w[root@localhost ~]# 说明： 运行命令：killall -9 bash 后，所有bash都会被卡掉了，所以当前所有连接丢失了。需要重新连接并登录。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"killall","slug":"killall","permalink":"ly2513.github.com/tags/killall/"}]},{"title":"ElasticSearch配置说明","date":"2017-10-20T07:51:15.000Z","path":"2017/10/20/ElasticSearch配置说明/","text":"配置文件详解1.0版配置文件位于es根目录的libexec/config目录下面，有elasticsearch.yml和logging.yml两个配置，主配置文件是elasticsearch.yml，日志配置文件是logging.yml，elasticsearch调用log4j记录日志，所以日志的配置文件可以按照默认的设置，我来介绍下elasticsearch.yml里面的选项。 cluster.name: elasticsearch配置的集群名称，默认是elasticsearch，es服务会通过广播方式自动连接在同一网段下的es服务，通过多播方式进行通信，同一网段下可以有多个集群，通过集群名称这个属性来区分不同的集群。 node.name: “Franz Kafka”当前配置所在机器的节点名，你不设置就默认随机指定一个name列表中名字，该name列表在es的jar包中config文件夹里name.txt文件中，其中有很多作者添加的有趣名字。 node.master: true指定该节点是否有资格被选举成为node（注意这里只是设置成有资格， 不代表该node一定就是master），默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master。 node.data: true指定该节点是否存储索引数据，默认为true。 index.number_of_shards: 5设置默认索引分片个数，默认为5片。 index.number_of_replicas: 1设置默认索引副本个数，默认为1个副本。如果采用默认设置，而你集群只配置了一台机器，那么集群的健康度为yellow，也就是所有的数据都是可用的，但是某些复制没有被分配（ 健康度可用 curl ‘localhost:9200/_cat/health?v’ 查看， 分为绿色、黄色或红色。绿色代表一切正常，集群功能齐全，黄色意味着所有的数据都是可用的，但是某些复制没有被分配，红色则代表因为某些原因，某些数据不可用）。 path.conf: /path/to/conf设置配置文件的存储路径，默认是es根目录下的config文件夹。 path.data: /path/to/data设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：path.data: /path/to/data1,/path/to/data2 path.work: /path/to/work设置临时文件的存储路径，默认是es根目录下的work文件夹。 path.logs: /path/to/logs设置日志文件的存储路径，默认是es根目录下的logs文件夹 path.plugins: /path/to/plugins设置插件的存放路径，默认是es根目录下的plugins文件夹, 插件在es里面普遍使用，用来增强原系统核心功能。 bootstrap.mlockall: true设置为true来锁住内存不进行swapping。因为当jvm开始swapping时es的效率 会降低，所以要保证它不swap，可以把ES_MIN_MEM和ES_MAX_MEM两个环境变量设置成同一个值，并且保证机器有足够的内存分配给es。 同时也要允许elasticsearch的进程可以锁住内存，linux下启动es之前可以通过ulimit -l unlimited命令设置。 network.bind_host: 192.168.0.1设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0，绑定这台机器的任何一个ip。 network.publish_host: 192.168.0.1设置其它节点和该节点交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。 network.host: 192.168.0.1这个参数是用来同时设置bind_host和publish_host上面两个参数。 transport.tcp.port: 9300设置节点之间交互的tcp端口，默认是9300。 transport.tcp.compress: true设置是否压缩tcp传输时的数据，默认为false，不压缩。 http.port: 9200设置对外服务的http端口，默认为9200。 http.max_content_length: 100mb设置内容的最大容量，默认100mb http.enabled: false是否使用http协议对外提供服务，默认为true，开启。 gateway.type: localgateway的类型，默认为local即为本地文件系统，可以设置为本地文件系统，分布式文件系统，hadoop的HDFS，和amazon的s3服务器等。 gateway.recover_after_nodes: 1设置集群中N个节点启动时进行数据恢复，默认为1。 gateway.recover_after_time: 5m设置初始化数据恢复进程的超时时间，默认是5分钟。 gateway.expected_nodes: 2设置这个集群中节点的数量，默认为2，一旦这N个节点启动，就会立即进行数据恢复。 cluster.routing.allocation.node_initial_primaries_recoveries: 4初始化数据恢复时，并发恢复线程的个数，默认为4。 cluster.routing.allocation.node_concurrent_recoveries: 2添加删除节点或负载均衡时并发恢复线程的个数，默认为4。 indices.recovery.max_size_per_sec: 0设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制。 indices.recovery.concurrent_streams: 5设置这个参数来限制从其它分片恢复数据时最大同时打开并发流的个数，默认为5。 discovery.zen.minimum_master_nodes: 1设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可以设置大一点的值（2-4） discovery.zen.ping.timeout: 3s设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错。 discovery.zen.ping.multicast.enabled: false设置是否打开多播发现节点，默认是true。 discovery.zen.ping.unicast.hosts: [“host1”, “host2:port”, “host3[portX-portY]”]设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。 ＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝ 配置文件2.0版配置文件位于%ES_HOME%/config/elasticsearch.yml文件中，用Editplus打开它，你便可以进行配置。 所有的配置都可以使用环境变量，例如：node.rack: ${RACK_ENV_VAR} 表示环境变量中有一个RACK_ENV_VAR变量。 下面列举一下elasticsearch的可配置项： 1. 集群名称，默认为elasticsearch： cluster.name: elasticsearch 2. 节点名称，es启动时会自动创建节点名称，但你也可进行配置： node.name: “Franz Kafka” 3. 是否作为主节点，每个节点都可以被配置成为主节点，默认值为true： node.master: true 4. 是否存储数据，即存储索引片段，默认值为true： node.data: true master和data同时配置会产生一些奇异的效果： 1) 当master为false，而data为true时，会对该节点产生严重负荷； 2) 当master为true，而data为false时，该节点作为一个协调者； 3) 当master为false，data也为false时，该节点就变成了一个负载均衡器。 你可以通过连接http://localhost:9200/_cluster/health或者http://localhost:9200/_cluster/nodes，或者使用插件http://github.com/lukas-vlcek/bigdesk或http://mobz.github.com/elasticsearch-head来查看集群状态。 5. 每个节点都可以定义一些与之关联的通用属性，用于后期集群进行碎片分配时的过滤： node.rack: rack314 6. 默认情况下，多个节点可以在同一个安装路径启动，如果你想让你的es只启动一个节点，可以进行如下设置： node.max_local_storage_nodes: 1 7. 设置一个索引的碎片数量，默认值为5： index.number_of_shards: 5 8. 设置一个索引可被复制的数量，默认值为1： index.number_of_replicas: 1 当你想要禁用公布式时，你可以进行如下设置：index.number_of_shards: 1index.number_of_replicas: 0 这两个属性的设置直接影响集群中索引和搜索操作的执行。假设你有足够的机器来持有碎片和复制品，那么可以按如下规则设置这两个值： 1) 拥有更多的碎片可以提升索引执行能力，并允许通过机器分发一个大型的索引； 2) 拥有更多的复制器能够提升搜索执行能力以及集群能力。 对于一个索引来说，number_of_shards只能设置一次，而number_of_replicas可以使用索引更新设置API在任何时候被增加或者减少。 ElasticSearch关注加载均衡、迁移、从节点聚集结果等等。可以尝试多种设计来完成这些功能。 可以连接http://localhost:9200/A/_status来检测索引的状态。 9. 配置文件所在的位置，即elasticsearch.yml和logging.yml所在的位置： path.conf: /path/to/conf 10. 分配给当前节点的索引数据所在的位置： path.data: /path/to/data 可以可选择的包含一个以上的位置，使得数据在文件级别跨越位置，这样在创建时就有更多的自由路径，如：path.data: /path/to/data1,/path/to/data2 11. 临时文件位置： path.work: /path/to/work 12. 日志文件所在位置： path.logs: /path/to/logs 13. 插件安装位置： path.plugins: /path/to/plugins 14. 插件托管位置，若列表中的某一个插件未安装，则节点无法启动： plugin.mandatory: mapper-attachments,lang-groovy 15. JVM开始交换时，ElasticSearch表现并不好：你需要保障JVM不进行交换，可以将bootstrap.mlockall设置为true禁止交换： bootstrap.mlockall: true 请确保ES_MIN_MEM和ES_MAX_MEM的值是一样的，并且能够为ElasticSearch分配足够的内在，并为系统操作保留足够的内存。 16. 默认情况下，ElasticSearch使用0.0.0.0地址，并为http传输开启9200-9300端口，为节点到节点的通信开启9300-9400端口，也可以自行设置IP地址： network.bind_host: 192.168.0.1 17. publish_host设置其他节点连接此节点的地址，如果不设置的话，则自动获取，publish_host的地址必须为真实地址： network.publish_host: 192.168.0.1 18. bind_host和publish_host可以一起设置： network.host: 192.168.0.1 19. 可以定制该节点与其他节点交互的端口： transport.tcp.port: 9300 20. 节点间交互时，可以设置是否压缩，转为为不压缩： transport.tcp.compress: true 21. 可以为Http传输监听定制端口： http.port: 9200 22. 设置内容的最大长度： http.max_content_length: 100mb 23. 禁止HTTP http.enabled: false 24. 网关允许在所有集群重启后持有集群状态，集群状态的变更都会被保存下来，当第一次启用集群时，可以从网关中读取到状态，默认网关类型（也是推荐的）是local： gateway.type: local 25. 允许在N个节点启动后恢复过程： gateway.recover_after_nodes: 1 26. 设置初始化恢复过程的超时时间： gateway.recover_after_time: 5m 27. 设置该集群中可存在的节点上限： gateway.expected_nodes: 2 28. 设置一个节点的并发数量，有两种情况，一种是在初始复苏过程中： cluster.routing.allocation.node_initial_primaries_recoveries: 4 另一种是在添加、删除节点及调整时：cluster.routing.allocation.node_concurrent_recoveries: 2 29. 设置复苏时的吞吐量，默认情况下是无限的： indices.recovery.max_size_per_sec: 0 30. 设置从对等节点恢复片段时打开的流的数量上限： indices.recovery.concurrent_streams: 5 31. 设置一个集群中主节点的数量，当多于三个节点时，该值可在2-4之间： discovery.zen.minimum_master_nodes: 1 32. 设置ping其他节点时的超时时间，网络比较慢时可将该值设大： discovery.zen.ping.timeout: 3shttp://elasticsearch.org/guide/reference/modules/discovery/zen.html上有更多关于discovery的设置。 33. 禁止当前节点发现多个集群节点，默认值为true： discovery.zen.ping.multicast.enabled: false 34. 设置新节点被启动时能够发现的主节点列表（主要用于不同网段机器连接）： discovery.zen.ping.unicast.hosts: [“host1”, “host2:port”, “host3[portX-portY]”] 35.设置是否可以通过正则或者_all删除或者关闭索引 action.destructive_requires_name 默认false 允许 可设置true不允许","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"ly2513.github.com/tags/ElasticSearch/"}]},{"title":"每天掌握一个Linux命令(42) : kill命令","date":"2017-10-20T03:29:34.000Z","path":"2017/10/20/每天掌握一个Linux命令-42-kill命令/","text":"Linux中的kill命令用来终止指定的进程（terminate a process）的运行，是Linux下进程管理的常用命令。通常，终止一个前台进程可以使用Ctrl+C键，但是，对于一个后台进程就须用kill命令来终止，我们就需要先使用ps/pidof/pstree/top等工具获取进程PID，然后使用kill命令来杀掉该进程。kill命令是通过向进程发送指定的信号来结束相应进程的。在默认情况下，采用编号为15的TERM信号。TERM信号将终止所有不能捕获该信号的进程。对于那些可以捕获该信号的进程就要用编号为9的kill信号，强行“杀掉”该进程。 1．命令格式： kill[参数][进程号] 2．命令功能： 发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用“-KILL” 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。 3．命令参数： -l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称-a 当处理当前进程时，不限制命令名和进程号的对应关系-p 指定kill 命令只打印相关进程的进程号，而不发送任何信号-s 指定发送信号-u 指定用户 注意： 1、kill命令可以带信号号码选项，也可以不带。如果没有信号号码，kill命令就会发出终止信号(15)，这个信号可以被进程捕获，使得进程在退出之前可以清理并释放资源。也可以用kill向进程发送特定的信号。例如： kill -2 123 它的效果等同于在前台运行PID为123的进程时按下Ctrl+C键。但是，普通用户只能使用不带signal参数的kill命令或最多使用-9信号。 2、kill可以带有进程ID号作为参数。当用kill向这些进程发送信号时，必须是这些进程的主人。如果试图撤销一个没有权限撤销的进程或撤销一个不存在的进程，就会得到一个错误信息。 3、可以向多个进程发信号或终止它们。 4、当kill成功地发送了信号后，shell会在屏幕上显示出进程的终止信息。有时这个信息不会马上显示，只有当按下Enter键使shell的命令提示符再次出现时，才会显示出来。 5、应注意，信号使进程强行终止，这常会带来一些副作用，如数据丢失或者终端无法恢复到正常状态。发送信号时必须小心，只有在万不得已时，才用kill信号(9)，因为进程不能首先捕获它。要撤销所有的后台作业，可以输入kill 0。因为有些在后台运行的命令会启动多个进程，跟踪并找到所有要杀掉的进程的PID是件很麻烦的事。这时，使用kill 0来终止所有由当前shell启动的进程，是个有效的方法。 4．使用实例： 实例一：列出所有信号名称命令：kill -l 输出： 1234567891011121314151617[root@localhost test6]# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR213) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+439) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+1247) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-1451) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-1055) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-659) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 说明：只有第9种信号(SIGKILL)才可以无条件终止进程，其他信号进程都有权利忽略。 下面是常用的信号： HUP 1 终端断线INT 2 中断（同 Ctrl + C）QUIT 3 退出（同 Ctrl + \\）TERM 15 终止KILL 9 强制终止CONT 18 继续（与STOP相反， fg/bg命令）STOP 19 暂停（同 Ctrl + Z） 实例二：得到指定信号的数值命令：kill -l 输出： 12345[root@localhost test6]# kill -l KILL[root@localhost test6]# kill -l SIGKILL[root@localhost test6]# kill -l TERM[root@localhost test6]# kill -l SIGTERM[root@localhost test6]# 实例三 ：先用ps查找进程，然后用kill杀掉命令：kill 3268 输出：1234567[root@localhost test6]# ps -ef|grep vimroot 3268 2884 0 16:21 pts/1 00:00:00 vim install.logroot 3370 2822 0 16:21 pts/0 00:00:00 grep vim[root@localhost test6]# kill 3268[root@localhost test6]# kill 3268-bash: kill: (3268) - 没有那个进程[root@localhost test6]# 实例四：彻底杀死进程命令：kill –9 3268 输出： 1234567[root@localhost test6]# ps -ef|grep vimroot 3268 2884 0 16:21 pts/1 00:00:00 vim install.logroot 3370 2822 0 16:21 pts/0 00:00:00 grep vim[root@localhost test6]# kill –9 3268[root@localhost test6]# kill 3268-bash: kill: (3268) - 没有那个进程[root@localhost test6]# 实例五：杀死指定用户所有进程命令：kill -9 $(ps -ef | grep peidalinux)kill -u peidalinux 输出： 12[root@localhost ~]# kill -9 $(ps -ef | grep peidalinux)[root@localhost ~]# kill -u peidalinux 说明： 方法一，过滤出hnlinux用户进程并杀死 实例六：init进程是不可杀的命令：kill -9 1 输出： 12345678910111213[root@localhost ~]# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3]root 17563 17534 0 17:37 pts/1 00:00:00 grep init[root@localhost ~]# kill -9 1[root@localhost ~]# kill -HUP 1[root@localhost ~]# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3]root 17565 17534 0 17:38 pts/1 00:00:00 grep init[root@localhost ~]# kill -KILL 1[root@localhost ~]# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3]root 17567 17534 0 17:38 pts/1 00:00:00 grep init[root@localhost ~]# 说明： init是Linux系统操作中不可缺少的程序之一。所谓的init进程，它是一个由内核启动的用户级进程。内核自行启动（已经被载入内存，开始运行，并已初始化所有的设备驱动程序和数据结构等）之后，就通过启动一个用户级程序init的方式，完成引导进程。所以,init始终是第一个进程（其进程编号始终为1）。 其它所有进程都是init进程的子孙。init进程是不可杀的！","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"kill","slug":"kill","permalink":"ly2513.github.com/tags/kill/"}]},{"title":"每天掌握一个Linux命令(41) : ps命令","date":"2017-10-20T03:10:32.000Z","path":"2017/10/20/每天掌握一个Linux命令-41-ps命令/","text":"Linux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。 要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，而 ps 命令就是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 ps 为我们提供了进程的一次性的查看，它所提供的查看结果并不动态连续的；如果想对进程时间监控，应该用 top 工具。 kill 命令用于杀死进程。 linux上进程有5种状态: 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码: D 不可中断 uninterruptible sleep (usually IO)R 运行 runnable (on run queue)S 中断 sleepingT 停止 traced or stoppedZ 僵死 a defunct (”zombie”) process 1．命令格式： ps[参数] 2．命令功能： 用来显示当前进程的状态 3．命令参数： a 显示所有进程-a 显示同一终端下的所有程序-A 显示所有进程c 显示进程的真实名称-N 反向选择-e 等于“-A”e 显示环境变量f 显示程序间的关系-H 显示树状结构r 显示当前终端的进程T 显示当前终端的所有程序u 指定用户的所有进程-au 显示较详细的资讯-aux 显示所有包含其他使用者的行程-C&lt;命令&gt; 列出指定命令的状况–lines&lt;行数&gt; 每页显示的行数–width&lt;字符数&gt; 每页显示的字符数–help 显示帮助信息–version 显示版本显示 4．使用实例： 实例一：显示所有进程信息命令：ps -A 输出： 12345678910111213141516[root@localhost test6]# ps -A PID TTY TIME CMD 1 ? 00:00:00 init 2 ? 00:00:01 migration/0 3 ? 00:00:00 ksoftirqd/0 4 ? 00:00:01 migration/1 5 ? 00:00:00 ksoftirqd/1 6 ? 00:29:57 events/0 7 ? 00:00:00 events/1 8 ? 00:00:00 khelper 49 ? 00:00:00 kthread 54 ? 00:00:00 kblockd/0 55 ? 00:00:00 kblockd/1 56 ? 00:00:00 kacpid 217 ? 00:00:00 cqueue/0 ……省略部分结果 实例二：显示指定用户信息命令：ps -u root 输出： 123456789101112131415[root@localhost test6]# ps -u root PID TTY TIME CMD 1 ? 00:00:00 init 2 ? 00:00:01 migration/0 3 ? 00:00:00 ksoftirqd/0 4 ? 00:00:01 migration/1 5 ? 00:00:00 ksoftirqd/1 6 ? 00:29:57 events/0 7 ? 00:00:00 events/1 8 ? 00:00:00 khelper 49 ? 00:00:00 kthread 54 ? 00:00:00 kblockd/0 55 ? 00:00:00 kblockd/1 56 ? 00:00:00 kacpid ……省略部分结果 实例三：显示所有进程信息，连同命令行命令：ps -ef 输出： 123456789101112131415[root@localhost test6]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 Nov02 ? 00:00:00 init [3]root 2 1 0 Nov02 ? 00:00:01 [migration/0]root 3 1 0 Nov02 ? 00:00:00 [ksoftirqd/0]root 4 1 0 Nov02 ? 00:00:01 [migration/1]root 5 1 0 Nov02 ? 00:00:00 [ksoftirqd/1]root 6 1 0 Nov02 ? 00:29:57 [events/0]root 7 1 0 Nov02 ? 00:00:00 [events/1]root 8 1 0 Nov02 ? 00:00:00 [khelper]root 49 1 0 Nov02 ? 00:00:00 [kthread]root 54 49 0 Nov02 ? 00:00:00 [kblockd/0]root 55 49 0 Nov02 ? 00:00:00 [kblockd/1]root 56 49 0 Nov02 ? 00:00:00 [kacpid]……省略部分结果 实例四： ps 与grep 常用组合用法，查找特定进程命令：ps -ef|grep ssh 输出： 1234[root@localhost test6]# ps -ef|grep sshroot 2720 1 0 Nov02 ? 00:00:00 /usr/sbin/sshdroot 17394 2720 0 14:58 ? 00:00:00 sshd: root@pts/0root 17465 17398 0 15:57 pts/0 00:00:00 grep ssh 实例五：将目前属于您自己这次登入的 PID 与相关信息列示出来命令：ps -l 输出： 1234[root@localhost test6]# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 17398 17394 0 75 0 - 16543 wait pts/0 00:00:00 bash4 R 0 17469 17398 0 77 0 - 15877 - pts/0 00:00:00 ps 说明：各相关信息的意义：F 代表这个程序的旗标 (flag)， 4 代表使用者为 super userS 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍UID 程序被该 UID 所拥有PID 就是这个程序的 ID ！PPID 则是其上级父程序的IDC CPU 使用的资源百分比PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍NI 这个是 Nice 值，在下一小节我们会持续介绍ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“SZ 使用掉的内存大小WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作TTY 登入者的终端机位置TIME 使用掉的 CPU 时间。CMD 所下达的指令为何 在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 实例六：列出目前所有的正在内存当中的程序命令：ps aux 输出： 123456789101112131415[root@localhost test6]# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 10368 676 ? Ss Nov02 0:00 init [3]root 2 0.0 0.0 0 0 ? S&lt; Nov02 0:01 [migration/0]root 3 0.0 0.0 0 0 ? SN Nov02 0:00 [ksoftirqd/0]root 4 0.0 0.0 0 0 ? S&lt; Nov02 0:01 [migration/1]root 5 0.0 0.0 0 0 ? SN Nov02 0:00 [ksoftirqd/1]root 6 0.0 0.0 0 0 ? S&lt; Nov02 29:57 [events/0]root 7 0.0 0.0 0 0 ? S&lt; Nov02 0:00 [events/1]root 8 0.0 0.0 0 0 ? S&lt; Nov02 0:00 [khelper]root 49 0.0 0.0 0 0 ? S&lt; Nov02 0:00 [kthread]root 54 0.0 0.0 0 0 ? S&lt; Nov02 0:00 [kblockd/0]root 55 0.0 0.0 0 0 ? S&lt; Nov02 0:00 [kblockd/1]root 56 0.0 0.0 0 0 ? S&lt; Nov02 0:00 [kacpid]……省略部分结果 说明： USER：该 process 属于那个使用者账号的PID ：该 process 的号码%CPU：该 process 使用掉的 CPU 资源百分比%MEM：该 process 所占用的物理内存百分比VSZ ：该 process 使用掉的虚拟内存量 (Kbytes)RSS ：该 process 占用的固定的内存量 (Kbytes)TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的，则表示为由网络连接进主机的程序。STAT：该程序目前的状态，主要的状态有R ：该程序目前正在运作，或者是可被运作S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。T ：该程序目前正在侦测或者是停止了Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态START：该 process 被触发启动的时间TIME ：该 process 实际使用 CPU 运作的时间COMMAND：该程序的实际指令 实例七：列出类似程序树的程序显示命令：ps -axjf 输出： 123456789101112131415[root@localhost test6]# ps -axjfWarning: bad syntax, perhaps a bogus '-'? See /usr/share/doc/procps-3.2.7/FAQ PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 0 1 1 1 ? -1 Ss 0 0:00 init [3] 1 2 1 1 ? -1 S&lt; 0 0:01 [migration/0] 1 3 1 1 ? -1 SN 0 0:00 [ksoftirqd/0] 1 4 1 1 ? -1 S&lt; 0 0:01 [migration/1] 1 5 1 1 ? -1 SN 0 0:00 [ksoftirqd/1] 1 6 1 1 ? -1 S&lt; 0 29:58 [events/0] 1 7 1 1 ? -1 S&lt; 0 0:00 [events/1] 1 8 1 1 ? -1 S&lt; 0 0:00 [khelper] 1 49 1 1 ? -1 S&lt; 0 0:00 [kthread] 49 54 1 1 ? -1 S&lt; 0 0:00 \\_ [kblockd/0] 49 55 1 1 ? -1 S&lt; 0 0:00 \\_ [kblockd/1] 49 56 1 1 ? -1 S&lt; 0 0:00 \\_ [kacpid] 实例八：找出与 cron 与 syslog 这两个服务有关的 PID 号码命令：输出： 12345[root@localhost test6]# ps aux | egrep '(cron|syslog)'root 2682 0.0 0.0 83384 2000 ? Sl Nov02 0:00 /sbin/rsyslogd -i /var/run/syslogd.pid -c 5root 2735 0.0 0.0 74812 1140 ? Ss Nov02 0:00 crondroot 17475 0.0 0.0 61180 832 pts/0 S+ 16:27 0:00 egrep (cron|syslog)[root@localhost test6]# 说明： 其他实例： 可以用 | 管道和 more 连接起来分页查看 命令：ps -aux |more 把所有进程显示出来，并输出到ps001.txt文件 命令：ps -aux &gt; ps001.txt 输出指定的字段 命令：ps -o pid,ppid,pgrp,session,tpgid,comm 输出： 12345[root@localhost test6]# ps -o pid,ppid,pgrp,session,tpgid,comm PID PPID PGRP SESS TPGID COMMAND17398 17394 17398 17398 17478 bash17478 17398 17478 17398 17478 ps[root@localhost test6]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"ps","slug":"ps","permalink":"ly2513.github.com/tags/ps/"}]},{"title":"每天掌握一个Linux命令(40): wc命令","date":"2017-10-20T02:07:17.000Z","path":"2017/10/20/每天掌握一个Linux命令-40-wc命令/","text":"Linux系统中的wc(Word Count)命令的功能为统计指定文件中的字节数、字数、行数，并将统计结果显示输出。 1．命令格式： wc [选项]文件… 2．命令功能： 统计指定文件中的字节数、字数、行数，并将统计结果显示输出。该命令统计指定文件中的字节数、字数、行数。如果没有给出文件名，则从标准输入读取。wc同时也给出所指定文件的总统计数。 3．命令参数： -c 统计字节数。-l 统计行数。-m 统计字符数。这个标志不能与 -c 标志一起使用。-w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L 打印最长行的长度。-help 显示帮助信息–version 显示版本信息 4．使用实例： 实例一：查看文件的字节数、字数、行数命令：wc test.txt 输出： 1234567891011121314151617181920[root@localhost test]# cat test.txthnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint[root@localhost test]# wc test.txt 7 8 70 test.txt[root@localhost test]# wc -l test.txt7 test.txt[root@localhost test]# wc -c test.txt70 test.txt[root@localhost test]# wc -w test.txt8 test.txt[root@localhost test]# wc -m test.txt70 test.txt[root@localhost test]# wc -L test.txt17 test.txt 说明： 7 8 70 test.txt行数 单词数 字节数 文件名 实例二：用wc命令怎么做到只打印统计数字不打印文件名命令：wc -l输出： 1234[root@localhost test]# wc -l test.txt7 test.txt[root@localhost test]# cat test.txt |wc -l7[root@localhost test]# 说明： 使用管道线，这在编写shell脚本时特别有用。 实例三：用来统计当前目录下的文件数命令： ls -l | wc -l 输出： 12345678910111213[root@localhost test]# cd test6[root@localhost test6]# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2014.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2015.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2016.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2017.log[root@localhost test6]# ls -l | wc -l8[root@localhost test6]# 说明：数量中包含当前目录","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"wc","slug":"wc","permalink":"ly2513.github.com/tags/wc/"}]},{"title":"怎么让自己的项目使用Composer的国内镜像","date":"2017-09-25T02:04:58.000Z","path":"2017/09/25/怎么让自己的项目使用Composer的国内镜像/","text":"Packagist 镜像使用方法 还没安装 Composer 吗？请往下看如何安装 Composer 。 镜像用法有两种方式启用本镜像服务： 系统全局配置： 即将配置信息添加到 Composer 的全局配置文件 config.json 中。见“方法一”单个项目配置： 将配置信息添加到某个项目的 composer.json 文件中。见“方法二” 方法一： 修改 composer 的全局配置文件（推荐方式） 打开命令行窗口（windows用户）或控制台（Linux、Mac 用户）并执行如下命令： 1composer config -g repo.packagist composer https://packagist.phpcomposer.com 方法二： 修改当前项目的 composer.json 配置文件： 打开命令行窗口（windows用户）或控制台（Linux、Mac 用户），进入你的项目的根目录（也就是 composer.json 文件所在目录），执行如下命令： 1composer config repo.packagist composer https://packagist.phpcomposer.com 上述命令将会在当前项目中的 composer.json 文件的末尾自动添加镜像的配置信息（你也可以自己手工添加）： 123456\"repositories\": &#123; \"packagist\": &#123; \"type\": \"composer\", \"url\": \"https://packagist.phpcomposer.com\" &#125;&#125; 以 laravel 项目的 composer.json 配置文件为例，执行上述命令后如下所示（注意最后几行）： 1234567891011121314151617181920&#123; \"name\": \"laravel/laravel\", \"description\": \"The Laravel Framework.\", \"keywords\": [\"framework\", \"laravel\"], \"license\": \"MIT\", \"type\": \"project\", \"require\": &#123; \"php\": \"&gt;=5.5.9\", \"laravel/framework\": \"5.2.*\" &#125;, \"config\": &#123; \"preferred-install\": \"dist\" &#125;, \"repositories\": &#123; \"packagist\": &#123; \"type\": \"composer\", \"url\": \"https://packagist.phpcomposer.com\" &#125; &#125;&#125; OK，一切搞定！试一下 composer install 来体验飞一般的速度吧！ 镜像原理：一般情况下，安装包的数据（主要是zip文件）一般是从github.com上下载的，安装包的元数据是从packagist.org上下载的。 然而，由于众所周知的原因，国外的网站连接速度很慢，并且随时可能被“墙”甚至“不存在”。 “Packagist 中国全量镜像”所做的就是缓存所有安装包和元数据到国内的机房并通过国内的 CDN 进行加速，这样就不必再去向国外的网站发起请求，从而达到加速composer install以及composer update的过程，并且更加快速、稳定。因此，即使packagist.org、github.com发生故障（主要是连接速度太慢和被墙），你仍然可以下载、更新安装包。","tags":[{"name":"PHP","slug":"PHP","permalink":"ly2513.github.com/tags/PHP/"},{"name":"Composer","slug":"Composer","permalink":"ly2513.github.com/tags/Composer/"}]},{"title":"使用Composer管理自己开发组件","date":"2017-09-21T02:49:46.000Z","path":"2017/09/21/使用Composer管理自己开发组件/","text":"提交PHP组件到Packagist 发布自己的Composer包 Composer是PHP的一个依赖管理工具，它使得PHP焕发新的生机，有了现代化的WEB开发规范，Packagist是PHP组件的库，也有其他的镜像。 在Packagist上提交了一个自己开发的PHP组件，这样其他开发者就可以使用Composer使用这个包了。这个组件并没什么功能，主要是看看提交PHP组件的流程，并记录了过程中遇到的问题及解决方法，以供参考。 提交PHP组件步骤： 1.新建一个项目目录，创建一个composer.json文件，格式如下： 12345678&#123; \"name\": \"your-vendor-name/package-name\", \"description\": \"A short description of what your package does\", \"require\": &#123; \"php\": \"^5.3.3 || ^7.0\", \"another-vendor/package\": \"1.*\" &#125;&#125; 这个json格式的文件中包含组件的基本信息，这里还差自动加载的方式，要根据具体开发模式指定自动加载方式，这里require可以指定这个组件依赖的其他组件，composer都会自动解决依赖。 附上完整的composer.json内容作为示例：12345678&#123; \"name\": \"your-vendor-name/package-name\", \"description\": \"A short description of what your package does\", \"require\": &#123; \"php\": \"^5.3.3 || ^7.0\", \"another-vendor/package\": \"1.*\" &#125;&#125; 这个json格式的文件中包含组件的基本信息，这里还差自动加载的方式，要根据具体开发模式指定自动加载方式，这里require可以指定这个组件依赖的其他组件，composer都会自动解决依赖。 附上完整的composer.json内容作为示例： 123456789101112131415161718192021&#123; \"name\": \"ly2513/redis-queue\", \"description\": \"Display page edis-queue\", \"keywords\": [\"ypphp\",\"queue\"], \"homepage\": \"https://github.com/ly2513/redis-queue\", \"license\": \"MIT\", \"authors\": [ &#123; \"name\": \"gaofeng\", \"email\": \"626375290@qq.com\", \"homepage\": \"https://www.liyongweb.com\", \"role\": \"Developer\" &#125; ], \"require\": &#123; \"php\": \"^5.3.3 || ^7.0\" &#125;, \"autoload\": &#123; \"psr-4\": &#123; \"Loading\\\\\": \"\" &#125; &#125;&#125; 2.开发组件功能 要注意遵循psr规范，使用命名空间。 3.把组件提交到Github上 提交组件到Packagist之前需要先把代码提交到github上，在Packagist只需填写组件的github地址。 4.提交组件地址到Packagist 这样就完成的PHP组件提交到Packagist的过程，具体请参见Packagist官网。 问题：使用composer require找不到组件组件提交到Packagist上，提示发布成功了，但是使用composer命令却找不到组件： 1composer require ly2513/redis-queue 如图： 由于我的composer使用的国内镜像，猜测可能是没有同步的原因，使用这个命令把“源”改回去还是不行。 1composer config -g repo.packagist composer https://packagist.org 原来我的组件还没有在github上发布正式，这个时候还是开发版本dev-master.应该加上dev-master版本。 1composer require ly2513/redis-queue:dev-master 果然指定了dev-master版本可以成功更新组件，但是这样不行，我们要有一个正式版本。 composer-dev-master github发布版本 进入组件的github主页，找到导航上“releases”，点击进去如图页面，就可以创建一个版本，填写好信息之后即可发布版本。 create_release 按照页面上的提示填写内容，完成后发布。发布版本后，通过composer require发现还是找不到包。 设置自动更新版本 auto-update 原来还要在Github上配置一下自动更新。具体步骤参考：点击这里查看 我直接通过手动的方式发送curl请求来设置，这样还简单一点，不过这样每次发新的版本都需要这样请求一下： 1curl -XPOST -H'content-type:application/json' 'https://packagist.org/api/update-package?username=tanteng&amp;apiToken=secret' -d'&#123;\"repository\":&#123;\"url\":\"https://github.com/tanteng/laravel-page-loaded-time\"&#125;&#125;'\\ 返回{“status”:”success”}表示成功。 再打开https://packagist.org/packages/ly2513/redis-queue，发现已经是正式版本了。 我用的是composer国内镜像，因为众所周知的原因，连代码仓库也要被墙，我服！等了几个小时再试，这个时候镜像同步更新了，再次输入：composer require ly2513/redis-queue,这个时候可以成功更新了。如图，vendor文件夹下也自动装载了依赖的其他组件。 success","tags":[{"name":"PHP","slug":"PHP","permalink":"ly2513.github.com/tags/PHP/"},{"name":"Composer","slug":"Composer","permalink":"ly2513.github.com/tags/Composer/"}]},{"title":"PHP多个版本管理","date":"2017-09-21T02:38:24.000Z","path":"2017/09/21/PHP-多个版本管理/","text":"安装php多版本 Mac下默认安装了php但是版本不是很高,用php -v查看php版本是php 5.6，我们希望安装php7又不把php5.6卸载。 运行命令：1brew install php70 运行结果：123456789 Installing php70 from homebrew/phpError: Cannot install homebrew/php/php70 because conflicting formulae are installed. php56: because different php versions install the same binaries.Please `brew unlink php56` before continuing.Unlinking removes a formula&apos;s symlinks from /usr/local.You canlink the formula again after the install finishes.You can --force thisinstall, but the build may fail or cause obscure side-effects in theresulting software. 显示错误，这个时候需要运行命令 brew unlink php56 取消homebrew与php56的关联,再安装php7。不出意外再次使用brew install php70便可成功安装php7。 安装php-version 继续安装php-version（php版本切换工具）： 第一步 brew install php-version 第二步 source $(brew --prefix php-version)/php-version.sh &amp;&amp; php-version 执行php-version查看已存在的php版本,前面带＊的是当前环境正在使用的php版本,使用php-version 版本号 的方式来切换php版本 后话 切换Apache上对应php版本操作如下，找到httpd.conf文件做相应修改即可：123sudo vi /private/etc/apache2/httpd.conf（mac下httpd.conf的默认路径）LoadModule php7_module /usr/local/opt/php70/libexec/apache2/libphp7.so #增加对php7的支持注释掉php5_module一行，重启Apache","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"cli","slug":"cli","permalink":"ly2513.github.com/tags/cli/"}]},{"title":"Mac OS下不产生.DS_Store 隐藏文件","date":"2017-09-19T14:16:54.000Z","path":"2017/09/19/Mac-OS下不产生-DS-Store-隐藏文件/","text":"删除.DS_Store 很烦，直接让它不要出现吧 其实这问题困扰我很久了，每个目录里面都会跑出一个 .DS_Store 的档案，我通常都是用1find /path/to -name 『.DS_Store』 -delete 不过删除了又会自动生成；显然这问题不是只有困扰我而已，都已经有专门命令在解决类似的状况。不过现在这问题得到了『终极的解决办法』，有一篇文章提到了说要怎样关掉这个功能：只要在命令行下执行下列这个指令, 然后重开机就可以啦 打开终端 - （shift + command + N）输入下面的命令 然后重启 OK1defaults write com.apple.desktopservices DSDontWriteNetworkStorestrue true 少了这个,档案会出什么问题吗? 根据其他的说法，这个文件主要是用来储存 『目录是以何种型式显示』的信息，例如说打开的时候要放在萤幕的什么地方啦、要用 small icon/big icon/list 的方式显示之类；意思就是说，如果你不是很在意这些的话，应该是可以大胆地把这个文件给干掉 .DS_Store 是 Finder 用来存储这个文件夹的显示属性的：比如文件图标的摆放位置。删除以后的副作用就是这些信息的失去。（当然，这点副作用其实不是太大。 和别人交换文件（或你做的网页需要上传的时候）应该把 .DS_Store 文件删除比较妥当，因为里面包含了一些你不一定希望别人看见的信息（尤其是网站，通过 .DS_Store 可以知道这个目录里面所有文件的清单，很多时候这是一个不希望出现的问题）。","tags":[{"name":"mac","slug":"mac","permalink":"ly2513.github.com/tags/mac/"}]},{"title":"每天掌握一个Linux命令(39) : grep 命令","date":"2017-09-12T13:54:42.000Z","path":"2017/09/12/每天掌握一个Linux命令-39-grep-命令/","text":"Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。 grep的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。 grep可用于shell脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。 1．命令格式： grep [option] pattern file 2．命令功能： 用于过滤/搜索的特定字符。可使用正则表达式能多种命令配合使用，使用上十分灵活。 3．命令参数： 1234567891011121314151617181920212223242526-a --text #不要忽略二进制的数据。-A&lt;显示行数&gt; --after-context=&lt;显示行数&gt; #除了显示符合范本样式的那一列之外，并显示该行之后的内容。-b --byte-offset #在显示符合样式的那一行之前，标示出该行第一个字符的编号。-B&lt;显示行数&gt; --before-context=&lt;显示行数&gt; #除了显示符合样式的那一行之外，并显示该行之前的内容。-c --count #计算符合样式的列数。-C&lt;显示行数&gt; --context=&lt;显示行数&gt;或-&lt;显示行数&gt; #除了显示符合样式的那一行之外，并显示该行之前后的内容。-d &lt;动作&gt; --directories=&lt;动作&gt; #当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。-e&lt;范本样式&gt; --regexp=&lt;范本样式&gt; #指定字符串做为查找文件内容的样式。-E --extended-regexp #将样式为延伸的普通表示法来使用。-f&lt;规则文件&gt; --file=&lt;规则文件&gt; #指定规则文件，其内容含有一个或多个规则样式，让grep查找符合规则条件的文件内容，格式为每行一个规则样式。-F --fixed-regexp #将样式视为固定字符串的列表。-G --basic-regexp #将样式视为普通的表示法来使用。-h --no-filename #在显示符合样式的那一行之前，不标示该行所属的文件名称。-H --with-filename #在显示符合样式的那一行之前，表示该行所属的文件名称。-i --ignore-case #忽略字符大小写的差别。-l --file-with-matches #列出文件内容符合指定的样式的文件名称。-L --files-without-match #列出文件内容不符合指定的样式的文件名称。-n --line-number #在显示符合样式的那一行之前，标示出该行的列数编号。-q --quiet或--silent #不显示任何信息。-r --recursive #此参数的效果和指定“-d recurse”参数相同。-s --no-messages #不显示错误信息。-v --revert-match #显示不包含匹配文本的所有行。-V --version #显示版本信息。-w --word-regexp #只显示全字符合的列。-x --line-regexp #只显示全列符合的列。-y #此参数的效果和指定“-i”参数相同。 4．规则表达式： grep的规则表达式: 12345678910111213141516^ #锚定行的开始 如：&apos;^grep&apos;匹配所有以grep开头的行。$ #锚定行的结束 如：&apos;grep$&apos;匹配所有以grep结尾的行。. #匹配一个非换行符的字符 如：&apos;gr.p&apos;匹配gr后接一个任意字符，然后是p。* #匹配零个或多个先前字符 如：&apos;*grep&apos;匹配所有一个或多个空格后紧跟grep的行。.* #一起用代表任意字符。[] #匹配一个指定范围内的字符，如&apos;[Gg]rep&apos;匹配Grep和grep。[^] #匹配一个不在指定范围内的字符，如：&apos;[^A-FH-Z]rep&apos;匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。\\(..\\) #标记匹配字符，如&apos;\\(love\\)&apos;，love被标记为1。\\&lt; #锚定单词的开始，如:&apos;\\&lt;grep&apos;匹配包含以grep开头的单词的行。\\&gt; #锚定单词的结束，如&apos;grep\\&gt;&apos;匹配包含以grep结尾的单词的行。x\\&#123;m\\&#125; #重复字符x，m次，如：&apos;0\\&#123;5\\&#125;&apos;匹配包含5个o的行。x\\&#123;m,\\&#125; #重复字符x,至少m次，如：&apos;o\\&#123;5,\\&#125;&apos;匹配至少有5个o的行。x\\&#123;m,n\\&#125; #重复字符x，至少m次，不多于n次，如：&apos;o\\&#123;5,10\\&#125;&apos;匹配5--10个o的行。\\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：&apos;G\\w*p&apos;匹配以G后跟零个或多个文字或数字字符，然后是p。\\W #\\w的反置形式，匹配一个或多个非单词字符，如点号句号等。\\b #单词锁定符，如: &apos;\\bgrep\\b&apos;只匹配grep。 POSIX字符: 为了在不同国家的字符编码中保持一至，POSIX(The Portable Operating System Interface)增加了特殊的字符类，如[:alnum:]是[A-Za-z0-9]的另一个写法。要把它们放到[]号内才能成为正则表达式，如[A- Za-z0-9]或[[:alnum:]]。在linux下的grep除fgrep外，都支持POSIX的字符类。 [:alnum:] #文字数字字符[:alpha:] #文字字符[:digit:] #数字字符[:graph:] #非空字符（非空格、控制字符）[:lower:] #小写字符[:cntrl:] #控制字符[:print:] #非空字符（包括空格）[:punct:] #标点符号[:space:] #所有空白字符（新行，空格，制表符）[:upper:] #大写字符[:xdigit:] #十六进制数字（0-9，a-f，A-F） 5．使用实例： 实例一：查找指定进程命令：ps -ef|grep svn 输出： 1234[root@localhost ~]# ps -ef|grep svnroot 4943 1 0 Dec05 ? 00:00:00 svnserve -d -r /opt/svndata/grape/root 16867 16838 0 19:53 pts/0 00:00:00 grep svn[root@localhost ~]# 说明： 第一条记录是查找出的进程；第二条结果是grep进程本身，并非真正要找的进程。 实例二：查找指定进程个数命令：ps -ef|grep svn -cps -ef|grep -c svn 输出：12345[root@localhost ~]# ps -ef|grep svn -c2[root@localhost ~]# ps -ef|grep -c svn2[root@localhost ~]# 实例三：从文件中读取关键词进行搜索命令：cat test.txt | grep -f test2.txt 输出： 1234567891011121314151617[root@localhost test]# cat test.txthnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint[root@localhost test]# cat test2.txtlinuxRedhat[root@localhost test]# cat test.txt | grep -f test2.txthnlinuxubuntu linuxRedhatlinuxmint[root@localhost test]# 说明： 输出test.txt文件中含有从test2.txt文件中读取出的关键词的内容行 实例四：从文件中读取关键词进行搜索 且显示行号命令：cat test.txt | grep -nf test2.txt 输出：1234567891011121314151617[root@localhost test]# cat test.txthnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint[root@localhost test]# cat test2.txtlinuxRedhat[root@localhost test]# cat test.txt | grep -nf test2.txt1:hnlinux4:ubuntu linux6:Redhat7:linuxmint[root@localhost test]# 说明： 输出test.txt文件中含有从test2.txt文件中读取出的关键词的内容行，并显示每一行的行号 实例五：从文件中查找关键词命令：grep ‘linux’ test.txt 输出： 123456789[root@localhost test]# grep 'linux' test.txthnlinuxubuntu linuxlinuxmint[root@localhost test]# grep -n 'linux' test.txt1:hnlinux4:ubuntu linux7:linuxmint[root@localhost test]# 实例六：从多个文件中查找关键词命令：grep ‘linux’ test.txt test2.txt 输出：1234567891011[root@localhost test]# grep -n 'linux' test.txt test2.txttest.txt:1:hnlinuxtest.txt:4:ubuntu linuxtest.txt:7:linuxminttest2.txt:1:linux[root@localhost test]# grep 'linux' test.txt test2.txttest.txt:hnlinuxtest.txt:ubuntu linuxtest.txt:linuxminttest2.txt:linux[root@localhost test]# 说明： 多文件时，输出查询到的信息内容行时，会把文件的命名在行最前面输出并且加上”:”作为标示符 实例七：grep不显示本身进程命令：ps aux|grep [s]shps aux | grep ssh | grep -v “grep” 输出：1234567891011[root@localhost test]# ps aux|grep sshroot 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0root 16901 0.0 0.0 61180 764 pts/0 S+ 20:31 0:00 grep ssh[root@localhost test]# ps aux|grep \\[s]sh][root@localhost test]# ps aux|grep \\[s]shroot 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0[root@localhost test]# ps aux | grep ssh | grep -v \"grep\"root 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0 实例八：找出已u开头的行内容命令：cat test.txt |grep ^u 输出： 1234[root@localhost test]# cat test.txt |grep ^uubuntuubuntu linux[root@localhost test]# 实例九：输出非u开头的行内容命令：cat test.txt |grep ^[^u] 输出：1234567[root@localhost test]# cat test.txt |grep ^[^u]hnlinuxpeida.cnblogs.comredhatRedhatlinuxmint[root@localhost test]# 实例十：输出以hat结尾的行内容命令：cat test.txt |grep hat$ 输出：1234[root@localhost test]# cat test.txt |grep hat$redhatRedhat[root@localhost test]# 实例十一：命令：输出：12345[root@localhost test]# ifconfig eth0|grep \"[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;\" inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0[root@localhost test]# ifconfig eth0|grep -E \"([0-9]&#123;1,3&#125;\\.)&#123;3&#125;[0-9]\" inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0[root@localhost test]# 说明： 实例十二：显示包含ed或者at字符的内容行命令：cat test.txt |grep -E “ed|at” 输出：123456[root@localhost test]# cat test.txt |grep -E \"peida|com\"peida.cnblogs.com[root@localhost test]# cat test.txt |grep -E \"ed|at\"redhatRedhat[root@localhost test]# 实例十三：显示当前目录下面以.txt 结尾的文件中的所有包含每个字符串至少有7个连续小写字符的字符串的行命令：grep ‘[a-z]{7}‘ *.txt 输出：12345[root@localhost test]# grep '[a-z]\\&#123;7\\&#125;' *.txttest.txt:hnlinuxtest.txt:peida.cnblogs.comtest.txt:linuxmint[root@localhost test]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"grep","slug":"grep","permalink":"ly2513.github.com/tags/grep/"}]},{"title":"每天掌握一个Linux命令(38) : cal命令","date":"2017-09-12T13:36:14.000Z","path":"2017/09/12/每天掌握一个Linux命令-38-cal命令/","text":"cal命令可以用来显示公历（阳历）日历。公历是现在国际通用的历法，又称格列历，通称阳历。“阳历”又名“太阳历”，系以地球绕行太阳一周为一年，为西方各国所通用，故又名“西历”。 1．命令格式： cal [参数][月份][年份] 2．命令功能： 用于查看日历等时间信息，如只有一个参数，则表示年份(1-9999)，如有两个参数，则表示月份和年份 3．命令参数： 123456-1 显示一个月的月历-3 显示系统前一个月，当前月，下一个月的月历-s 显示星期天为一个星期的第一天，默认的格式-m 显示星期一为一个星期的第一天-j 显示在当年中的第几天（一年日期按天算，从1月1号算起，默认显示当前月在一年中的天数）-y 显示当前年份的日历 4．使用实例： 实例一：显示当前月份日历命令：cal 输出： 123456789[root@localhost ~]# cal 十二月 2012日 一 二 三 四 五 六 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1516 17 18 19 20 21 2223 24 25 26 27 28 2930 31[root@localhost ~]# 实例二：显示指定月份的日历命令：cal 9 2012 输出： 123456789[root@localhost ~]# cal 9 2012 九月 2012日 一 二 三 四 五 六 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1516 17 18 19 20 21 2223 24 25 26 27 28 2930 实例三：显示2013年日历命令：cal -y 2013cal 2013 输出： 实例四：显示自1月1日的天数命令：cal -j 输出：123456789[root@localhost ~]# cal -j 十二月 2012 日 一 二 三 四 五 六 336337 338 339 340 341 342 343344 345 346 347 348 349 350351 352 353 354 355 356 357358 359 360 361 362 363 364365 366[root@localhost ~]# 实例五：星期一显示在第一列命令：cal -m 输出：123456789[root@localhost ~]# cal -m 十二月 2012一 二 三 四 五 六 日 1 2 3 4 5 6 7 8 910 11 12 13 14 15 1617 18 19 20 21 22 2324 25 26 27 28 29 3031[root@localhost ~]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"cal","slug":"cal","permalink":"ly2513.github.com/tags/cal/"}]},{"title":"Thrift笔记—IDL基本类型","date":"2017-08-27T17:05:40.000Z","path":"2017/08/28/Thrift笔记—IDL基本类型/","text":"thrift 采用IDL（Interface Definition Language）来定义通用的服务接口，并通过生成不同的语言代理实现来达到跨语言、平台的功能。在thrift的IDL中可以定义以下一些类型：基本数据类型，结构体，容器，异常、服务 1、基本类型 bool: 布尔值 (true or false), one byte byte: 有符号字节 i16: 16位有符号整型 i32: 32位有符号整型 i64: 64位有符号整型 double: 64位浮点型 string: Encoding agnostic text or binary string基本类型中基本都是有符号数，因为有些语言没有无符号数，所以Thrift不支持无符号整型。 2、特殊类型1binary: Blob (byte array) a sequence of unencoded bytes 这是string类型的一种变形，主要是为java使用，目前我主要使用C++的语言，所以java的这个类型没有用过 3、structthrift中struct是定义为一种对象，和面向对象语言的class差不多.,但是struct有以下一些约束： struct不能继承，但是可以嵌套，不能嵌套自己。其成员都是有明确类型成员是被正整数编号过的，其中的编号使不能重复的，这个是为了在传输过程中编码使用。成员分割符可以是逗号（,）或是分号（;），而且可以混用，但是为了清晰期间，建议在定义中只使用一种，比如C++学习者可以就使用分号（;）。字段会有optional和required之分和protobuf一样，但是如果不指定则为无类型—可以不填充该值，但是在序列化传输的时候也会序列化进去，optional是不填充则部序列化，required是必须填充也必须序列化。每个字段可以设置默认值同一文件可以定义多个struct，也可以定义在不同的文件，进行include引入。数字标签作用非常大，但是随着项目开发的不断发展，也许字段会有变化，但是建议不要轻易修改这些数字标签，修改之后如果没有同步客户端和服务器端会让一方解析出问题。123456struct Report&#123; 1: required string msg, //改字段必须填写 2: optional i32 type = 0; //默认值 3: i32 time //默认字段类型为optional&#125; 规范的struct定义中的每个域均会使用required或者 optional关键字进行标识。如果required标识的域没有赋值，Thrift将给予提示；如果optional标识的域没有赋值，该域将不会被序列化传输；如果某个optional标识域有缺省值而用户没有重新赋值，则该域的值一直为缺省值；如果某个optional标识域有缺省值或者用户已经重新赋值，而不设置它的__isset为true，也不会被序列化传输。 4、容器（Containers） Thrift容器与目前流行编程语言的容器类型相对应，有3种可用容器类型： list: 元素类型为t的有序表，容许元素重复。对应c++的vector，java的ArrayList或者其他语言的数组（官方文档说是ordered list不知道如何理解？排序的？c++的vector不排序）set:元素类型为t的无序表，不容许元素重复。对应c++中的set，java中的HashSet,python中的set，php中没有set，则转换为list类型了map: 键类型为t，值类型为t的kv对，键不容许重复。对用c++中的map, Java的HashMap, PHP 对应 array, Python/Ruby 的dictionary。 容器中元素类型可以是除了service外的任何合法Thrift类型（包括结构体和异常）。为了最大的兼容性，map的key最好是thrift的基本类型，有些语言不支持复杂类型的key，JSON协议只支持那些基本类型的key。 容器都是同构容器，不失异构容器。 例子123456struct Test &#123; 1: map&lt;Numberz, UserId&gt; user_map, 2: set&lt;Numberz&gt; num_sets, 3: list&lt;Stusers&gt; users&#125; 5、枚举（enmu）很多语言都有枚举，意义都一样。比如，当定义一个消息类型时，它只能是预定义的值列表中的一个，可以用枚举实现。说明： 编译器默认从0开始赋值 可以赋予某个常量某个整数 允许常量是十六进制整数 末尾没有分号 给常量赋缺省值时，使用常量的全称 注意，不同于protocal buffer，thrift不支持枚举类嵌套，枚举常量必须是32位的正整数1234567891011121314151617181920enum EnOpType &#123; CMD_OK = 0, // (0) CMD_NOT_EXIT = 2000, // (2000) CMD_EXIT = 2001, // (2001) CMD_ADD = 2002 // (2002)&#125;struct StUser &#123; 1: required i32 userId; 2: required string userName; 3: optional EnOpType cmd_code = EnOpType.CMD_OK; // (0) 4: optional string language = “english”&#125; 枚举类型和struct编译后的c++代码1234567891011121314151617struct EnOpType &#123; enum type &#123; CMD_OK = 0, CMD_NOT_EXIT = 2000, CMD_EXIT = 2001, CMD_ADD = 2002 &#125;;&#125;;class StUser &#123;public:。。。。。。 枚举类型编译后被struct封装了，struct编译后成了class，当然其中方法了多了很多。 6、常量定义和类型定义 Thrift允许定义跨语言使用的常量，复杂的类型和结构体可使用JSON形式表示。1const i32 INT_CONST = 1234; const EnOpType myEnOpType = EnOpType.CMD_EXIT; //2001 说明：分号可有可无。支持16进制。 Thrift支持C/C++类型定义。123typedef i32 MyInteger // atypedef StUser ReU // btypedef i64 UserId 说明：a.末尾没有逗号。b. Struct也可以使用typedef。 7、异常（Exceptions） Thrift结构体在概念上类似于（similar to）C语言结构体类型—将相关属性封装在一起的简便方式。Thrift结构体将会被转换成面向对象语言的类。 异常在语法和功能上类似于结构体，差别是异常使用关键字exception，而且异常是继承每种语言的基础异常类。 12345678exception Extest &#123; 1: i32 errorCode, 2: string message, 3: StUser userinfo&#125; 8、服务（Services） 服务的定义方法在语义(semantically)上等同于面向对象语言中的接口。Thrift编译器会产生执行这些接口的client和server stub。具体参见下一节。 在流行的序列化/反序列化框架（如protocal buffer）中，Thrift是少有的提供多语言间RPC服务的框架。这是Thrift的一大特色。 Thrift编译器会根据选择的目标语言为server产生服务接口代码，为client产生stubs。 1234567service SeTest &#123; void ping(), bool postTweet(1: StUser user); StUser searchTweets(1:string name); oneway void zip()&#125; 编译后的c++代码123456789class SeTestIf &#123; public: virtual ~SeTestIf() &#123;&#125; virtual bool AddUser(const StUser&amp; user) = 0; virtual void SearchUser(StUser&amp; _return, const std::string&amp; name) = 0; virtual void NopNop() = 0;&#125;; 首先所有的方法都是纯虚汗数，也就是继承类必须实现这些方法返回值不是基本类型的都把返回值放到了函数参数中第一个参数，命名_return所有的参数（除返回值）都是const类型，意味这函数一般参数无法作为返回值携带者。只会有一个返回参数，如果返回值有多个，那只能封装复杂类型作为返回值参数。oneway的返回值一定是void当然服务是支持继承。服务不支持重载 9、名字空间（Namespace）Thrift中的命名空间类似于C++中的namespace和java中的package，它们提供了一种组织（隔离）代码的简便方式。名字空间也可以用于解决类型定义中的名字冲突。 由于每种语言均有自己的命名空间定义方式（如python中有module）, thrift允许开发者针对特定语言定义namespace：123namespace cpp com.example.testnamespace java com.example.testnamespace php com.example.test c++转化成1namespace com &#123; namespace example &#123; namespace test &#123; 在C++的版本中如果不加namespace，那么生成的代码中*server.skeleton.cpp代码中的namespace是空的，会产生编译错误。 10、注释（Comment） Thrift支持C多行风格和Java/C++单行风格。1234567891011121314/** * This is a multi-line comment. * Just like in C. */// C++/Java style single-line comments work just as well.11Includes 便于管理、重用和提高模块性/组织性，我们常常分割Thrift定义在不同的文件中。包含文件搜索方式与c++一样。Thrift允许文件包含其它thrift文件，用户需要使用thrift文件名作为前缀访问被包含的对象，如：include \"test.thrift\"...struct StSearchResult &#123; 1: in32 uid; ...&#125; thrift文件名要用双引号包含，末尾没有逗号或者分号","tags":[{"name":"PHP","slug":"PHP","permalink":"ly2513.github.com/tags/PHP/"},{"name":"Thrift","slug":"Thrift","permalink":"ly2513.github.com/tags/Thrift/"}]},{"title":"Thrift 在PHP中的使用","date":"2017-08-27T15:24:54.000Z","path":"2017/08/27/Thrift-在PHP中的使用/","text":"最近在研究Thrift在PHP中的运用,并将其结合我司框架,为框架扩展RPC服务。屁话不多说,直接进入今天的主题,进入主题之前先来认识下Thrift. Apache Thrift是一个跨语言的服务部署框架，通过一个中间语言(IDL, 接口定义语言)来定义RPC的接口和数据类型，然后通过一个编译器生成不同语言的代码（支持C++，Java，Python，PHP, GO，Javascript，Ruby，Erlang，Perl， Haskell， C#等）,并由生成的代码负责RPC协议层和传输层的实现。 在CentOS 6.5上安装Thrift 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758sudo yum -y updatesudo yum -y groupinstall \"Development Tools\"#升级autoconf，必须2.65以上wget http://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gztar xvf autoconf-2.69.tar.gzcd autoconf-2.69./configure --prefix=/usrmakesudo make installcd ..#升级automake必须1.14以上wget http://ftp.gnu.org/gnu/automake/automake-1.14.tar.gztar xvf automake-1.14.tar.gzcd automake-1.14./configure --prefix=/usrmakesudo make installcd ..#升级bsionwget http://ftp.gnu.org/gnu/bison/bison-2.5.1.tar.gztar xvf bison-2.5.1.tar.gzcd bison-2.5.1./configure --prefix=/usrmakesudo make installcd ..#安装boostwget http://sourceforge.net/projects/boost/files/boost/1.55.0/boost_1_55_0.tar.gztar xvf boost_1_55_0.tar.gzcd boost_1_55_0./bootstrap.shsudo ./b2 installcd ..#安装thrift，编译会比较久，内存最好1024M以上git clone https://git-wip-us.apache.org/repos/asf/thrift.gitcd thrift./bootstrap.sh./configuremakesudo make installcd ..#查看版本thrift -version#安装thrift_protocol扩展，仅支持二进制读写cd thrift/lib/php/src/ext/thrift_protocolphpize./configuresudo makesudo make install#这里不需要更改php.ini，已自动在/etc/php.d/thrift_protocol.ini里面添加php -m | grep thrift Thrift的PHP类库位于thrift/lib/php/lib/Thrift目录下面，Thrift对于数据传输格式、数据传输方式，服务器模型均做了定义，方便自行扩展。 数据传输格式（protocol）是定义的了传输内容，对Thrift Type的打包解包，包括 TBinaryProtocol，二进制格式，TBinaryProtocolAccelerated则是依赖于thrift_protocol扩展的快速打包解包。 TCompactProtocol，压缩格式 TJSONProtocol，JSON格式 TMultiplexedProtocol，利用前三种数据格式与支持多路复用协议的服务端（同时提供多个服务，TMultiplexedProcessor）交互 数据传输方式（transport），定义了如何发送（write）和接收（read）数据，包括 TBufferedTransport，缓存传输，写入数据并不立即开始传输，直到刷新缓存。 TSocket，使用socket传输 TFramedTransport，采用分块方式进行传输，具体传输实现依赖其他传输方式，比如TSocket TCurlClient，使用curl与服务端交互 THttpClient，采用stream方式与HTTP服务端交互 TMemoryBuffer，使用内存方式交换数据 TPhpStream，使用PHP标准输入输出流进行传输 TNullTransport，关闭数据传输 TSocketPool在TSocket基础支持多个服务端管理（需要APC支持），自动剔除无效的服务器 TNonblockingSocket，非官方实现非阻塞socket 服务模型，定义了当PHP作为服务端如何监听端口处理请求 TForkingServer，采用子进程处理请求 TSimpleServer，在TServerSocket基础上处理请求 TNonblockingServer，基于libevent的非官方实现非阻塞服务端，与TNonblockingServerSocket，TNonblockingSocket配合使用 另外还定义了一些工厂，以便在Server模式下对数据传输格式和传输方式进行绑定 TProtocolFactory，数据传输格式工厂类，对protocol的工厂化生产，包括TBinaryProtocolFactory，TCompactProtocolFactory，TJSONProtocolFactory TTransportFactory，数据传输方式工厂类，对transport的工厂化生产，作为server时，需要自行实现 TStringFuncFactory，字符串处理工厂类其他文件便是异常，字符串处理，自动加载器的定义等等。 现在开始编写一个简单接IDL文件HelloWorld.thrift 12345namespace php Services.HelloWorldservice HelloWorld&#123; string sayHello(1:string name);&#125; 然后通过生成器生成PHP文件 12#不指明:server不生成processor。。thrift --gen php:server HelloWorld.thrift 生成文件在gen-php目录下面的Services/HelloWord/HelloWorld.php（目录与namesapce定义一致），这是个公共文件，服务端和客户端都需要包括它。其中客户端调用的代码（HelloWorldClient ）已经生成好了 12345678910111213141516171819202122232425262728293031//服务端需要继承该接口interface HelloWorldIf &#123; /** * @param string $name * @return string */ public function sayHello($name);&#125;//提供给客户端调用的方法class HelloWorldClient implements \\Services\\HelloWorld\\HelloWorldIf &#123; public function sayHello($name) &#123; $this-&gt;send_sayHello($name); return $this-&gt;recv_sayHello(); &#125; public function send_sayHello($name) &#123; &#125; public function recv_sayHello() &#123; &#125;&#125;//HelloWord类sayHello方法参数读取class HelloWorld_sayHello_args &#123;&#125;//HelloWord类sayHello方法结果写入class HelloWorld_sayHello_result &#123;&#125;//作为服务端才会生成class HelloWorldProcessor &#123;&#125; 而服务端的服务实现代码则需要继承HelloWorldIf 实现代码HelloWorldHandler.php 123456789&lt;?phpnamespace Services\\HelloWorld;class HelloWorldHandler implements HelloWorldIf &#123; public function sayHello($name) &#123; return \"Hello $name\"; &#125;&#125; 编写服务端代码Server.php 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?phpnamespace Services\\HelloWorld;error_reporting(E_ALL);define('THRIFT_ROOT', __DIR__.'/../../../');require_once THRIFT_ROOT.'Thrift/ClassLoader/ThriftClassLoader.php';use Thrift\\ClassLoader\\ThriftClassLoader;$loader = new ThriftClassLoader();$loader-&gt;registerNamespace('Thrift', THRIFT_ROOT);$loader-&gt;registerDefinition('Service', THRIFT_ROOT.'/gen-php');$loader-&gt;register();use Thrift\\Exception\\TException;use Thrift\\Factory\\TBinaryProtocolFactory;use Thrift\\Factory\\TBufferedTransportFactory;use Thrift\\Server\\TServerSocket;use Thrift\\Server\\TSimpleServer;//use Thrift\\Server\\TNonblockingServerSocket;//use Thrift\\Server\\TNonblockingServer;//use Thrift\\Protocol\\TBinaryProtocol;//use Thrift\\Transport\\TPhpStream;//use Thrift\\Transport\\TBufferedTransport;try &#123; require_once 'HelloWorldHandler.php'; $handler = new \\Services\\HelloWorld\\HelloWorldHandler(); $processor = new \\Services\\HelloWorld\\HelloWorldProcessor($handler); $transportFactory = new TBufferedTransportFactory(); $protocolFactory = new TBinaryProtocolFactory(true, true); //作为cli方式运行，监听端口，官方实现 $transport = new TServerSocket('localhost', 9090); $server = new TSimpleServer($processor, $transport, $transportFactory, $transportFactory, $protocolFactory, $protocolFactory); $server-&gt;serve(); //作为cli方式运行，非阻塞方式监听，基于libevent实现，非官方实现 //$transport = new TNonblockingServerSocket('localhost', 9090); //$server = new TNonblockingServer($processor, $transport, $transportFactory, $transportFactory, $protocolFactory, $protocolFactory); //$server-&gt;serve(); //客户端和服务端在同一个输入输出流上 //使用方式 //1) cli 方式：php Client.php | php Server.php //2) cgi 方式：利用Apache或nginx监听http请求，调用php-fpm处理，将请求转换为PHP标准输入输出流 //$transport = new TBufferedTransport(new TPhpStream(TPhpStream::MODE_R | TPhpStream::MODE_W)); //$protocol = new TBinaryProtocol($transport, true, true); //$transport-&gt;open(); //$processor-&gt;process($protocol, $protocol); //$transport-&gt;close();&#125; catch (TException $tx) &#123; print 'TException: '.$tx-&gt;getMessage().\"\\n\";&#125; 服务端创建的步骤： 首先初始化服务提供者handler然后利用该handler初始化自动生成的processor初始化数据传输方式transport利用该传输方式初始化数据传输格式protocol开始服务编写客户端代码Client.php 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?phpnamespace Services\\HelloWorld;error_reporting(E_ALL);define('THRIFT_ROOT', __DIR__.'/../../../');require_once THRIFT_ROOT.'Thrift/ClassLoader/ThriftClassLoader.php';use Thrift\\ClassLoader\\ThriftClassLoader;$loader = new ThriftClassLoader();$loader-&gt;registerNamespace('Thrift', THRIFT_ROOT);$loader-&gt;registerDefinition('Service', THRIFT_ROOT.'/gen-php');$loader-&gt;register();//use Thrift\\Transport\\TPhpStream;use Thrift\\Protocol\\TBinaryProtocol;use Thrift\\Transport\\TSocket;use Thrift\\Transport\\TBufferedTransport;use Thrift\\Exception\\TException;try &#123; //仅在与服务端处于同一输出输出流有用 //使用方式：php Client.php | php Server.php //$transport = new TBufferedTransport(new TPhpStream(TPhpStream::MODE_R | TPhpStream::MODE_W)); //socket方式连接服务端 //数据传输格式和数据传输方式与服务端一一对应 //如果服务端以http方式提供服务，可以使用THttpClient/TCurlClient数据传输方式 $transport = new TBufferedTransport(new TSocket('localhost', 9090)); $protocol = new TBinaryProtocol($transport); $client = new \\Services\\HelloWorld\\HelloWorldClient($protocol); $transport-&gt;open(); //同步方式进行交互 $recv = $client-&gt;sayHello('Courages'); echo \"\\n sayHello11dd:\".$recv.\" \\n\"; //异步方式进行交互 $client-&gt;send_sayHello('Us'); echo \"\\n send_sayHello \\n\"; $recv = $client-&gt;recv_sayHello(); echo \"\\n recv_sayHello:\".$recv.\" \\n\"; $transport-&gt;close();&#125; catch (TException $tx) &#123; print 'TException: '.$tx-&gt;getMessage().\"\\n\";&#125; 客户端调用的步骤： 初始化数据传输方式transport，与服务端对应利用该传输方式初始化数据传输格式protocol，与服务端对应实例化自动生成的Client对象开始调用在终端上运行 123456789#以cli方式运行TPhpStream#php Client.php | php Server.php#先运行Server.php#要不然会报错：TException: TSocket: Could not connect to localhost:9090 (Connection refused [111])php Server.php#在另外一个终端运行php Client.php 官方给的例子，PHP作为服务端是以web方式进行提供的，在cli方式下并不能运行。 Thrift作为一个跨语言的服务框架，方便不同语言、模块之间互相调用，解耦服务逻辑代码，拓展了PHP的处理能力（如与Hbase交互），使得WEB架构更具弹性。与基于 SOAP 消息格式的 Web Service和基于 JSON 消息格式的 RESTful 服务不同，Thrif数据传输格式默认采用二进制传格式，对 XML 和 JSON 体积更小，但对于服务端的CPU占用比JSON、XML要高。PHP虽然有thrift_protocol扩展，但仅仅作为二进制数据传输格式化使用，其他文件的加载仍然为PHP，需要更多的开销。 如果由PHP来做为Thrift的服务端，仅仅这样子做仍然是不够的，Thrift仅仅实现的数据定义和传输，未实现RPC架构 需要避免重复加载各类文件，是否做成PHP扩展数据传输格式和方式是否适需要自行扩展客户端要能够自动连可使用的服务端，剔除失效的服务器服务端需要处理客户端并发情况，是否多进程/异步处理服务端需要监控服务是否正常workerman-thrift-rpc对这些问题进行了解决，基于thrift提供了一个可靠性的RPC框架。对客户端和服务端的调用做了封装，提供统一入口，利用workerman做socket中转，当客户端发出请求时，将给socket转给服务端使用，提供服务。workerman-json-rpc与workerman-thrift-rpc类似，采用异步（分步）收发，但简单多了，更像是一种约定。数据格式，发送时仅发送class，function，parameters三个参数，接收时，仅code，msg，data三个返回值，在格式约束及跨语言上，需要自行处理；不需要thrift那样依赖于生成器所生成的文件，客户端完全独立于服务端。","tags":[{"name":"PHP","slug":"PHP","permalink":"ly2513.github.com/tags/PHP/"},{"name":"Thrift","slug":"Thrift","permalink":"ly2513.github.com/tags/Thrift/"}]},{"title":"每天掌握一个Linux命令(37) : date命令","date":"2017-08-16T13:48:29.000Z","path":"2017/08/16/每天掌握一个Linux命令-37-date命令/","text":"在linux环境中，不管是编程还是其他维护，时间是必不可少的，也经常会用到时间的运算，熟练运用date命令来表示自己想要表示的时间，肯定可以给自己的工作带来诸多方便。 1．命令格式： date [参数]… [+格式] 2．命令功能： date 可以用来显示或设定系统的日期与时间。 3．命令参数： 必要参数: %H 小时(以00-23来表示)。%I 小时(以01-12来表示)。%K 小时(以0-23来表示)。%l 小时(以0-12来表示)。%M 分钟(以00-59来表示)。%P AM或PM。%r 时间(含时分秒，小时以12小时AM/PM来表示)。%s 总秒数。起算时间为1970-01-01 00:00:00 UTC。%S 秒(以本地的惯用法来表示)。%T 时间(含时分秒，小时以24小时制来表示)。%X 时间(以本地的惯用法来表示)。%Z 市区。%a 星期的缩写。%A 星期的完整名称。%b 月份英文名的缩写。%B 月份的完整英文名称。%c 日期与时间。只输入date指令也会显示同样的结果。%d 日期(以01-31来表示)。%D 日期(含年月日)。%j 该年中的第几天。%m 月份(以01-12来表示)。%U 该年中的周数。%w 该周的天数，0代表周日，1代表周一，异词类推。%x 日期(以本地的惯用法来表示)。%y 年份(以00-99来表示)。%Y 年份(以四位数来表示)。%n 在显示时，插入新的一行。%t 在显示时，插入tab。MM 月份(必要)DD 日期(必要)hh 小时(必要)mm 分钟(必要)ss 秒(选择性) 选择参数: -d&lt;字符串&gt; 显示字符串所指的日期与时间。字符串前后必须加上双引号。-s&lt;字符串&gt; 根据字符串来设置日期与时间。字符串前后必须加上双引号。-u 显示GMT。–help 在线帮助。–version 显示版本信息 4．使用说明： 1.在显示方面，使用者可以设定欲显示的格式，格式设定为一个加号后接数个标记，其中可用的标记列表如下: % : 打印出 %： %n : 下一行%t : 跳格%H : 小时(00..23)%I : 小时(01..12)%k : 小时(0..23)%l : 小时(1..12)%M : 分钟(00..59)%p : 显示本地 AM 或 PM%r : 直接显示时间 (12 小时制，格式为 hh:mm:ss [AP]M)%s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数%S : 秒(00..61)%T : 直接显示时间 (24 小时制)%X : 相当于 %H:%M:%S%Z : 显示时区 %a : 星期几 (Sun..Sat)%A : 星期几 (Sunday..Saturday)%b : 月份 (Jan..Dec)%B : 月份 (January..December)%c : 直接显示日期与时间%d : 日 (01..31)%D : 直接显示日期 (mm/dd/yy)%h : 同 %b%j : 一年中的第几天 (001..366)%m : 月份 (01..12)%U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形)%w : 一周中的第几天 (0..6)%W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形)%x : 直接显示日期 (mm/dd/yy)%y : 年份的最后两位数字 (00.99)%Y : 完整年份 (0000..9999) 2.在设定时间方面： date -s //设置当前时间，只有root权限才能设置，其他只能查看。date -s 20080523 //设置成20080523，这样会把具体时间设置成空00:00:00date -s 01:01:01 //设置具体时间，不会对日期做更改date -s “01:01:01 2008-05-23″ //这样可以设置全部时间date -s “01:01:01 20080523″ //这样可以设置全部时间date -s “2008-05-23 01:01:01″ //这样可以设置全部时间date -s “20080523 01:01:01″ //这样可以设置全部时间 3.加减： date +%Y%m%d //显示前天年月日date +%Y%m%d –date=”+1 day” //显示前一天的日期date +%Y%m%d –date=”-1 day” //显示后一天的日期date +%Y%m%d –date=”-1 month” //显示上一月的日期date +%Y%m%d –date=”+1 month” //显示下一月的日期date +%Y%m%d –date=”-1 year” //显示前一年的日期date +%Y%m%d –date=”+1 year” //显示下一年的日期 5．使用实例： 实例一：显示当前时间命令：123456datedate '+%c'date '+%D'date '+%x'date '+%T'date '+%X' 输出：1234567891011[root@localhost ~]# date2012年 12月 08日 星期六 08:31:35 CST[root@localhost ~]# date '+%c'2012年12月08日 星期六 08时34分44秒[root@localhost ~]# date '+%D'12/08/12[root@localhost ~]# date '+%x'2012年12月08日[root@localhost ~]# date '+%T'08:35:36[root@localhost ~]# date '+%X'08时35分54秒[root@localhost ~]# 实例二：显示日期和设定时间 命令：1date --date 08:42:00 输出：1234567[root@localhost ~]# date '+%c'2012年12月08日 星期六 08时41分37秒[root@localhost ~]# date --date 08:42:002012年 12月 08日 星期六 08:42:00 CST[root@localhost ~]# date '+%c' --date 08:45:002012年12月08日 星期六 08时45分00秒[root@localhost ~]# 实例三：date -d参数使用 命令：1date -d 输出：1234567891011121314151617181920[root@localhost ~]# date -d \"nov 22\"2012年 11月 22日 星期四 00:00:00 CST[root@localhost ~]# date -d '2 weeks'2012年 12月 22日 星期六 08:50:21 CST[root@localhost ~]# date -d 'next monday'2012年 12月 10日 星期一 00:00:00 CST[root@localhost ~]# date -d next-day +%Y%m%d20121209[root@localhost ~]# date -d tomorrow +%Y%m%d20121209[root@localhost ~]# date -d last-day +%Y%m%d20121207[root@localhost ~]# date -d yesterday +%Y%m%d20121207[root@localhost ~]# date -d last-month +%Y%m201211[root@localhost ~]# date -d next-month +%Y%m201301[root@localhost ~]# date -d '30 days ago'2012年 11月 08日 星期四 08:51:37 CST[root@localhost ~]# date -d '-100 days'2012年 08月 30日 星期四 08:52:03 CST[root@localhost ~]# date -d 'dec 14 -2 weeks'2012年 11月 30日 星期五 00:00:00 CST[root@localhost ~]# date -d '50 days'2013年 01月 27日 星期日 08:52:27 CST 说明： date 命令的另一个扩展是 -d 选项，该选项非常有用。使用这个功能强大的选项，通过将日期作为引号括起来的参数提供，您可以快速地查明一个特定的日期。-d 选项还可以告诉您，相对于当前日期若干天的究竟是哪一天，从现在开始的若干天或若干星期以后，或者以前（过去）。通过将这个相对偏移使用引号括起来，作为 -d 选项的参数，就可以完成这项任务。 具体说明如下： date -d “nov 22” 今年的 11 月 22 日是星期三date -d ‘2 weeks’ 2周后的日期date -d ‘next monday’ (下周一的日期)date -d next-day +%Y%m%d（明天的日期）或者：date -d tomorrow +%Y%m%ddate -d last-day +%Y%m%d(昨天的日期) 或者：date -d yesterday +%Y%m%ddate -d last-month +%Y%m(上个月是几月)date -d next-month +%Y%m(下个月是几月)使用 ago 指令，您可以得到过去的日期：date -d ‘30 days ago’ （30天前的日期）使用负数以得到相反的日期：date -d ‘dec 14 -2 weeks’ （相对:dec 14这个日期的两周前的日期）date -d ‘-100 days’ (100天以前的日期)date -d ‘50 days’(50天后的日期) 实例四：显示月份和日数命令：1date '+%B %d' 输出：12[root@localhost ~]# date '+%B %d'十二月 08[root@localhost ~]# 实例五：显示时间后跳行，再显示目前日期命令：1date '+%T%n%D' 输出：123[root@localhost ~]# date '+%T%n%D'09:00:3012/08/12[root@localhost ~]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"date","slug":"date","permalink":"ly2513.github.com/tags/date/"}]},{"title":"每天掌握一个Linux命令(36) : diff命令","date":"2017-08-16T13:09:47.000Z","path":"2017/08/16/每天掌握一个Linux命令-36-diff命令/","text":"diff 命令是 linux上非常重要的工具，用于比较文件的内容，特别是比较两个版本不同的文件以找到改动的地方。diff在命令行中打印每一个行的改动。最新版本的diff还支持二进制文件。diff程序的输出被称为补丁 (patch)，因为Linux系统中还有一个patch程序，可以根据diff的输出将a.c的文件内容更新为b.c。diff是svn、cvs、git等版本控制工具不可或缺的一部分。 1．命令格式： diff[参数][文件1或目录1][文件2或目录2] 2．命令功能： diff命令能比较单个文件或者目录内容。如果指定比较的是文件，则只有当输入为文本文件时才有效。以逐行的方式，比较文本文件的异同处。如果指定比较的是目录的的时候，diff 命令会比较两个目录下名字相同的文本文件。列出不同的二进制文件、公共子目录和只在一个目录出现的文件。 3．命令参数： 指定要显示多少行的文本。此参数必须与-c或-u参数一并使用。 -a或–text diff预设只会逐行比较文本文件。-b或–ignore-space-change 不检查空格字符的不同。-B或–ignore-blank-lines 不检查空白行。-c 显示全部内文，并标出不同之处。-C或–context 与执行”-c-“指令相同。-d或–minimal 使用不同的演算法，以较小的单位来做比较。-D或ifdef 此参数的输出格式可用于前置处理器巨集。-e或–ed 此参数的输出格式可用于ed的script文件。-f或-forward-ed 输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处。-H或–speed-large-files 比较大文件时，可加快速度。-l或–ignore-matching-lines 若两个文件在某几行有所不同，而这几行同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异。-i或–ignore-case 不检查大小写的不同。-l或–paginate 将结果交由pr程序来分页。-n或–rcs 将比较结果以RCS的格式来显示。-N或–new-file 在比较目录时，若文件A仅出现在某个目录中，预设会显示：Only in目录：文件A若使用-N参数，则diff会将文件A与一个空白的文件比较。-p 若比较的文件为C语言的程序码文件时，显示差异所在的函数名称。-P或–unidirectional-new-file 与-N类似，但只有当第二个目录包含了一个第一个目录所没有的文件时，才会将这个文件与空白的文件做比较。-q或–brief 仅显示有无差异，不显示详细的信息。-r或–recursive 比较子目录中的文件。-s或–report-identical-files 若没有发现任何差异，仍然显示信息。-S或–starting-file 在比较目录时，从指定的文件开始比较。-t或–expand-tabs 在输出时，将tab字符展开。-T或–initial-tab 在每行前面加上tab字符以便对齐。-u,-U或–unified= 以合并的方式来显示文件内容的不同。-v或–version 显示版本信息。-w或–ignore-all-space 忽略全部的空格字符。-W或–width 在使用-y参数时，指定栏宽。-x或–exclude 不比较选项中所指定的文件或目录。-X或–exclude-from 您可以将文件或目录类型存成文本文件，然后在=中指定此文本文件。-y或–side-by-side 以并列的方式显示文件的异同之处。–help 显示帮助。–left-column 在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容。–suppress-common-lines 在使用-y参数时，仅显示不同之处。 4．使用实例： 实例一：比较两个文件命令：1diff 输出： 123456789101112[root@localhost test3]# diff log2014.log log2013.log3c3&lt; 2014-03---&gt; 2013-038c8&lt; 2013-07---&gt; 2013-0811,12d10&lt; 2013-11&lt; 2013-12 说明： 上面的“3c3”和“8c8”表示log2014.log和log20143log文件在3行和第8行内容有所不同；”11,12d10”表示第一个文件比第二个文件多了第11和12行。 diff的normal 显示格式有三种提示: a - addc - changed - delete 实例二：并排格式输出命令：1diff log2013.log log2014.log -y -W 50 输出：1234567891011121314151617181920212223242526[root@localhost test3]# diff log2014.log log2013.log -y -W 502013-01 2013-012013-02 2013-022014-03 | 2013-032013-04 2013-042013-05 2013-052013-06 2013-062013-07 2013-072013-07 | 2013-082013-09 2013-092013-10 2013-102013-11 &lt;2013-12 &lt;[root@localhost test3]# diff log2013.log log2014.log -y -W 502013-01 2013-012013-02 2013-022013-03 | 2014-032013-04 2013-042013-05 2013-052013-06 2013-062013-07 2013-072013-08 | 2013-072013-09 2013-092013-10 2013-10 &gt; 2013-11 &gt; 2013-12 说明： “|”表示前后2个文件内容有不同“&lt;”表示后面文件比前面文件少了1行内容“&gt;”表示后面文件比前面文件多了1行内容 实例三：上下文输出格式命令：1diff log2013.log log2014.log -c 输出：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@localhost test3]# diff log2013.log log2014.log -c*** log2013.log 2012-12-07 16:36:26.000000000 +0800--- log2014.log 2012-12-07 18:01:54.000000000 +0800****************** 1,10 **** 2013-01 2013-02! 2013-03 2013-04 2013-05 2013-06 2013-07! 2013-08 2013-09 2013-10--- 1,12 ---- 2013-01 2013-02! 2014-03 2013-04 2013-05 2013-06 2013-07! 2013-07 2013-09 2013-10+ 2013-11+ 2013-12[root@localhost test3]# diff log2014.log log2013.log -c*** log2014.log 2012-12-07 18:01:54.000000000 +0800--- log2013.log 2012-12-07 16:36:26.000000000 +0800****************** 1,12 **** 2013-01 2013-02! 2014-03 2013-04 2013-05 2013-06 2013-07! 2013-07 2013-09 2013-10- 2013-11- 2013-12--- 1,10 ---- 2013-01 2013-02! 2013-03 2013-04 2013-05 2013-06 2013-07! 2013-08 2013-09 2013-10[root@localhost test3]# 说明： 这种方式在开头两行作了比较文件的说明，这里有三中特殊字符： “＋” 比较的文件的后者比前着多一行“－” 比较的文件的后者比前着少一行“！” 比较的文件两者有差别的行 实例四：统一格式输出命令：1diff log2014.log log2013.log -u 输出：123456789101112131415161718[root@localhost test3]# diff log2014.log log2013.log -u--- log2014.log 2012-12-07 18:01:54.000000000 +0800+++ log2013.log 2012-12-07 16:36:26.000000000 +0800@@ -1,12 +1,10 @@ 2013-01 2013-02-2014-03+2013-03 2013-04 2013-05 2013-06 2013-07-2013-07+2013-08 2013-09 2013-10-2013-11-2013-12 说明： 它的第一部分，也是文件的基本信息： — log2014.log 2012-12-07 18:01:54.000000000 +0800+++ log2013.log 2012-12-07 16:36:26.000000000 +0800 “—“表示变动前的文件，”+++”表示变动后的文件。 第二部分，变动的位置用两个@作为起首和结束。 @@ -1,12 +1,10 @@ 前面的”-1,12”分成三个部分：减号表示第一个文件（即log2014.log），”1”表示第1行，”12”表示连续12行。合在一起，就表示下面是第一个文件从第1行开始的连续12行。同样的，”+1,10”表示变动后，成为第二个文件从第1行开始的连续10行。 实例五：比较文件夹不同命令：1diff test3 test6 输出：12345678910111213141516171819202122232425262728293031323334353637[root@localhost test]# diff test3 test6Only in test6: linklog.logOnly in test6: log2012.logdiff test3/log2013.log test6/log2013.log1,10c1,3&lt; 2013-01&lt; 2013-02&lt; 2013-03&lt; 2013-04&lt; 2013-05&lt; 2013-06&lt; 2013-07&lt; 2013-08&lt; 2013-09&lt; 2013-10---&gt; hostnamebaidu=baidu.com&gt; hostnamesina=sina.com&gt; hostnames=truediff test3/log2014.log test6/log2014.log1,12d0&lt; 2013-01&lt; 2013-02&lt; 2014-03&lt; 2013-04&lt; 2013-05&lt; 2013-06&lt; 2013-07&lt; 2013-07&lt; 2013-09&lt; 2013-10&lt; 2013-11&lt; 2013-12Only in test6: log2015.logOnly in test6: log2016.logOnly in test6: log2017.log[root@localhost test]# 实例六：比较两个文件不同，并生产补丁命令：1diff -ruN log2013.log log2014.log &gt;patch.log 输出：1234567891011121314151617181920212223242526[root@localhost test3]# diff -ruN log2013.log log2014.log &gt;patch.log[root@localhost test3]# ll总计 12-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log-rw-r--r-- 1 root root 96 12-07 18:01 log2014.log-rw-r--r-- 1 root root 248 12-07 21:33 patch.log[root@localhost test3]# cat patc.logcat: patc.log: 没有那个文件或目录[root@localhost test3]# cat patch.log--- log2013.log 2012-12-07 16:36:26.000000000 +0800+++ log2014.log 2012-12-07 18:01:54.000000000 +0800@@ -1,10 +1,12 @@ 2013-01 2013-02-2013-03+2014-03 2013-04 2013-05 2013-06 2013-07-2013-08+2013-07 2013-09 2013-10+2013-11+2013-12[root@localhost test3]# 实例七：打补丁命令：输出：1234567891011121314151617181920212223242526[root@localhost test3]# cat log2013.log2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-10[root@localhost test3]# patch log2013.log patch.logpatching file log2013.log[root@localhost test3]#[root@localhost test3]# cat log2013.log2013-012013-022014-032013-042013-052013-062013-072013-072013-092013-102013-112013-12[root@localhost test3]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"diff","slug":"diff","permalink":"ly2513.github.com/tags/diff/"}]},{"title":"每天掌握一个Linux命令(35) : ln命令","date":"2017-08-15T13:43:17.000Z","path":"2017/08/15/每天掌握一个Linux命令-35-ln命令/","text":"ln是linux中又一个非常重要命令，它的功能是为某一个文件在另外一个位置建立一个同步的链接.当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。 1．命令格式： ln [参数][源文件或目录][目标文件或目录] 2．命令功能： Linux文件系统中，有所谓的链接(link)，我们可以将其视为档案的别名，而链接又可分为两种 : 硬链接(hard link)与软链接(symbolic link)，硬链接的意思是一个档案可以有多个名称，而软链接的方式则是产生一个特殊的档案，该档案的内容是指向另一个档案的位置。硬链接是存在同一个文件系统中，而软链接却可以跨越不同的文件系统。 软链接： 1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式2.软链接可以 跨文件系统 ，硬链接不可以3.软链接可以对一个不存在的文件名进行链接4.软链接可以对目录进行链接 硬链接: 1.硬链接，以文件副本的形式存在。但不占用实际空间。2.不允许给目录创建硬链接3.硬链接只有在同一个文件系统中才能创建 这里有两点要注意： 第一，ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 第二，ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 3．命令参数： 必要参数: -b 删除，覆盖以前建立的链接-d 允许超级用户制作目录的硬链接-f 强制执行-i 交互模式，文件存在则提示用户是否覆盖-n 把符号链接视为一般目录-s 软链接(符号链接)-v 显示详细的处理过程 选择参数: -S “-S&lt;字尾备份字符串&gt; ”或 “–suffix=&lt;字尾备份字符串&gt;”-V “-V&lt;备份方式&gt;”或“–version-control=&lt;备份方式&gt;”–help 显示帮助信息–version 显示版本信息 4．使用实例： 实例一：给文件创建软链接命令： 1ln -s log2013.log link2013 输出： 123456[root@localhost test]# ll-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log[root@localhost test]# ln -s log2013.log link2013[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log 说明： 为log2013.log文件创建软链接link2013，如果log2013.log丢失，link2013将失效 实例二：给文件创建硬链接命令：1ln log2013.log ln2013 输出：12345678[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log[root@localhost test]# ln log2013.log ln2013[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 2 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root bin 61 11-13 06:03 log2013.log 说明： 为log2013.log创建硬链接ln2013，log2013.log与ln2013的各项属性相同 实例三：接上面两实例，链接完毕后，删除和重建链接原文件命令： 输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 2 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root bin 61 11-13 06:03 log2013.log[root@localhost test]# rm -rf log2013.log[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013[root@localhost test]# touch log2013.log[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013---xrw-r-- 1 root bin 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 0 12-07 16:19 log2013.log[root@localhost test]# vi log2013.log2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-102013-112013-12[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 1 root root 96 12-07 16:21 log2013.log[root@localhost test]# cat link20132013-012013-022013-032013-042013-052013-062013-072013-082013-092013-102013-112013-12[root@localhost test]# cat ln2013hostnamebaidu=baidu.comhostnamesina=sina.comhostnames=true 说明： 1.源文件被删除后，并没有影响硬链接文件；软链接文件在centos系统下不断的闪烁，提示源文件已经不存在 2.重建源文件后，软链接不在闪烁提示，说明已经链接成功，找到了链接文件系统；重建后，硬链接文件并没有受到源文件影响，硬链接文件的内容还是保留了删除前源文件的内容，说明硬链接已经失效 实例四：将文件链接为另一个目录中的相同名字命令：1ln log2013.log test3 输出：1234567891011121314151617181920212223242526[root@localhost test]# ln log2013.log test3[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root root 96 12-07 16:21 log2013.log[root@localhost test]# cd test3[root@localhost test3]# ll-rw-r--r-- 2 root root 96 12-07 16:21 log2013.log[root@localhost test3]# vi log2013.log2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-10[root@localhost test3]# ll-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log[root@localhost test3]# cd ..[root@localhost test]# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log[root@localhost test]# 说明： 在test3目录中创建了log2013.log的硬链接，修改test3目录中的log2013.log文件，同时也会同步到源文件 实例五：给目录创建软链接命令：1ln -sv /opt/soft/test/test3 /opt/soft/test/test5 输出：123456789101112131415161718192021222324252627282930[root@localhost test]# lldrwxr-xr-x 2 root root 4096 12-07 16:36 test3drwxr-xr-x 2 root root 4096 12-07 16:57 test5[root@localhost test]# cd test5[root@localhost test5]# lllrwxrwxrwx 1 root root 5 12-07 16:57 test3 -&gt; test3[root@localhost test5]# cd test3-bash: cd: test3: 符号连接的层数过多[root@localhost test5]#[root@localhost test5]#[root@localhost test5]# lllrwxrwxrwx 1 root root 5 12-07 16:57 test3 -&gt; test3[root@localhost test5]# rm -rf test3[root@localhost test5]# ll[root@localhost test5]# ln -sv /opt/soft/test/test3 /opt/soft/test/test5创建指向“/opt/soft/test/test3”的符号链接“/opt/soft/test/test5/test3”[root@localhost test5]# lllrwxrwxrwx 1 root root 20 12-07 16:59 test3 -&gt; /opt/soft/test/test3[root@localhost test5]#[root@localhost test5]# cd test3[root@localhost test3]# ll总计 4-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log[root@localhost test3]# touch log2014.log[root@localhost test3]# ll总计 4-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log-rw-r--r-- 1 root root 0 12-07 17:05 log2014.log[root@localhost test3]# cd ..[root@localhost test5]# cd .. 说明： 1.目录只能创建软链接 2.目录创建链接必须用绝对路径，相对路径创建会不成功，会提示：符号连接的层数过多 这样的错误 3.在链接目标目录中修改文件都会在源文件目录中同步变化","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"ln","slug":"ln","permalink":"ly2513.github.com/tags/ln/"}]},{"title":"每天掌握一个Linux命令(34) : du命令","date":"2017-08-15T13:27:22.000Z","path":"2017/08/15/每天掌握一个Linux命令-34-du命令/","text":"Linux du命令也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看，还是和df命令有一些区别的. 1．命令格式： du [选项][文件] 2．命令功能： 显示每个文件和目录的磁盘使用空间。 3．命令参数： -a或-all 显示目录中个别文件的大小。-b或-bytes 显示目录或文件大小时，以byte为单位。-c或–total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。-k或–kilobytes 以KB(1024bytes)为单位输出。-m或–megabytes 以MB为单位输出。-s或–summarize 仅显示总计，只列出最后加总的值。-h或–human-readable 以K，M，G为单位，提高信息的可读性。-x或–one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。-L&lt;符号链接&gt;或–dereference&lt;符号链接&gt; 显示选项中所指定符号链接的源文件大小。-S或–separate-dirs 显示个别目录的大小时，并不含其子目录的大小。-X&lt;文件&gt;或–exclude-from=&lt;文件&gt; 在&lt;文件&gt;指定目录或文件。–exclude=&lt;目录或文件&gt; 略过指定的目录或文件。-D或–dereference-args 显示指定符号链接的源文件大小。-H或–si 与-h参数相同，但是K，M，G是以1000为换算单位。-l或–count-links 重复计算硬件链接的文件。 4．使用实例： 实例一：显示目录或者文件所占空间命令:1du 输出： 1234567891011121314[root@localhost test]# du608 ./test6308 ./test44 ./scf/lib4 ./scf/service/deploy/product4 ./scf/service/deploy/info12 ./scf/service/deploy16 ./scf/service4 ./scf/doc4 ./scf/bin32 ./scf8 ./test31288 .[root@localhost test]# 说明： 只显示当前目录下面的子目录的目录大小和当前目录的总的大小，最下面的1288为当前目录的总大小 实例二：显示指定文件所占空间命令:1du log2012.log 输出：123[root@localhost test]# du log2012.log300 log2012.log[root@localhost test]# 实例三：查看指定目录的所占空间命令:1du scf 输出：12345678910[root@localhost test]# du scf4 scf/lib4 scf/service/deploy/product4 scf/service/deploy/info12 scf/service/deploy16 scf/service4 scf/doc4 scf/bin32 scf[root@localhost test]# 实例四：显示多个文件所占空间命令:1du log30.tar.gz log31.tar.gz 输出：1234[root@localhost test]# du log30.tar.gz log31.tar.gz4 log30.tar.gz4 log31.tar.gz[root@localhost test]# 实例五：只显示总和的大小命令:1du -s 输出：12345678[root@localhost test]# du -s1288 .[root@localhost test]# du -s scf32 scf[root@localhost test]# cd ..[root@localhost soft]# du -s test1288 test[root@localhost soft]# #####实例六：方便阅读的格式显示 命令:1du -h test 输出：1234567891011121314[root@localhost soft]# du -h test608K test/test6308K test/test44.0K test/scf/lib4.0K test/scf/service/deploy/product4.0K test/scf/service/deploy/info12K test/scf/service/deploy16K test/scf/service4.0K test/scf/doc4.0K test/scf/bin32K test/scf8.0K test/test31.3M test[root@localhost soft]# 实例七：文件和目录都显示命令:1du -ah 输出：1234567891011121314151617181920212223242526272829303132333435363738[root@localhost soft]# du -ah test4.0K test/log31.tar.gz4.0K test/test13.tar.gz0 test/linklog.log0 test/test6/log2014.log300K test/test6/linklog.log0 test/test6/log2015.log4.0K test/test6/log2013.log300K test/test6/log2012.log0 test/test6/log2017.log0 test/test6/log2016.log608K test/test60 test/log2015.log0 test/test4/log2014.log4.0K test/test4/log2013.log300K test/test4/log2012.log308K test/test44.0K test/scf/lib4.0K test/scf/service/deploy/product4.0K test/scf/service/deploy/info12K test/scf/service/deploy16K test/scf/service4.0K test/scf/doc4.0K test/scf/bin32K test/scf4.0K test/log2013.log300K test/log2012.log0 test/log2017.log0 test/log2016.log4.0K test/log30.tar.gz4.0K test/log.tar.bz24.0K test/log.tar.gz0 test/test3/log2014.log4.0K test/test3/log2013.log8.0K test/test34.0K test/scf.tar.gz1.3M test[root@localhost soft]# 实例八：显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和命令:1du -c log30.tar.gz log31.tar.gz 输出：12345[root@localhost test]# du -c log30.tar.gz log31.tar.gz4 log30.tar.gz4 log31.tar.gz8 总计[root@localhost test]# 说明： 加上-c选项后，du不仅显示两个目录各自占用磁盘空间的大小，还在最后一行统计它们的总和。 实例九：按照空间大小排序命令:1du|sort -nr|more 输出：1234567891011121314[root@localhost test]# du|sort -nr|more1288 .608 ./test6308 ./test432 ./scf16 ./scf/service12 ./scf/service/deploy8 ./test34 ./scf/service/deploy/product4 ./scf/service/deploy/info4 ./scf/lib4 ./scf/doc4 ./scf/bin[root@localhost test]# 实例十：输出当前目录下各个子目录所使用的空间命令:1du -h --max-depth=1 输出：1234567[root@localhost test]# du -h --max-depth=1608K ./test6308K ./test432K ./scf8.0K ./test31.3M .[root@localhost test]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"du","slug":"du","permalink":"ly2513.github.com/tags/du/"}]},{"title":"每天掌握一个Linux命令(33) : df命令","date":"2017-08-15T13:08:24.000Z","path":"2017/08/15/每天掌握一个Linux命令-33-df命令/","text":"linux中df命令的功能是用来检查linux服务器的文件系统的磁盘空间占用情况。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 1．命令格式： df [选项] [文件] 2．命令功能： 显示指定磁盘文件的可用空间。如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示 3．命令参数： 必要参数： -a 全部文件系统列表-h 方便阅读方式显示-H 等于“-h”，但是计算式，1K=1000，而不是1K=1024-i 显示inode信息-k 区块为1024字节-l 只显示本地文件系统-m 区块为1048576字节–no-sync 忽略 sync 命令-P 输出格式为POSIX–sync 在取得磁盘信息前，先执行sync命令-T 文件系统类型 选择参数： –block-size=&lt;区块大小&gt; 指定区块大小-t&lt;文件系统类型&gt; 只显示选定文件系统的磁盘信息-x&lt;文件系统类型&gt; 不显示选定文件系统的磁盘信息–help 显示帮助信息–version 显示版本信息 4．使用实例： 实例一：显示磁盘使用情况命令:1df 输出：12345678[root@CT1190 log]# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda7 19840892 890896 17925856 5% //dev/sda9 203727156 112797500 80413912 59% /opt/dev/sda8 4956284 570080 4130372 13% /var/dev/sda6 19840892 1977568 16839184 11% /usr/dev/sda3 988116 23880 913232 3% /boottmpfs 16473212 0 16473212 0% /dev/shm 说明： linux中df命令的输出清单的第1列是代表文件系统对应的设备文件的路径名（一般是硬盘上的分区）；第2列给出分区包含的数据块（1024字节）的数目；第3，4列分别表示已用的和可用的数据块数目。用户也许会感到奇怪的是，第3，4列块数之和不等于第2列中的块数。这是因为缺省的每个分区都留了少量空间供系统管理员使用。即使遇到普通用户空间已满的情况，管理员仍能登录和留有解决问题所需的工作空间。清单中Use% 列表示普通用户空间使用的百分比，即使这一数字达到100％，分区仍然留有系统管理员使用的空间。最后，Mounted on列表示文件系统的挂载点。 实例二：以inode模式来显示磁盘使用情况命令:1df -i 输出： 12345678[root@CT1190 log]# df -i文件系统 Inode (I)已用 (I)可用 (I)已用% 挂载点/dev/sda7 5124480 5560 5118920 1% //dev/sda9 52592640 50519 52542121 1% /opt/dev/sda8 1280000 8799 1271201 1% /var/dev/sda6 5124480 80163 5044317 2% /usr/dev/sda3 255232 34 255198 1% /boottmpfs 4118303 1 4118302 1% /dev/shm 实例三：显示指定类型磁盘命令:1df -t ext3 输出：1234567[root@CT1190 log]# df -t ext3文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda7 19840892 890896 17925856 5% //dev/sda9 203727156 93089700 100121712 49% /opt/dev/sda8 4956284 570104 4130348 13% /var/dev/sda6 19840892 1977568 16839184 11% /usr/dev/sda3 988116 23880 913232 3% /boot 实例四：列出各文件系统的i节点使用情况命令:1df -ia 输出：123456789101112[root@CT1190 log]# df -ia文件系统 Inode (I)已用 (I)可用 (I)已用% 挂载点/dev/sda7 5124480 5560 5118920 1%/proc 0 0 0 - /procsysfs 0 0 0 - /sysdevpts 0 0 0 - /dev/pts/dev/sda9 52592640 50519 52542121 1% /opt/dev/sda8 1280000 8799 1271201 1% /var/dev/sda6 5124480 80163 5044317 2% /usr/dev/sda3 255232 34 255198 1% /boottmpfs 4118303 1 4118302 1% /dev/shmnone 0 0 0 - /proc/sys/fs/binfmt_misc 实例五：列出文件系统的类型命令:1df -T 输出：12345678root@CT1190 log]# df -T文件系统 类型 1K-块 已用 可用 已用% 挂载点/dev/sda7 ext3 19840892 890896 17925856 5% //dev/sda9 ext3 203727156 93175692 100035720 49% /opt/dev/sda8 ext3 4956284 570104 4130348 13% /var/dev/sda6 ext3 19840892 1977568 16839184 11% /usr/dev/sda3 ext3 988116 23880 913232 3% /boottmpfs tmpfs 16473212 0 16473212 0% /dev/shm 实例六：以更易读的方式显示目前磁盘空间和使用情况命令:1df -h 输出：12345678[root@CT1190 log]# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sda7 19G 871M 18G 5% //dev/sda9 195G 89G 96G 49% /opt/dev/sda8 4.8G 557M 4.0G 13% /var/dev/sda6 19G 1.9G 17G 11% /usr/dev/sda3 965M 24M 892M 3% /boottmpfs 16G 0 16G 0% /dev/shm 12345678[root@CT1190 log]# df -H文件系统 容量 已用 可用 已用% 挂载点/dev/sda7 21G 913M 19G 5% //dev/sda9 209G 96G 103G 49% /opt/dev/sda8 5.1G 584M 4.3G 13% /var/dev/sda6 21G 2.1G 18G 11% /usr/dev/sda3 1.1G 25M 936M 3% /boottmpfs 17G 0 17G 0% /dev/shm 12345678[root@CT1190 log]# df -lh文件系统 容量 已用 可用 已用% 挂载点/dev/sda7 19G 871M 18G 5% //dev/sda9 195G 89G 96G 49% /opt/dev/sda8 4.8G 557M 4.0G 13% /var/dev/sda6 19G 1.9G 17G 11% /usr/dev/sda3 965M 24M 892M 3% /boottmpfs 16G 0 16G 0% /dev/shm 12345678[root@CT1190 log]# df -k文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda7 19840892 890896 17925856 5% //dev/sda9 203727156 93292572 99918840 49% /opt/dev/sda8 4956284 570188 4130264 13% /var/dev/sda6 19840892 1977568 16839184 11% /usr/dev/sda3 988116 23880 913232 3% /boottmpfs 16473212 0 16473212 0% /dev/shm 说明： -h更具目前磁盘空间和使用情况 以更易读的方式显示 -H根上面的-h参数相同,不过在根式化的时候,采用1000而不是1024进行容量转换 -k以单位显示磁盘的使用情况 -l显示本地的分区的磁盘空间使用率,如果服务器nfs了远程服务器的磁盘,那么在df上加上-l后系统显示的是过滤nsf驱动器后的结果 -i显示inode的使用情况。linux采用了类似指针的方式管理磁盘空间影射.这也是一个比较关键应用","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"df","slug":"df","permalink":"ly2513.github.com/tags/df/"}]},{"title":"每天掌握一个Linux命令(32): gzip命令","date":"2017-03-29T09:37:38.000Z","path":"2017/03/29/每天掌握一个Linux命令-32-gzip命令/","text":"减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。gzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。gzip不仅可以用来压缩大的、较少使用的文件以节省磁盘空间，还可以和tar命令一起构成Linux操作系统中比较流行的压缩文件格式。据统计，gzip命令对文本文件有60%～70%的压缩率。 1．命令格式： gzip[参数][文件或者目录] 2．命令功能： gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多出”.gz”的扩展名。 3．命令参数： -a或–ascii 使用ASCII文字模式。-c或–stdout或–to-stdout 把压缩后的文件输出到标准输出设备，不去更动原始文件。-d或–decompress或—-uncompress 解开压缩文件。-f或–force 强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接。-h或–help 在线帮助。-l或–list 列出压缩文件的相关信息。-L或–license 显示版本与版权信息。-n或–no-name 压缩文件时，不保存原来的文件名称及时间戳记。-N或–name 压缩文件时，保存原来的文件名称及时间戳记。-q或–quiet 不显示警告信息。-r或–recursive 递归处理，将指定目录下的所有文件及子目录一并处理。-S&lt;压缩字尾字符串&gt;或—-suffix&lt;压缩字尾字符串&gt; 更改压缩字尾字符串。-t或–test 测试压缩文件是否正确无误。-v或–verbose 显示指令执行过程。-V或–version 显示版本信息。-num 用指定的数字num调整压缩的速度，-1或–fast表示最快压缩方法（低压缩比），-9或–best表示最慢压缩方法（高压缩比）。系统缺省值为6。 4．使用实例： 实例一：把test6目录下的每个文件压缩成.gz文件命令：gzip * 输出： [root@localhost test6]# ll总计 604—xr–r– 1 root mail 302108 11-30 08:39 linklog.log—xr–r– 1 mail users 302108 11-30 08:39 log2012.log-rw-r–r– 1 mail users 61 11-30 08:39 log2013.log-rw-r–r– 1 root mail 0 11-30 08:39 log2014.log-rw-r–r– 1 root mail 0 11-30 08:39 log2015.log-rw-r–r– 1 root mail 0 11-30 08:39 log2016.log-rw-r–r– 1 root mail 0 11-30 08:39 log2017.log[root@localhost test6]# gzip *[root@localhost test6]# ll总计 28—xr–r– 1 root mail 1341 11-30 08:39 linklog.log.gz—xr–r– 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r–r– 1 mail users 70 11-30 08:39 log2013.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2014.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2015.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2016.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2017.log.gz[root@localhost test6]# 实例二：把例1中每个压缩的文件解压，并列出详细的信息命令：gzip -dv * 输出： [root@localhost test6]# ll总计 28—xr–r– 1 root mail 1341 11-30 08:39 linklog.log.gz—xr–r– 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r–r– 1 mail users 70 11-30 08:39 log2013.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2014.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2015.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2016.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2017.log.gz[root@localhost test6]# gzip -dv *linklog.log.gz: 99.6% – replaced with linklog.loglog2012.log.gz: 99.6% – replaced with log2012.loglog2013.log.gz: 47.5% – replaced with log2013.loglog2014.log.gz: 0.0% – replaced with log2014.loglog2015.log.gz: 0.0% – replaced with log2015.loglog2016.log.gz: 0.0% – replaced with log2016.loglog2017.log.gz: 0.0% – replaced with log2017.log[root@localhost test6]# ll总计 604—xr–r– 1 root mail 302108 11-30 08:39 linklog.log—xr–r– 1 mail users 302108 11-30 08:39 log2012.log-rw-r–r– 1 mail users 61 11-30 08:39 log2013.log-rw-r–r– 1 root mail 0 11-30 08:39 log2014.log-rw-r–r– 1 root mail 0 11-30 08:39 log2015.log-rw-r–r– 1 root mail 0 11-30 08:39 log2016.log-rw-r–r– 1 root mail 0 11-30 08:39 log2017.log[root@localhost test6]# 实例三：详细显示例1中每个压缩的文件的信息，并不解压命令：gzip -l * 输出： [root@localhost test6]# gzip -l * compressed uncompressed ratio uncompressed_name 1341 302108 99.6% linklog.log 1341 302108 99.6% log2012.log 70 61 47.5% log2013.log 32 0 0.0% log2014.log 32 0 0.0% log2015.log 32 0 0.0% log2016.log 32 0 0.0% log2017.log 2880 604277 99.5% (totals) 实例四：压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gz 命令：gzip -r log.tar 输出： [root@localhost test]# ls -al log.tar-rw-r–r– 1 root root 307200 11-29 17:54 log.tar[root@localhost test]# gzip -r log.tar[root@localhost test]# ls -al log.tar.gz-rw-r–r– 1 root root 1421 11-29 17:54 log.tar.gz 实例五：递归的压缩目录 命令：gzip -rv test6 输出： [root@localhost test6]# ll总计 604—xr–r– 1 root mail 302108 11-30 08:39 linklog.log—xr–r– 1 mail users 302108 11-30 08:39 log2012.log-rw-r–r– 1 mail users 61 11-30 08:39 log2013.log-rw-r–r– 1 root mail 0 11-30 08:39 log2014.log-rw-r–r– 1 root mail 0 11-30 08:39 log2015.log-rw-r–r– 1 root mail 0 11-30 08:39 log2016.log-rw-r–r– 1 root mail 0 11-30 08:39 log2017.log[root@localhost test6]# cd ..[root@localhost test]# gzip -rv test6test6/log2014.log: 0.0% – replaced with test6/log2014.log.gztest6/linklog.log: 99.6% – replaced with test6/linklog.log.gztest6/log2015.log: 0.0% – replaced with test6/log2015.log.gztest6/log2013.log: 47.5% – replaced with test6/log2013.log.gztest6/log2012.log: 99.6% – replaced with test6/log2012.log.gztest6/log2017.log: 0.0% – replaced with test6/log2017.log.gztest6/log2016.log: 0.0% – replaced with test6/log2016.log.gz[root@localhost test]# cd test6[root@localhost test6]# ll总计 28—xr–r– 1 root mail 1341 11-30 08:39 linklog.log.gz—xr–r– 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r–r– 1 mail users 70 11-30 08:39 log2013.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2014.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2015.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2016.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2017.log.gz 说明： 这样，所有test下面的文件都变成了.gz，目录依然存在只是目录里面的文件相应变成了.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。 实例六：递归地解压目录命令：gzip -dr test6 输出： [root@localhost test6]# ll总计 28—xr–r– 1 root mail 1341 11-30 08:39 linklog.log.gz—xr–r– 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r–r– 1 mail users 70 11-30 08:39 log2013.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2014.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2015.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2016.log.gz-rw-r–r– 1 root mail 32 11-30 08:39 log2017.log.gz[root@localhost test6]# cd ..[root@localhost test]# gzip -dr test6[root@localhost test]# cd test6[root@localhost test6]# ll总计 604—xr–r– 1 root mail 302108 11-30 08:39 linklog.log—xr–r– 1 mail users 302108 11-30 08:39 log2012.log-rw-r–r– 1 mail users 61 11-30 08:39 log2013.log-rw-r–r– 1 root mail 0 11-30 08:39 log2014.log-rw-r–r– 1 root mail 0 11-30 08:39 log2015.log-rw-r–r– 1 root mail 0 11-30 08:39 log2016.log-rw-r–r– 1 root mail 0 11-30 08:39 log2017.log[root@localhost test6]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"gzip","slug":"gzip","permalink":"ly2513.github.com/tags/gzip/"}]},{"title":"每天掌握一个Linux命令(31): /etc/group文件详解","date":"2017-03-29T09:32:43.000Z","path":"2017/03/29/每天掌握一个Linux命令-31-etc-group文件详解/","text":"Linux /etc/group文件与/etc/passwd和/etc/shadow文件都是有关于系统管理员对用户和用户组管理时相关的文件。linux /etc/group文件是有关于系统管理员对用户和用户组管理的文件,linux用户组的所有信息都存放在/etc/group文件中。具有某种共同特征的用户集合起来就是用户组（Group）。用户组（Group）配置文件主要有 /etc/group和/etc/gshadow，其中/etc/gshadow是/etc/group的加密信息文件。 将用户分组是Linux系统中对用户进行管理及控制访问权限的一种手段。每个用户都属于某个用户组；一个组中可以有多个用户，一个用户也可以属于不 同的组。当一个用户同时是多个组中的成员时，在/etc/passwd文件中记录的是用户所属的主组，也就是登录时所属的默认组，而其他组称为附加组。 用户组的所有信息都存放在/etc/group文件中。此文件的格式是由冒号(:)隔开若干个字段，这些字段具体如下： 组名:口令:组标识号:组内用户列表 具体解释： 组名： 组名是用户组的名称，由字母或数字构成。与/etc/passwd中的登录名一样，组名不应重复。 口令： 口令字段存放的是用户组加密后的口令字。一般Linux系统的用户组都没有口令，即这个字段一般为空，或者是*。 组标识号： 组标识号与用户标识号类似，也是一个整数，被系统内部用来标识组。别称GID. 组内用户列表： 是属于这个组的所有用户的列表，不同用户之间用逗号(,)分隔。这个用户组可能是用户的主组，也可能是附加组。 使用实例： 输出： [root@localhost test6]# cat /etc/grouproot:x:0:root,linuxsirbin:x:1:root,bin,daemondaemon:x:2:root,bin,daemonsys:x:3:root,bin 说明： 我们以root:x:0:root,linuxsir 为例： 用户组root，x是密码段，表示没有设置密码，GID是0,root用户组下包括root、linuxsir以及GID为0的其它用户。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"/etc/group","slug":"etc-group","permalink":"ly2513.github.com/tags/etc-group/"}]},{"title":"每天掌握一个Linux命令(30): chown命令","date":"2017-03-29T09:29:48.000Z","path":"2017/03/29/每天掌握一个Linux命令-30-chown命令/","text":"chown将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户ID；组可以是组名或者组ID；文件是以空格分开的要改变权限的文件列表，支持通配符。系统管理员经常使用chown命令，在将文件拷贝到另一个用户的名录下之后，让用户拥有使用该文件的权限。 1.命令格式： chown [选项]… [所有者][:[组]] 文件… 2．命令功能： 通过chown改变文件的拥有者和群组。在更改文件的所有者或所属群组时，可以使用用户名称和用户识别码设置。普通用户不能将自己的文件改变成其他的拥有者。其操作权限一般为管理员。 3．命令参数： 必要参数: -c 显示更改的部分的信息 -f 忽略错误信息-h 修复符号链接 -R 处理指定目录以及其子目录下的所有文件 -v 显示详细的处理信息 -deference 作用于符号链接的指向，而不是链接文件本身 选择参数: –reference=&lt;目录或文件&gt; 把指定的目录/文件作为参考，把操作的文件/目录设置成参考文件/目录相同拥有者和群组–from=&lt;当前用户：当前群组&gt; 只有当前用户和群组跟指定的用户和群组相同时才进行改变–help 显示帮助信息–version 显示版本信息 4．使用实例： 实例一：改变拥有者和群组 命令：chown mail:mail log2012.log 输出： [root@localhost test6]# ll—xr–r– 1 root users 302108 11-30 08:39 linklog.log—xr–r– 1 root users 302108 11-30 08:39 log2012.log-rw-r–r– 1 root users 61 11-30 08:39 log2013.log-rw-r–r– 1 root users 0 11-30 08:39 log2014.log-rw-r–r– 1 root users 0 11-30 08:39 log2015.log-rw-r–r– 1 root users 0 11-30 08:39 log2016.log-rw-r–r– 1 root users 0 11-30 08:39 log2017.log[root@localhost test6]# chown mail:mail log2012.log[root@localhost test6]# ll—xr–r– 1 root users 302108 11-30 08:39 linklog.log—xr–r– 1 mail mail 302108 11-30 08:39 log2012.log-rw-r–r– 1 root users 61 11-30 08:39 log2013.log-rw-r–r– 1 root users 0 11-30 08:39 log2014.log-rw-r–r– 1 root users 0 11-30 08:39 log2015.log-rw-r–r– 1 root users 0 11-30 08:39 log2016.log-rw-r–r– 1 root users 0 11-30 08:39 log2017.log[root@localhost test6]# 说明： 实例二：改变文件拥有者和群组 命令：chown root: log2012.log 输出： [root@localhost test6]# ll总计 604—xr–r– 1 root users 302108 11-30 08:39 linklog.log—xr–r– 1 mail mail 302108 11-30 08:39 log2012.log-rw-r–r– 1 root users 61 11-30 08:39 log2013.log-rw-r–r– 1 root users 0 11-30 08:39 log2014.log-rw-r–r– 1 root users 0 11-30 08:39 log2015.log-rw-r–r– 1 root users 0 11-30 08:39 log2016.log-rw-r–r– 1 root users 0 11-30 08:39 log2017.log[root@localhost test6]# chown root: log2012.log[root@localhost test6]# ll总计 604—xr–r– 1 root users 302108 11-30 08:39 linklog.log—xr–r– 1 root root 302108 11-30 08:39 log2012.log-rw-r–r– 1 root users 61 11-30 08:39 log2013.log-rw-r–r– 1 root users 0 11-30 08:39 log2014.log-rw-r–r– 1 root users 0 11-30 08:39 log2015.log-rw-r–r– 1 root users 0 11-30 08:39 log2016.log-rw-r–r– 1 root users 0 11-30 08:39 log2017.log[root@localhost test6]# 说明： 实例三：改变文件群组命令：chown :mail log2012.log 输出： [root@localhost test6]# ll总计 604—xr–r– 1 root users 302108 11-30 08:39 linklog.log—xr–r– 1 root root 302108 11-30 08:39 log2012.log-rw-r–r– 1 root users 61 11-30 08:39 log2013.log-rw-r–r– 1 root users 0 11-30 08:39 log2014.log-rw-r–r– 1 root users 0 11-30 08:39 log2015.log-rw-r–r– 1 root users 0 11-30 08:39 log2016.log-rw-r–r– 1 root users 0 11-30 08:39 log2017.log[root@localhost test6]# chown :mail log2012.log[root@localhost test6]# ll总计 604—xr–r– 1 root users 302108 11-30 08:39 linklog.log—xr–r– 1 root mail 302108 11-30 08:39 log2012.log-rw-r–r– 1 root users 61 11-30 08:39 log2013.log-rw-r–r– 1 root users 0 11-30 08:39 log2014.log-rw-r–r– 1 root users 0 11-30 08:39 log2015.log-rw-r–r– 1 root users 0 11-30 08:39 log2016.log-rw-r–r– 1 root users 0 11-30 08:39 log2017.log 说明： 实例四：改变指定目录以及其子目录下的所有文件的拥有者和群组 命令：chown -R -v root:mail test6 输出： [root@localhost test]# lldrwxr-xr-x 2 root users 4096 11-30 08:39 test6[root@localhost test]# chown -R -v root:mail test6“test6/log2014.log” 的所有者已更改为 root:mail“test6/linklog.log” 的所有者已更改为 root:mail“test6/log2015.log” 的所有者已更改为 root:mail“test6/log2013.log” 的所有者已更改为 root:mail“test6/log2012.log” 的所有者已保留为 root:mail“test6/log2017.log” 的所有者已更改为 root:mail“test6/log2016.log” 的所有者已更改为 root:mail“test6” 的所有者已更改为 root:mail[root@localhost test]# lldrwxr-xr-x 2 root mail 4096 11-30 08:39 test6[root@localhost test]# cd test6[root@localhost test6]# ll总计 604—xr–r– 1 root mail 302108 11-30 08:39 linklog.log—xr–r– 1 root mail 302108 11-30 08:39 log2012.log-rw-r–r– 1 root mail 61 11-30 08:39 log2013.log-rw-r–r– 1 root mail 0 11-30 08:39 log2014.log-rw-r–r– 1 root mail 0 11-30 08:39 log2015.log-rw-r–r– 1 root mail 0 11-30 08:39 log2016.log-rw-r–r– 1 root mail 0 11-30 08:39 log2017.log","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"chown","slug":"chown","permalink":"ly2513.github.com/tags/chown/"}]},{"title":"每天掌握一个Linux命令(29): chgrp命令","date":"2017-03-29T08:39:32.000Z","path":"2017/03/29/每天掌握一个Linux命令-29-chgrp命令/","text":"在lunix系统里，文件或目录的权限的掌控以拥有者及所诉群组来管理。可以使用chgrp指令取变更文件与目录所属群组，这种方式采用群组名称或群组识别码都可以。Chgrp命令就是change group的缩写！要被改变的组名必须要在/etc/group文件内存在才行。 1．命令格式： chgrp [选项] [组] [文件] 2．命令功能： chgrp命令可采用群组名称或群组识别码的方式改变文件或目录的所属群组。使用权限是超级用户。 3．命令参数： 必要参数: -c 当发生改变时输出调试信息-f 不显示错误信息-R 处理指定目录以及其子目录下的所有文件-v 运行时显示详细的处理信息–dereference 作用于符号链接的指向，而不是符号链接本身–no-dereference 作用于符号链接本身 选择参数: –reference=&lt;文件或者目录&gt;–help 显示帮助信息–version 显示版本信息 4．使用实例： 实例一：改变文件的群组属性命令：chgrp -v bin log2012.log 输出： [root@localhost test]# ll—xrw-r– 1 root root 302108 11-13 06:03 log2012.log[root@localhost test]# chgrp -v bin log2012.log“log2012.log” 的所属组已更改为 bin[root@localhost test]# ll—xrw-r– 1 root bin 302108 11-13 06:03 log2012.log 说明： 将log2012.log文件由root群组改为bin群组 实例二：根据指定文件改变文件的群组属性命令：chgrp –reference=log2012.log log2013.log 输出： [root@localhost test]# ll—xrw-r– 1 root bin 302108 11-13 06:03 log2012.log-rw-r–r– 1 root root 61 11-13 06:03 log2013.log[root@localhost test]# chgrp –reference=log2012.log log2013.log[root@localhost test]# ll—xrw-r– 1 root bin 302108 11-13 06:03 log2012.log-rw-r–r– 1 root bin 61 11-13 06:03 log2013.log 说明： 改变文件log2013.log 的群组属性，使得文件log2013.log的群组属性和参考文件log2012.log的群组属性相同 实例三：改变指定目录以及其子目录下的所有文件的群组属性命令：chgrp -R bin test6输出： [root@localhost test]# lldrwxr-xr-x 2 root root 4096 11-30 08:39 test6[root@localhost test]# cd test6[root@localhost test6]# ll—xr–r– 1 root root 302108 11-30 08:39 linklog.log—xr–r– 1 root root 302108 11-30 08:39 log2012.log-rw-r–r– 1 root root 61 11-30 08:39 log2013.log-rw-r–r– 1 root root 0 11-30 08:39 log2014.log-rw-r–r– 1 root root 0 11-30 08:39 log2015.log-rw-r–r– 1 root root 0 11-30 08:39 log2016.log-rw-r–r– 1 root root 0 11-30 08:39 log2017.log[root@localhost test6]# cd ..[root@localhost test]# chgrp -R bin test6[root@localhost test]# cd test6[root@localhost test6]# ll—xr–r– 1 root bin 302108 11-30 08:39 linklog.log—xr–r– 1 root bin 302108 11-30 08:39 log2012.log-rw-r–r– 1 root bin 61 11-30 08:39 log2013.log-rw-r–r– 1 root bin 0 11-30 08:39 log2014.log-rw-r–r– 1 root bin 0 11-30 08:39 log2015.log-rw-r–r– 1 root bin 0 11-30 08:39 log2016.log-rw-r–r– 1 root bin 0 11-30 08:39 log2017.log[root@localhost test6]# cd ..[root@localhost test]# lldrwxr-xr-x 2 root bin 4096 11-30 08:39 test6[root@localhost test]# 说明： 改变指定目录以及其子目录下的所有文件的群组属性 实例四：通过群组识别码改变文件群组属性命令：chgrp -R 100 test6 输出： [root@localhost test]# chgrp -R 100 test6[root@localhost test]# lldrwxr-xr-x 2 root users 4096 11-30 08:39 test6[root@localhost test]# cd test6[root@localhost test6]# ll—xr–r– 1 root users 302108 11-30 08:39 linklog.log—xr–r– 1 root users 302108 11-30 08:39 log2012.log-rw-r–r– 1 root users 61 11-30 08:39 log2013.log-rw-r–r– 1 root users 0 11-30 08:39 log2014.log-rw-r–r– 1 root users 0 11-30 08:39 log2015.log-rw-r–r– 1 root users 0 11-30 08:39 log2016.log-rw-r–r– 1 root users 0 11-30 08:39 log2017.log[root@localhost test6]# 说明： 通过群组识别码改变文件群组属性，100为users群组的识别码，具体群组和群组识别码可以去/etc/group文件中查看","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"chgrp","slug":"chgrp","permalink":"ly2513.github.com/tags/chgrp/"}]},{"title":"每天掌握一个Linux命令(28): tar命令","date":"2017-03-29T08:15:45.000Z","path":"2017/03/29/每天掌握一个Linux命令-28-tar命令/","text":"通过SSH访问服务器，难免会要用到压缩，解压缩，打包，解包等，这时候tar命令就是是必不可少的一个功能强大的工具。linux中最流行的tar是麻雀虽小，五脏俱全，功能强大。 tar命令可以为linux的文件和目录创建档案。利用tar，可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。 tar最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。利用tar命令，可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。 首先要弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 linux下最常用的打包程序就是tar了，使用tar程序打出来的包我们常称为tar包，tar包文件的命令通常都是以.tar结尾的。生成tar包后，就可以用其它的程序来进行压缩。 1．命令格式： tar[必要参数][选择参数][文件] 2．命令功能： 用来压缩和解压文件。tar本身不具有压缩功能。他是调用压缩功能实现的 3．命令参数： 必要参数有如下： -A 新增压缩文件到已存在的压缩-B 设置区块大小-c 建立新的压缩文件-d 记录文件的差别-r 添加文件到已经压缩的文件-u 添加改变了和现有的文件到已经存在的压缩文件-x 从压缩的文件中提取文件-t 显示压缩文件的内容-z 支持gzip解压文件-j 支持bzip2解压文件-Z 支持compress解压文件-v 显示操作过程-l 文件系统边界设置-k 保留原有文件不覆盖-m 保留文件不被覆盖-W 确认压缩文件的正确性 可选参数如下： -b 设置区块数目-C 切换到指定目录-f 指定压缩文件–help 显示帮助信息–version 显示版本信息 4．常见解压/压缩命令 tar 解包：tar xvf FileName.tar打包：tar cvf FileName.tar DirName（注：tar是打包，不是压缩！） .gz 解压1：gunzip FileName.gz解压2：gzip -d FileName.gz压缩：gzip FileName .tar.gz 和 .tgz 解压：tar zxvf FileName.tar.gz压缩：tar zcvf FileName.tar.gz DirName .bz2 解压1：bzip2 -d FileName.bz2解压2：bunzip2 FileName.bz2压缩： bzip2 -z FileName .tar.bz2 解压：tar jxvf FileName.tar.bz2压缩：tar jcvf FileName.tar.bz2 DirName .bz 解压1：bzip2 -d FileName.bz解压2：bunzip2 FileName.bz压缩：未知 .tar.bz 解压：tar jxvf FileName.tar.bz压缩：未知 .Z 解压：uncompress FileName.Z压缩：compress FileName .tar.Z 解压：tar Zxvf FileName.tar.Z压缩：tar Zcvf FileName.tar.Z DirName .zip 解压：unzip FileName.zip压缩：zip FileName.zip DirName .rar 解压：rar x FileName.rar压缩：rar a FileName.rar DirName 5．使用实例 实例一：将文件全部打包成tar包命令： tar -cvf log.tar log2012.logtar -zcvf log.tar.gz log2012.logtar -jcvf log.tar.bz2 log2012.log 输出： [root@localhost test]# ls -al log2012.log—xrw-r– 1 root root 302108 11-13 06:03 log2012.log[root@localhost test]# tar -cvf log.tar log2012.loglog2012.log[root@localhost test]# tar -zcvf log.tar.gz log2012.loglog2012.log[root@localhost test]# tar -jcvf log.tar.bz2 log2012.loglog2012.log[root@localhost test]# ls -al .tar-rw-r–r– 1 root root 307200 11-29 17:54 log.tar-rw-r–r– 1 root root 1413 11-29 17:55 log.tar.bz2-rw-r–r– 1 root root 1413 11-29 17:54 log.tar.gz 说明： tar -cvf log.tar log2012.log 仅打包，不压缩！tar -zcvf log.tar.gz log2012.log 打包后，以 gzip 压缩tar -zcvf log.tar.bz2 log2012.log 打包后，以 bzip2 压缩 在参数 f 之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加 z 参数，则以 .tar.gz 或 .tgz 来代表 gzip 压缩过的 tar包； 如果加 j 参数，则以 .tar.bz2 来作为tar包名。 实例二：查阅上述 tar包内有哪些文件 命令：tar -ztvf log.tar.gz 输出： [root@localhost test]# tar -ztvf log.tar.gz—xrw-r– root/root 302108 2012-11-13 06:03:25 log2012.log 说明： 由于我们使用 gzip 压缩的log.tar.gz，所以要查阅log.tar.gz包内的文件时，就得要加上 z 这个参数了。 实例三：将tar 包解压缩 命令：tar -zxvf /opt/soft/test/log.tar.gz 输出： [root@localhost test3]# ll总计 0[root@localhost test3]# tar -zxvf /opt/soft/test/log.tar.gzlog2012.log[root@localhost test3]# lslog2012.log[root@localhost test3]# 说明： 在预设的情况下，我们可以将压缩档在任何地方解开的 实例四：只将 /tar 内的 部分文件解压出来 命令：tar -zxvf /opt/soft/test/log30.tar.gz log2013.log 输出： [root@localhost test]# tar -zcvf log30.tar.gz log2012.log log2013.loglog2012.loglog2013.log[root@localhost test]# ls -al log30.tar.gz-rw-r–r– 1 root root 1512 11-30 08:19 log30.tar.gz[root@localhost test]# tar -zxvf log30.tar.gz log2013.loglog2013.log[root@localhost test]# ll-rw-r–r– 1 root root 1512 11-30 08:19 log30.tar.gz[root@localhost test]# cd test3[root@localhost test3]# tar -zxvf /opt/soft/test/log30.tar.gz log2013.loglog2013.log[root@localhost test3]# ll总计 4-rw-r–r– 1 root root 61 11-13 06:03 log2013.log[root@localhost test3]# 说明： 我可以透过 tar -ztvf 来查阅 tar 包内的文件名称，如果单只要一个文件，就可以透过这个方式来解压部分文件！ 实例五：文件备份下来，并且保存其权限 命令：tar -zcvpf log31.tar.gz log2014.log log2015.log log2016.log 输出： [root@localhost test]# ll总计 0-rw-r–r– 1 root root 0 11-13 06:03 log2014.log-rw-r–r– 1 root root 0 11-13 06:06 log2015.log-rw-r–r– 1 root root 0 11-16 14:41 log2016.log[root@localhost test]# tar -zcvpf log31.tar.gz log2014.log log2015.log log2016.loglog2014.loglog2015.loglog2016.log[root@localhost test]# cd test6[root@localhost test6]# ll[root@localhost test6]# tar -zxvpf /opt/soft/test/log31.tar.gzlog2014.loglog2015.loglog2016.log[root@localhost test6]# ll总计 0-rw-r–r– 1 root root 0 11-13 06:03 log2014.log-rw-r–r– 1 root root 0 11-13 06:06 log2015.log-rw-r–r– 1 root root 0 11-16 14:41 log2016.log[root@localhost test6]# 说明： 这个 -p 的属性是很重要的，尤其是当您要保留原本文件的属性时 实例六：在 文件夹当中，比某个日期新的文件才备份 命令：tar -N “2012/11/13” -zcvf log17.tar.gz test 输出： [root@localhost soft]# tar -N “2012/11/13” -zcvf log17.tar.gz testtar: Treating date `2012/11/13’ as 2012-11-13 00:00:00 + 0 nanosecondstest/test/log31.tar.gztest/log2014.logtest/linklog.logtest/log2015.logtest/log2013.logtest/log2012.logtest/log2017.logtest/log2016.logtest/log30.tar.gztest/log.tartest/log.tar.bz2test/log.tar.gz 说明： 实例七：备份文件夹内容是排除部分文件 命令：tar –exclude scf/service -zcvf scf.tar.gz scf/* 输出： [root@localhost test]# tree scfscf|– bin|– doc|– lib-- service– deploy |– info `– product7 directories, 0 files[root@localhost test]# tar –exclude scf/service -zcvf scf.tar.gz scf/*scf/bin/scf/doc/scf/lib/[root@localhost test]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"tar","slug":"tar","permalink":"ly2513.github.com/tags/tar/"}]},{"title":"每天掌握一个Linux命令(27): chmod命令","date":"2017-03-29T08:09:18.000Z","path":"2017/03/29/每天掌握一个Linux命令-27-chmod命令/","text":"chmod命令用于改变linux系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 Linux系统中的每个文件和目录都有访问许可权限，用它来确定谁可以通过何种方式对文件和目录进行访问和操作。 文件或目录的访问权限分为只读，只写和可执行三种。以文件为例，只读权限表示只允许读其内容，而禁止对其做任何的更改操作。可执行权限表示允许将该文件作为一个程序执行。文件被创建时，文件所有者自动拥有对该文件的读、写和可执行权限，以便于对文件的阅读和修改。用户也可根据需要把访问权限设置为需要的任何组合。 有三种不同类型的用户可对文件或目录进行访问：文件所有者，同组用户、其他用户。所有者一般是文件的创建者。所有者可以允许同组用户有权访问文件，还可以将文件的访问权限赋予系统中的其他用户。在这种情况下，系统中每一位用户都能访问该用户拥有的文件或目录。 每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。当用ls -l命令显示文件或目录的详细信息时，最左边的一列为文件的访问权限。 例如： 命令： ls -al 输出： [root@localhost test]# ll -al总计 316lrwxrwxrwx 1 root root 11 11-22 06:58 linklog.log -&gt; log2012.log-rw-r–r– 1 root root 302108 11-13 06:03 log2012.log-rw-r–r– 1 root root 61 11-13 06:03 log2013.log-rw-r–r– 1 root root 0 11-13 06:03 log2014.log-rw-r–r– 1 root root 0 11-13 06:06 log2015.log-rw-r–r– 1 root root 0 11-16 14:41 log2016.log-rw-r–r– 1 root root 0 11-16 14:43 log2017.log 我们以log2012.log为例： -rw-r–r– 1 root root 296K 11-13 06:03 log2012.log 第一列共有10个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是d，表示是一个目录。从第二个字符开始到第十个共9个字符，3个字符一组，分别表示了3组用户对文件或者目录的权限。权限字符用横线代表空许可，r代表只读，w代表写，x代表可执行。 例如： rw- r– r– 表示log2012.log是一个普通文件；log2012.log的属主有读写权限；与log2012.log属主同组的用户只有读权限；其他用户也只有读权限。 确定了一个文件的访问权限后，用户可以利用Linux系统提供的chmod命令来重新设定不同的访问权限。也可以利用chown命令来更改某个文件或目录的所有者。利用chgrp命令来更改某个文件或目录的用户组。 chmod命令是非常重要的，用于改变文件或目录的访问权限。用户用它控制文件或目录的访问权限。chmod命令详细情况如下。 1. 命令格式:chmod [-cfvR] [–help] [–version] mode file 2. 命令功能：用于改变文件或目录的访问权限，用它控制文件或目录的访问权限。 3. 命令参数：必要参数： -c 当发生改变时，报告处理信息-f 错误信息不输出-R 处理指定目录以及其子目录下的所有文件-v 运行时显示详细处理信息 选择参数： –reference=&lt;目录或者文件&gt; 设置成具有指定目录或者文件具有相同的权限–version 显示版本信息&lt;权限范围&gt;+&lt;权限设置&gt; 使权限范围内的目录或者文件具有指定的权限&lt;权限范围&gt;-&lt;权限设置&gt; 删除权限范围的目录或者文件的指定权限&lt;权限范围&gt;=&lt;权限设置&gt; 设置权限范围内的目录或者文件的权限为指定的值 权限范围： u ：目录或者文件的当前的用户g ：目录或者文件的当前的群组o ：除了目录或者文件的当前用户或群组之外的用户或者群组a ：所有的用户及群组 权限代号： r ：读权限，用数字4表示w ：写权限，用数字2表示x ：执行权限，用数字1表示 ：删除权限，用数字0表示s ：特殊权限 该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 1）. 文字设定法: chmod ［who］ ［+ | - | =］ ［mode］ 文件名 2）. 数字设定法 我们必须首先了解用数字表示的属性的含义：0表示没有权限，1表示可执行权限，2表示可写权限，4表示可读权限，然后将其相加。所以数字属性的格式应为3个从0到7的八进制数，其顺序是（u）（g）（o）。 例如，如果想让某个文件的属主有“读/写”二种权限，需要把4（可读）+2（可写）＝6（读/写）。 数字设定法的一般形式为： chmod ［mode］ 文件名 数字与字符对应关系如下： r=4，w=2，x=1若要rwx属性则4+2+1=7若要rw-属性则4+2=6；若要r-x属性则4+1=7。 4. 使用实例：实例一：增加文件所有用户组可执行权限命令：chmod a+x log2012.log 输出： [root@localhost test]# ls -al log2012.log-rw-r–r– 1 root root 302108 11-13 06:03 log2012.log[root@localhost test]# chmod a+x log2012.log[root@localhost test]# ls -al log2012.log-rwxr-xr-x 1 root root 302108 11-13 06:03 log2012.log[root@localhost test]# 说明： 即设定文件log2012.log的属性为：文件属主（u） 增加执行权限；与文件属主同组用户（g） 增加执行权限；其他用户（o） 增加执行权限。 实例二：同时修改不同用户权限命令：chmod ug+w,o-x log2012.log 输出： [root@localhost test]# ls -al log2012.log-rwxr-xr-x 1 root root 302108 11-13 06:03 log2012.log[root@localhost test]# chmod ug+w,o-x log2012.log[root@localhost test]# ls -al log2012.log-rwxrwxr– 1 root root 302108 11-13 06:03 log2012.log 说明： 即设定文件text的属性为：文件属主（u） 增加写权限;与文件属主同组用户（g） 增加写权限;其他用户（o） 删除执行权限 实例三：删除文件权限命令：chmod a-x log2012.log 输出： [root@localhost test]# ls -al log2012.log-rwxrwxr– 1 root root 302108 11-13 06:03 log2012.log[root@localhost test]# chmod a-x log2012.log[root@localhost test]# ls -al log2012.log-rw-rw-r– 1 root root 302108 11-13 06:03 log2012.log 说明： 删除所有用户的可执行权限 实例四：使用“=”设置权限命令：chmod u=x log2012.log 输出： [root@localhost test]# ls -al log2012.log-rw-rw-r– 1 root root 302108 11-13 06:03 log2012.log[root@localhost test]# chmod u=x log2012.log[root@localhost test]# ls -al log2012.log—xrw-r– 1 root root 302108 11-13 06:03 log2012.log 说明： 撤销原来所有的权限，然后使拥有者具有可读权限 实例五：对一个目录及其子目录所有文件添加权限命令：chmod -R u+x test4 输出： [root@localhost test]# cd test4[root@localhost test4]# ls -al总计 312drwxrwxr-x 2 root root 4096 11-13 05:50 .drwxr-xr-x 5 root root 4096 11-22 06:58 ..-rw-r–r– 1 root root 302108 11-12 22:54 log2012.log-rw-r–r– 1 root root 61 11-12 22:54 log2013.log-rw-r–r– 1 root root 0 11-12 22:54 log2014.log[root@localhost test4]# cd ..[root@localhost test]# chmod -R u+x test4[root@localhost test]# cd test4[root@localhost test4]# ls -al总计 312drwxrwxr-x 2 root root 4096 11-13 05:50 .drwxr-xr-x 5 root root 4096 11-22 06:58 ..-rwxr–r– 1 root root 302108 11-12 22:54 log2012.log-rwxr–r– 1 root root 61 11-12 22:54 log2013.log-rwxr–r– 1 root root 0 11-12 22:54 log2014.log 说明： 递归地给test4目录下所有文件和子目录的属主分配权限 其他一些实例： 1）. 命令：chmod 751 file 说明： 给file的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限 2）. 命令：chmod u=rwx,g=rx,o=x file 说明： 上例的另一种形式 3）. 命令: chmod =r file 说明： 为所有用户分配读权限 4）. 命令：chmod 444 file 说明： 同上例 5）. 命令：chmod a-wx,a+r file 说明： 同上例","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"chmod","slug":"chmod","permalink":"ly2513.github.com/tags/chmod/"}]},{"title":"每天掌握一个Linux命令(26) :用SecureCRT来上传和下载文件","date":"2017-03-29T07:11:56.000Z","path":"2017/03/29/每天掌握一个Linux命令-26-用SecureCRT来上传和下载文件/","text":"用SSH管理linux服务器时经常需要远程与本地之间交互文件.而直接用SecureCRT自带的上传下载功能无疑是最方便的，SecureCRT下的文件传输协议有ASCII、Xmodem、Zmodem。 文件传输协议： 文件传输是数据交换的主要形式。在进行文件传输时，为使文件能被正确识别和传送，我们需要在两台计算机之间建立统一的传输协议。这个协议包括了文件的识别、传送的起止时间、错误的判断与纠正等内容。常见的传输协议有以下几种： ASCII：这是最快的传输协议，但只能传送文本文件。 Xmodem：这种古老的传输协议速度较慢，但由于使用了CRC错误侦测方法，传输的准确率可高达99.6%。 Ymodem：这是Xmodem的改良版，使用了1024位区段传送，速度比Xmodem要快 Zmodem：Zmodem采用了串流式（streaming）传输方式，传输速度较快，而且还具有自动改变区段大小和断点续传、快速错误侦测等功能。这是目前最流行的文件传输协议。 除以上几种外，还有Imodem、Jmodem、Bimodem、Kermit、Lynx等协议，由于没有多数厂商支持，这里就略去不讲。 SecureCRT可以使用linux下的zmodem协议来快速的传送文件,使用非常方便.具体步骤： 一．在使用SecureCRT上传下载之前需要给服务器安装lrzsz：1、从下面的地址下载 lrzsz-0.12.20.tar.gz http://down1.chinaunix.net/distfiles/lrzsz-0.12.20.tar.gz 2、查看里面的INSTALL文档了解安装参数说明和细节 3、解压文件 tar zxvf lrzsz-0.12.20.tar.gz 4、进入目录 cd lrzsz-0.12.20 5、./configure –prefix=/usr/local/lrzsz 6、make 7、make install 8、建立软链接 #cd /usr/bin #ln -s /usr/local/lrzsz/bin/lrz rz #ln -s /usr/local/lrzsz/bin/lsz sz 9、测试 运行 rz 弹出SecureCRT上传窗口,用SecureCRT来上传和下载文件。 二．设置SecureCRT上传和下载的默认目录就行options-&gt;session options -&gt;Terminal-&gt;Xmodem/Zmodem 下在右栏directory设置上传和下载的目录 三．使用Zmodem从客户端上传文件到linux服务器1.在用SecureCRT登陆linux终端. 2.选中你要放置上传文件的路径，在目录下然后输入rz命令,SecureCRT会弹出文件选择对话框，在查找范围中找到你要上传的文件，按Add按钮。然后OK就可以把文件上传到linux上了。 或者在Transfer-&gt;Zmodem Upoad list弹出文件选择对话框，选好文件后按Add按钮。然后OK窗口自动关闭。然后在linux下选中存放文件的目录，输入rz命令。liunx就把那个文件上传到这个目录下了。 四．使用Zmodem下载文件到客户端：sz filename zmodem接收可以自行启动.下载的文件存放在你设定的默认下载目录下. rz，sz是Linux/Unix同Windows进行ZModem文件传输的命令行工具windows端需要支持ZModem的telnet/ssh客户端，SecureCRT就可以用SecureCRT登陆到Unix/Linux主机（telnet或ssh均可）O 运行命令rz，即是接收文件，SecureCRT就会弹出文件选择对话框，选好文件之后关闭对话框，文件就会上传到当前目录 O 运行命令sz file1 file2就是发文件到windows上（保存的目录是可以配置） 比ftp命令方便多了，而且服务器不用再开FTP服务了","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"SecureCRT","slug":"SecureCRT","permalink":"ly2513.github.com/tags/SecureCRT/"}]},{"title":"每天掌握一个Linux命令(25) :Linux文件属性详解","date":"2017-03-29T07:09:28.000Z","path":"2017/03/29/每天掌握一个Linux命令-25-Linux文件属性详解/","text":"Linux 文件或目录的属性主要包括：文件或目录的节点、种类、权限模式、链接数量、所归属的用户和用户组、最近访问或修改的时间等内容。具体情况如下： 命令： ls -lih 输出： [root@localhost test]# ls -lih总计 316K2095120 lrwxrwxrwx 1 root root 11 11-22 06:58 linklog.log -&gt; log2012.log2095112 -rw-r–r– 1 root root 296K 11-13 06:03 log2012.log2095110 -rw-r–r– 1 root root 61 11-13 06:03 log2013.log2095107 -rw-r–r– 1 root root 0 11-13 06:03 log2014.log2095117 -rw-r–r– 1 root root 0 11-13 06:06 log2015.log2095118 -rw-r–r– 1 root root 0 11-16 14:41 log2016.log2095119 -rw-r–r– 1 root root 0 11-16 14:43 log2017.log2095113 drwxr-xr-x 6 root root 4.0K 10-27 01:58 scf2095109 drwxrwxr-x 2 root root 4.0K 11-13 06:08 test32095131 drwxrwxr-x 2 root root 4.0K 11-13 05:50 test4 说明： 第一列：inode第二列：文件种类和权限；第三列： 硬链接个数；第四列： 属主；第五列：所归属的组；第六列：文件或目录的大小；第七列和第八列：最后访问或修改时间；第九列：文件名或目录名 我们以log2012.log为例： 2095112 -rw-r–r– 1 root root 296K 11-13 06:03 log2012.log inode 的值是：2095112 文件类型：文件类型是-，表示这是一个普通文件； 关于文件的类型，请参考：《每天一个 Linux 命令（24）：Linux 文件类型与扩展名》 文件权限：文件权限是rw-r–r– ，表示文件属主可读、可写、不可执行，文件所归属的用户组不可写，可读，不可执行，其它用户不可写，可读，不可执行； 硬链接个数： log2012.log这个文件没有硬链接；因为数值是1，就是他本身； 文件属主：也就是这个文件归哪于哪个用户 ，它归于root，也就是第一个root； 文件属组：也就是说，对于这个文件，它归属于哪个用户组，在这里是root用户组； 文件大小：文件大小是296k个字节； 访问可修改时间 ：这里的时间是最后访问的时间，最后访问和文件被修改或创建的时间，有时并不是一致的； 当然文档的属性不仅仅包括这些，这些是我们最常用的一些属性。 关于inode： inode 译成中文就是索引节点。每个存储设备或存储设备的分区（存储设备是硬盘、软盘、U盘等等）被格式化为文件系统后，应该有两部份，一部份是inode，另一部份是Block，Block是用来存储数据用的。而inode呢，就是用来存储这些数 据的信息，这些信息包括文件大小、属主、归属的用户组、读写权限等。inode为每个文件进行信息索引，所以就有了inode的数值。操作系统根据指令， 能通过inode值最快的找到相对应的文件。 做个比喻，比如一本书，存储设备或分区就相当于这本书，Block相当于书中的每一页，inode 就相当于这本书前面的目录，一本书有很多的内容，如果想查找某部份的内容，我们可以先查目录，通过目录能最快的找到我们想要看的内容。虽然不太恰当，但还是比较形象。 当我们用ls 查看某个目录或文件时，如果加上-i 参数，就可以看到inode节点了；比如我们前面所说的例子： [root@localhost test]# ls -li log2012.log2095112 -rw-r–r– 1 root root 302108 11-13 06:03 log2012.log log2012.log 的inode值是 2095112 ； 查看一个文件或目录的inode，要通过ls 命令的的 -i参数。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"}]},{"title":"每天掌握一个Linux命令(24): Linux文件类型与扩展名","date":"2017-03-29T07:01:54.000Z","path":"2017/03/29/每天掌握一个Linux命令-24-Linux文件类型与扩展名/","text":"由于昨天的失误，正文有部分内容遗漏，故而今天重发。 Linux文件类型和Linux文件的文件名所代表的意义是两个不同的概念。我们通过一般应用程序而创建的比如file.txt、file.tar.gz ，这些文件虽然要用不同的程序来打开，但放在Linux文件类型中衡量的话，大多是常规文件（也被称为普通文件）。 一. 文件类型Linux文件类型常见的有：普通文件、目录文件、字符设备文件和块设备文件、符号链接文件等，现在我们进行一个简要的说明。 1. 普通文件我们用 ls -lh 来查看某个文件的属性，可以看到有类似-rwxrwxrwx，值得注意的是第一个符号是 - ，这样的文件在Linux中就是普通文件。这些文件一般是用一些相关的应用程序创建，比如图像工具、文档工具、归档工具… …. 或 cp工具等。这类文件的删除方式是用rm 命令。 另外，依照文件的内容，又大略可以分为： 纯文本档(ASCII) 这是Linux系统中最多的一种文件类型，称为纯文本档是因为内容为我们人类可以直接读到的数据，例如数字、字母等等。 几乎只要我们可以用来做为设定的文件都属于这一种文件类型。 举例来说，你可以用命令： cat ~/.bashrc 来看到该文件的内容。 (cat 是将一个文件内容读出来的指令). 二进制文件(binary) Linux系统其实仅认识且可以执行二进制文件(binary file)。Linux当中的可执行文件(scripts, 文字型批处理文件不算)就是这种格式的文件。 刚刚使用的命令cat就是一个binary file。 数据格式文件(data) 有些程序在运作的过程当中会读取某些特定格式的文件，那些特定格式的文件可以被称为数据文件 (data file)。举例来说，我们的Linux在使用者登录时，都会将登录的数据记录在 /var/log/wtmp那个文件内，该文件是一个data file，他能够透过last这个指令读出来！ 但是使用cat时，会读出乱码～因为他是属于一种特殊格式的文件？ 2. 目录文件当我们在某个目录下执行，看到有类似 drwxr-xr-x ，这样的文件就是目录，目录在Linux是一个比较特殊的文件。注意它的第一个字符是d。创建目录的命令可以用 mkdir 命令，或cp命令，cp可以把一个目录复制为另一个目录。删除用rm 或rmdir命令。 3. 字符设备或块设备文件如时您进入/dev目录，列一下文件，会看到类似如下的: [root@localhost ~]# ls -al /dev/ttycrw-rw-rw- 1 root tty 5, 0 11-03 15:11 /dev/tty[root@localhost ~]# ls -la /dev/sda1brw-r—– 1 root disk 8, 1 11-03 07:11 /dev/sda1 我们看到/dev/tty的属性是 crw-rw-rw- ，注意前面第一个字符是 c ，这表示字符设备文件。比如猫等串口设备。我们看到 /dev/sda1 的属性是 brw-r—– ，注意前面的第一个字符是b，这表示块设备，比如硬盘，光驱等设备。 这个种类的文件，是用mknode来创建，用rm来删除。目前在最新的Linux发行版本中，我们一般不用自己来创建设备文件。因为这些文件是和内核相关联的。 与系统周边及储存等相关的一些文件， 通常都集中在/dev这个目录之下！通常又分为两种： 区块(block)设备档 ： 就是一些储存数据， 以提供系统随机存取的接口设备，举例来说，硬盘与软盘等就是啦！ 你可以随机的在硬盘的不同区块读写，这种装置就是成组设备！你可以自行查一下/dev/sda看看， 会发现第一个属性为[ b ]！ 字符(character)设备文件： 亦即是一些串行端口的接口设备， 例如键盘、鼠标等等！这些设备的特色就是一次性读取的，不能够截断输出。 举例来说，你不可能让鼠标跳到另一个画面，而是滑动到另一个地方！第一个属性为 [ c ]。 4. 数据接口文件(sockets)：数据接口文件（或者：套接口文件），这种类型的文件通常被用在网络上的数据承接了。我们可以启动一个程序来监听客户端的要求， 而客户端就可以透过这个socket来进行数据的沟通了。第一个属性为 [ s ]， 最常在/var/run这个目录中看到这种文件类型了。 例如：当我们启动MySQL服务器时，会产生一个mysql.sock的文件。 [root@localhost ~]# ls -lh /var/lib/mysql/mysql.socksrwxrwxrwx 1 mysql mysql 0 04-19 11:12 /var/lib/mysql/mysql.sock 注意这个文件的属性的第一个字符是 s。 5. 符号链接文件：当我们查看文件属性时，会看到有类似 lrwxrwxrwx,注意第一个字符是l，这类文件是链接文件。是通过ln -s 源文件名 新文件名 。上面是一个例子，表示setup.log是install.log的软链接文件。怎么理解呢？这和Windows操作系统中的快捷方式有点相似。 符号链接文件的创建方法举例: [root@localhost test]# ls -lh log2012.log-rw-r–r– 1 root root 296K 11-13 06:03 log2012.log[root@localhost test]# ln -s log2012.log linklog.log[root@localhost test]# ls -lh *.loglrwxrwxrwx 1 root root 11 11-22 06:58 linklog.log -&gt; log2012.log-rw-r–r– 1 root root 296K 11-13 06:03 log2012.log 6. 数据输送文件（FIFO,pipe）:FIFO也是一种特殊的文件类型，他主要的目的在解决多个程序同时存取一个文件所造成的错误问题。 FIFO是first-in-first-out的缩写。第一个属性为[p] 。 二. Linux文件扩展名1. 扩展名类型基本上，Linux的文件是没有所谓的扩展名的，一个Linux文件能不能被执行，与他的第一栏的十个属性有关， 与档名根本一点关系也没有。这个观念跟Windows的情况不相同喔！在Windows底下， 能被执行的文件扩展名通常是 .com .exe .bat等等，而在Linux底下，只要你的权限当中具有x的话，例如[ -rwx-r-xr-x ] 即代表这个文件可以被执行。 不过，可以被执行跟可以执行成功是不一样的～举例来说，在root家目录下的install.log 是一个纯文本档，如果经由修改权限成为 -rwxrwxrwx 后，这个文件能够真的执行成功吗？ 当然不行～因为他的内容根本就没有可以执行的数据。所以说，这个x代表这个文件具有可执行的能力， 但是能不能执行成功，当然就得要看该文件的内容. 虽然如此，不过我们仍然希望可以藉由扩展名来了解该文件是什么东西，所以，通常我们还是会以适当的扩展名来表示该文件是什么种类的。底下有数种常用的扩展名： *.sh ： 脚本或批处理文件 (scripts)，因为批处理文件为使用shell写成的，所以扩展名就编成 .sh Z, .tar, .tar.gz, .zip, *.tgz： 经过打包的压缩文件。这是因为压缩软件分别为 gunzip, tar 等等的，由于不同的压缩软件，而取其相关的扩展名！ .html, .php：网页相关文件，分别代表 HTML 语法与 PHP 语法的网页文件。 .html 的文件可使用网页浏览器来直接开启，至于 .php 的文件， 则可以透过 client 端的浏览器来 server 端浏览，以得到运算后的网页结果。 基本上，Linux系统上的文件名真的只是让你了解该文件可能的用途而已，真正的执行与否仍然需要权限的规范才行。例如虽然有一个文件为可执行文件，如常见的/bin/ls这个显示文件属性的指令，不过，如果这个文件的权限被修改成无法执行时，那么ls就变成不能执行。 上述的这种问题最常发生在文件传送的过程中。例如你在网络上下载一个可执行文件，但是偏偏在你的 Linux系统中就是无法执行！呵呵！那么就是可能文件的属性被改变了。不要怀疑，从网络上传送到你的 Linux系统中，文件的属性与权限确实是会被改变的。 2. Linux文件名长度限制：在Linux底下，使用预设的Ext2/Ext3文件系统时，针对文件名长度限制为： 单一文件或目录的最大容许文件名为 255 个字符 包含完整路径名称及目录 (/) 之完整档名为 4096 个字符 是相当长的档名！我们希望Linux的文件名可以一看就知道该文件在干嘛的， 所以档名通常是很长很长。 3. Linux文件名的字符的限制：由于Linux在文字接口下的一些指令操作关系，一般来说，你在设定Linux底下的文件名时， 最好可以避免一些特殊字符比较好！例如底下这些： ? &gt; &lt; ; &amp; ! [ ] | \\ ‘ “ ` ( ) { } 因为这些符号在文字接口下，是有特殊意义的。另外，文件名的开头为小数点“.”时， 代表这个文件为隐藏文件！同时，由于指令下达当中，常常会使用到 -option 之类的选项， 所以你最好也避免将文件档名的开头以 - 或 + 来命名。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"}]},{"title":"每天掌握一个Linux命令(23): Linux目录结构","date":"2017-03-29T06:14:55.000Z","path":"2017/03/29/每天掌握一个Linux命令-23-Linux目录结构/","text":"对于每一个Linux学习者来说，了解Linux文件系统的目录结构，是学好Linux的至关重要的一步.，深入了解linux文件目录结构的标准和每个目录的详细功能，对于我们用好linux系统只管重要，下面我们就开始了解一下linux目录结构的相关知识。 当在使用Linux的时候，如果您通过ls –l / 就会发现，在/下包涵很多的目录，比如etc、usr、var、bin … … 等目录，而在这些目录中，我们进去看看，发现也有很多的目录或文件。文件系统在Linux下看上去就象树形结构，所以我们可以把文件系统的结构形象的称为 树形结构。 文件系统的是用来组织和排列文件存取的，所以她是可见的，在Linux中，我们可以通过ls等工具来查看其结构，在Linux系统中，我们见到的都是树形结构；比如操作系统安装在一个文件系统中，他表现为由/ 起始的树形结构。linux文件系统的最顶端是/，我们称/为Linux的root，也就是 Linux操作系统的文件系统。Linux的文件系统的入口就是/，所有的目录、文件、设备都在/之下，/就是Linux文件系统的组织者，也是最上级的领导者。 由于linux是开放源代码，各大公司和团体根据linux的核心代码做各自的操作，编程。这样就造成在根下的目录的不同。这样就造成个人不能使用他人的linux系统的PC。因为你根本不知道一些基本的配置，文件在哪里。。。这就造成了混乱。这就是FHS（Filesystem Hierarchy Standard ）机构诞生的原因。该机构是linux爱好者自发的组成的一个团体，主要是是对linux做一些基本的要求，不至于是操作者换一台主机就成了linux的‘文盲’。 根据FHS(http://www.pathname.com/fhs/)的官方文件指出， 他们的主要目的是希望让使用者可以了解到已安装软件通常放置于那个目录下， 所以他们希望独立的软件开发商、操作系统制作者、以及想要维护系统的用户，都能够遵循FHS的标准。 也就是说，FHS的重点在于规范每个特定的目录下应该要放置什么样子的数据而已。 这样做好处非常多，因为Linux操作系统就能够在既有的面貌下(目录架构不变)发展出开发者想要的独特风格。 事实上，FHS是根据过去的经验一直再持续的改版的，FHS依据文件系统使用的频繁与否与是否允许使用者随意更动， 而将目录定义成为四种交互作用的形态，用表格来说有点像底下这样： 可分享的(shareable) 不可分享的(unshareable) 不变的(static) /usr (软件放置处) /etc (配置文件) /opt (第三方协力软件) /boot (开机与核心档) 可变动的(variable) /var/mail (使用者邮件信箱) /var/run (程序相关) /var/spool/news (新闻组) /var/lock (程序相关) 四种类型:1. 可分享的：可以分享给其他系统挂载使用的目录，所以包括执行文件与用户的邮件等数据， 是能够分享给网络上其他主机挂载用的目录； 2. 不可分享的：自己机器上面运作的装置文件或者是与程序有关的socket文件等， 由于仅与自身机器有关，所以当然就不适合分享给其他主机了。 3. 不变的：有些数据是不会经常变动的，跟随着distribution而不变动。 例如函式库、文件说明文件、系统管理员所管理的主机服务配置文件等等； 4. 可变动的：经常改变的数据，例如登录文件、一般用户可自行收受的新闻组等。 事实上，FHS针对目录树架构仅定义出三层目录底下应该放置什么数据而已，分别是底下这三个目录的定义： / (root, 根目录)：与开机系统有关； /usr (unix software resource)：与软件安装/执行有关； /var (variable)：与系统运作过程有关。 一. 根目录 (/) 的意义与内容：根目录是整个系统最重要的一个目录，因为不但所有的目录都是由根目录衍生出来的， 同时根目录也与开机/还原/系统修复等动作有关。 由于系统开机时需要特定的开机软件、核心文件、开机所需程序、 函式库等等文件数据，若系统出现错误时，根目录也必须要包含有能够修复文件系统的程序才行。 因为根目录是这么的重要，所以在FHS的要求方面，他希望根目录不要放在非常大的分区， 因为越大的分区内你会放入越多的数据，如此一来根目录所在分区就可能会有较多发生错误的机会。 因此FHS标准建议：根目录(/)所在分区应该越小越好， 且应用程序所安装的软件最好不要与根目录放在同一个分区内，保持根目录越小越好。 如此不但效能较佳，根目录所在的文件系统也较不容易发生问题。说白了，就是根目录和Windows的C盘一个样。 根据以上原因，FHS认为根目录(/)下应该包含如下子目录： 目录 应放置档案内容 /bin 系统有很多放置执行档的目录，但/bin比较特殊。因为/bin放置的是在单人维护模式下还能够被操作的指令。在/bin底下的指令可以被root与一般帐号所使用，主要有：cat,chmod(修改权限), chown, date, mv, mkdir, cp, bash等等常用的指令。 /boot 主要放置开机会使用到的档案，包括Linux核心档案以及开机选单与开机所需设定档等等。Linux kernel常用的档名为：vmlinuz ，如果使用的是grub这个开机管理程式，则还会存在/boot/grub/这个目录。 /dev 在Linux系统上，任何装置与周边设备都是以档案的型态存在于这个目录当中。 只要通过存取这个目录下的某个档案，就等于存取某个装置。比要重要的档案有/dev/null, /dev/zero, /dev/tty , /dev/lp, / dev/hd, /dev/sd*等等 /etc 系统主要的设定档几乎都放置在这个目录内，例如人员的帐号密码档、各种服务的启始档等等。 一般来说，这个目录下的各档案属性是可以让一般使用者查阅的，但是只有root有权力修改。 FHS建议不要放置可执行档(binary)在这个目录中。 比较重要的档案有：/etc/inittab, /etc/init.d/, /etc/modprobe.conf, /etc/X11/, /etc/fstab, /etc/sysconfig/等等。 另外，其下重要的目录有：/etc/init.d/ ：所有服务的预设启动script都是放在这里的，例如要启动或者关闭iptables的话： /etc/init.d/iptables start、/etc/init.d/ iptables stop etc/xinetd.d/ ：这就是所谓的super daemon管理的各项服务的设定档目录。 /etc/X11/ ：与X Window有关的各种设定档都在这里，尤其是xorg.conf或XF86Config这两个X Server的设定档。 /home 这是系统预设的使用者家目录(home directory)。 在你新增一个一般使用者帐号时，预设的使用者家目录都会规范到这里来。比较重要的是，家目录有两种代号：~ ：代表当前使用者的家目录，而 ~guest：则代表用户名为guest的家目录。 /lib 系统的函式库非常的多，而/lib放置的则是在开机时会用到的函式库，以及在/bin或/sbin底下的指令会呼叫的函式库而已 。 什么是函式库呢？妳可以将他想成是外挂，某些指令必须要有这些外挂才能够顺利完成程式的执行之意。 尤其重要的是/lib/modules/这个目录，因为该目录会放置核心相关的模组(驱动程式)。 /media media是媒体的英文，顾名思义，这个/media底下放置的就是可移除的装置。 包括软碟、光碟、DVD等等装置都暂时挂载于此。 常见的档名有：/media/floppy, /media/cdrom等等。 /mnt 如果妳想要暂时挂载某些额外的装置，一般建议妳可以放置到这个目录中。在古早时候，这个目录的用途与/media相同啦。 只是有了/media之后，这个目录就用来暂时挂载用了。 /opt 这个是给第三方协力软体放置的目录 。 什么是第三方协力软体啊？举例来说，KDE这个桌面管理系统是一个独立的计画，不过他可以安装到Linux系统中，因此KDE的软体就建议放置到此目录下了。 另外，如果妳想要自行安装额外的软体(非原本的distribution提供的)，那么也能够将你的软体安装到这里来。 不过，以前的Linux系统中，我们还是习惯放置在/usr/local目录下。 /root 系统管理员(root)的家目录。 之所以放在这里，是因为如果进入单人维护模式而仅挂载根目录时，该目录就能够拥有root的家目录，所以我们会希望root的家目录与根目录放置在同一个分区中。 /sbin Linux有非常多指令是用来设定系统环境的，这些指令只有root才能够利用来设定系统，其他使用者最多只能用来查询而已。放在/sbin底下的为开机过程中所需要的，里面包括了开机、修复、还原系统所需要的指令。至于某些伺服器软体程式，一般则放置到/usr/sbin/当中。至于本机自行安装的软体所产生的系统执行档(system binary)，则放置到/usr/local/sbin/当中了。常见的指令包括：fdisk, fsck, ifconfig, init, mkfs等等。 /srv srv可以视为service的缩写，是一些网路服务启动之后，这些服务所需要取用的资料目录。 常见的服务例如WWW, FTP等等。 举例来说，WWW伺服器需要的网页资料就可以放置在/srv/www/里面。呵呵，看来平时我们编写的代码应该放到这里了。 /tmp 这是让一般使用者或者是正在执行的程序暂时放置档案的地方。这个目录是任何人都能够存取的，所以你需要定期的清理一下。当然，重要资料不可放置在此目录啊。 因为FHS甚至建议在开机时，应该要将/tmp下的资料都删除。 事实上FHS针对根目录所定义的标准就仅限于上表，不过仍旧有些目录也需要我们了解一下，具体如下： 目录 应放置文件内容 /lost+found 这个目录是使用标准的ext2/ext3档案系统格式才会产生的一个目录，目的在于当档案系统发生错误时，将一些遗失的片段放置到这个目录下。 这个目录通常会在分割槽的最顶层存在，例如你加装一个硬盘于/disk中，那在这个系统下就会自动产生一个这样的目录/disk/lost+found /proc 这个目录本身是一个虚拟文件系统(virtual filesystem)喔。 他放置的资料都是在内存当中，例如系统核心、行程资讯(process)（是进程吗?）、周边装置的状态及网络状态等等。因为这个目录下的资料都是在记忆体（内存）当中，所以本身不占任何硬盘空间。比较重要的档案（目录）例如： /proc/cpuinfo, /proc/dma, /proc/interrupts, /proc/ioports, /proc/net/*等等。呵呵，是虚拟内存吗[guest]？ /sys 这个目录其实跟/proc非常类似，也是一个虚拟的档案系统，主要也是记录与核心相关的资讯。 包括目前已载入的核心模组与核心侦测到的硬体装置资讯等等。 这个目录同样不占硬盘容量。 除了这些目录的内容之外，另外要注意的是，因为根目录与开机有关，开机过程中仅有根目录会被挂载， 其他分区则是在开机完成之后才会持续的进行挂载的行为。就是因为如此，因此根目录下与开机过程有关的目录， 就不能够与根目录放到不同的分区去。那哪些目录不可与根目录分开呢？有底下这些： /etc：配置文件 /bin：重要执行档 /dev：所需要的装置文件 /lib：执行档所需的函式库与核心所需的模块 /sbin：重要的系统执行文件 这五个目录千万不可与根目录分开在不同的分区。请背下来啊。 二. /usr 的意义与内容：依据FHS的基本定义，/usr里面放置的数据属于可分享的与不可变动的(shareable, static)， 如果你知道如何透过网络进行分区的挂载(例如在服务器篇会谈到的NFS服务器)，那么/usr确实可以分享给局域网络内的其他主机来使用喔。 /usr不是user的缩写，其实usr是Unix Software Resource的缩写， 也就是Unix操作系统软件资源所放置的目录，而不是用户的数据啦。这点要注意。 FHS建议所有软件开发者，应该将他们的数据合理的分别放置到这个目录下的次目录，而不要自行建立该软件自己独立的目录。 因为是所有系统默认的软件(distribution发布者提供的软件)都会放置到/usr底下，因此这个目录有点类似Windows 系统的C:Windows + C:Program files这两个目录的综合体，系统刚安装完毕时，这个目录会占用最多的硬盘容量。 一般来说，/usr的次目录建议有底下这些： 目录 应放置文件内容 /usr/X11R6/ 为X Window System重要数据所放置的目录，之所以取名为X11R6是因为最后的X版本为第11版，且该版的第6次释出之意。 /usr/bin/ 绝大部分的用户可使用指令都放在这里。请注意到他与/bin的不同之处。(是否与开机过程有关) /usr/include/ c/c++等程序语言的档头(header)与包含档(include)放置处，当我们以tarball方式 (*.tar.gz 的方式安装软件)安装某些数据时，会使用到里头的许多包含档。 /usr/lib/ 包含各应用软件的函式库、目标文件(object file)，以及不被一般使用者惯用的执行档或脚本(script)。 某些软件会提供一些特殊的指令来进行服务器的设定，这些指令也不会经常被系统管理员操作， 那就会被摆放到这个目录下啦。要注意的是，如果你使用的是X86_64的Linux系统， 那可能会有/usr/lib64/目录产生 /usr/local/ 统管理员在本机自行安装自己下载的软件(非distribution默认提供者)，建议安装到此目录， 这样会比较便于管理。举例来说，你的distribution提供的软件较旧，你想安装较新的软件但又不想移除旧版， 此时你可以将新版软件安装于/usr/local/目录下，可与原先的旧版软件有分别啦。 你可以自行到/usr/local去看看，该目录下也是具有bin, etc, include, lib…的次目录 /usr/sbin/ 非系统正常运作所需要的系统指令。最常见的就是某些网络服务器软件的服务指令(daemon) /usr/share/ 放置共享文件的地方，在这个目录下放置的数据几乎是不分硬件架构均可读取的数据， 因为几乎都是文本文件嘛。在此目录下常见的还有这些次目录：/usr/share/man：联机帮助文件 /usr/share/doc：软件杂项的文件说明 /usr/share/zoneinfo：与时区有关的时区文件 /usr/src/ 一般原始码建议放置到这里，src有source的意思。至于核心原始码则建议放置到/usr/src/linux/目录下。 三. /var 的意义与内容：如果/usr是安装时会占用较大硬盘容量的目录，那么/var就是在系统运作后才会渐渐占用硬盘容量的目录。 因为/var目录主要针对常态性变动的文件，包括缓存(cache)、登录档(log file)以及某些软件运作所产生的文件， 包括程序文件(lock file, run file)，或者例如MySQL数据库的文件等等。常见的次目录有： 目录 应放置文件内容 /var/cache/ 应用程序本身运作过程中会产生的一些暂存档/var/lib/ 程序本身执行的过程中，需要使用到的数据文件放置的目录。在此目录下各自的软件应该要有各自的目录。 举例来说，MySQL的数据库放置到/var/lib/mysql/而rpm的数据库则放到/var/lib/rpm去/var/lock/ 某些装置或者是文件资源一次只能被一个应用程序所使用，如果同时有两个程序使用该装置时， 就可能产生一些错误的状况，因此就得要将该装置上锁(lock)，以确保该装置只会给单一软件所使用。 举例来说，刻录机正在刻录一块光盘，你想一下，会不会有两个人同时在使用一个刻录机烧片？ 如果两个人同时刻录，那片子写入的是谁的数据？所以当第一个人在刻录时该刻录机就会被上锁， 第二个人就得要该装置被解除锁定(就是前一个人用完了)才能够继续使用/var/log/ 非常重要。这是登录文件放置的目录。里面比较重要的文件如/var/log/messages, /var/log/wtmp(记录登入者的信息)等。/var/mail/ 放置个人电子邮件信箱的目录，不过这个目录也被放置到/var/spool/mail/目录中，通常这两个目录是互为链接文件。/var/run/ 某些程序或者是服务启动后，会将他们的PID放置在这个目录下/var/spool/ 这个目录通常放置一些队列数据，所谓的“队列”就是排队等待其他程序使用的数据。 这些数据被使用后通常都会被删除。举例来说，系统收到新信会放置到/var/spool/mail/中， 但使用者收下该信件后该封信原则上就会被删除。信件如果暂时寄不出去会被放到/var/spool/mqueue/中， 等到被送出后就被删除。如果是工作排程数据(crontab)，就会被放置到/var/spool/cron/目录中。由于FHS仅是定义出最上层(/)及次层(/usr, /var)的目录内容应该要放置的文件或目录数据， 因此，在其他次目录层级内，就可以随开发者自行来配置了。 四. 目录树(directory tree) :在Linux底下，所有的文件与目录都是由根目录开始的。那是所有目录与文件的源头, 然后再一个一个的分支下来，因此，我们也称这种目录配置方式为：目录树(directory tree), 这个目录树的主要特性有： 目录树的启始点为根目录 (/, root)； 每一个目录不止能使用本地端的 partition 的文件系统，也可以使用网络上的 filesystem 。举例来说， 可以利用 Network File System (NFS) 服务器挂载某特定目录等。 每一个文件在此目录树中的文件名(包含完整路径)都是独一无二的。 如果我们将整个目录树以图的方法来显示，并且将较为重要的文件数据列出来的话，那么目录树架构就如下图所示： 五. 绝对路径与相对路径除了需要特别注意的FHS目录配置外，在文件名部分我们也要特别注意。因为根据档名写法的不同，也可将所谓的路径(path)定义为绝对路径(absolute)与相对路径(relative)。 这两种文件名/路径的写法依据是这样的： 绝对路径： 由根目录(/)开始写起的文件名或目录名称， 例如 /home/dmtsai/.bashrc； 相对路径： 相对于目前路径的文件名写法。 例如 ./home/dmtsai 或 http://www.cnblogs.com/home/dmtsai/ 等等。反正开头不是 / 就属于相对路径的写法 而你必须要了解，相对路径是以你当前所在路径的相对位置来表示的。举例来说，你目前在 /home 这个目录下， 如果想要进入 /var/log 这个目录时，可以怎么写呢？ cd /var/log (absolute) cd ../var/log (relative) 因为你在 /home 底下，所以要回到上一层 (../) 之后，才能继续往 /var 来移动的，特别注意这两个特殊的目录： . ：代表当前的目录，也可以使用 ./ 来表示； .. ：代表上一层目录，也可以 ../ 来代表。 这个 . 与 .. 目录概念是很重要的，你常常会看到 cd .. 或 ./command 之类的指令下达方式， 就是代表上一层与目前所在目录的工作状态。 实例1：如何先进入/var/spool/mail/目录，再进入到/var/spool/cron/目录内？ 命令：cd /var/spool/mail cd ../cron 说明： 由于/var/spool/mail与/var/spool/cron是同样在/var/spool/目录中。如此就不需要在由根目录开始写起了。这个相对路径是非常有帮助的，尤其对于某些软件开发商来说。 一般来说，软件开发商会将数据放置到/usr/local/里面的各相对目录。 但如果用户想要安装到不同目录呢？就得要使用相对路径。 实例二：网络文件常常提到类似./run.sh之类的数据，这个指令的意义为何？说明： 由于指令的执行需要变量的支持，若你的执行文件放置在本目录，并且本目录并非正规的执行文件目录(/bin, /usr/bin等为正规)，此时要执行指令就得要严格指定该执行档。./代表本目录的意思，所以./run.sh代表执行本目录下， 名为run.sh的文件。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"}]},{"title":"每天掌握一个Linux命令(22): find命令的参数详解","date":"2017-03-15T15:54:25.000Z","path":"2017/03/15/每天掌握一个Linux命令-22-find命令的参数详解/","text":"find一些常用参数的一些常用实例和一些具体用法和注意事项。 使用name选项 文件名选项是find命令最常用的选项，要么单独使用该选项，要么和其他选项一起使用。 可以使用某种文件名模式来匹配文件，记住要用引号将文件名模式引起来。 不管当前路径是什么，如果想要在自己的根目录$HOME中查找文件名符合*.log的文件，使用~作为 ‘pathname’参数，波浪号~代表了你的$HOME目录。 1find ~ -name &quot;*.log&quot; -print 想要在当前目录及子目录中查找所有的‘ *.log‘文件，可以用： 1find . -name &quot;*.log&quot; -print 想要的当前目录及子目录中查找文件名以一个大写字母开头的文件，可以用： 1find . -name &quot;[A-Z]*&quot; -print 想要在/etc目录中查找文件名以host开头的文件，可以用： 1find /etc -name &quot;host*&quot; -print 想要查找$HOME目录中的文件，可以用： 1find ~ -name &quot;*&quot; -print 或find . -print 要想让系统高负荷运行，就从根目录开始查找所有的文件: 1find / -name &quot;*&quot; -print 如果想在当前目录查找文件名以一个个小写字母开头，最后是4到9加上.log结束的文件： 命令：find . -name “[a-z]*[4-9].log” -print 输出： 1234567891011121314[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.log-rw-r--r-- 1 root root 0 11-13 06:06 log2015.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:08 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4[root@localhost test]# find . -name &quot;[a-z]*[4-9].log&quot; -print./log2014.log./log2015.log./test4/log2014.log[root@localhost test]# 用perm选项 按照文件权限模式用-perm选项,按文件权限模式来查找文件的话。最好使用八进制的权限表示法。 如在当前目录下查找文件权限位为755的文件，即文件属主可以读、写、执行，其他用户可以读、执行的文件，可以用： 1234567891011[root@localhost test]# find . -perm 755 -print../scf./scf/lib./scf/service./scf/service/deploy./scf/service/deploy/product./scf/service/deploy/info./scf/doc./scf/bin[root@localhost test]# 还有一种表达方法：在八进制数字前面要加一个横杠-，表示都匹配，如-007就相当于777，-005相当于555, 命令：find . -perm -005 输出： 12345678910111213141516171819202122[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.log-rw-r--r-- 1 root root 0 11-13 06:06 log2015.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:08 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4[root@localhost test]# find . -perm -005../test4./scf./scf/lib./scf/service./scf/service/deploy./scf/service/deploy/product./scf/service/deploy/info./scf/doc./scf/bin./test3[root@localhost test]# 忽略某个目录 如果在查找文件时希望忽略某个目录，因为你知道那个目录中没有你所要查找的文件，那么可以使用-prune选项来指出需要忽略的目录。在使用-prune选项时要当心，因为如果你同时使用了-depth选项，那么-prune选项就会被find命令忽略。如果希望在test目录下查找文件，但不希望在test/test3目录下查找，可以用： 命令：find test -path “test/test3” -prune -o -print 输出： 12345678910111213141516171819[root@localhost soft]# find test -path &quot;test/test3&quot; -prune -o -printtesttest/log2014.logtest/log2015.logtest/test4test/test4/log2014.logtest/test4/log2013.logtest/test4/log2012.logtest/scftest/scf/libtest/scf/servicetest/scf/service/deploytest/scf/service/deploy/producttest/scf/service/deploy/infotest/scf/doctest/scf/bintest/log2013.logtest/log2012.log[root@localhost soft]# 使用find查找文件的时候怎么避开某个文件目录 例一：在test 目录下查找不在test4子目录之内的所有文件命令：find test -path “test/test4” -prune -o -print 输出： 1234567891011121314151617181920212223242526272829303132333435[root@localhost soft]# find testtesttest/log2014.logtest/log2015.logtest/test4test/test4/log2014.logtest/test4/log2013.logtest/test4/log2012.logtest/scftest/scf/libtest/scf/servicetest/scf/service/deploytest/scf/service/deploy/producttest/scf/service/deploy/infotest/scf/doctest/scf/bintest/log2013.logtest/log2012.logtest/test3[root@localhost soft]# find test -path &quot;test/test4&quot; -prune -o -printtesttest/log2014.logtest/log2015.logtest/scftest/scf/libtest/scf/servicetest/scf/service/deploytest/scf/service/deploy/producttest/scf/service/deploy/infotest/scf/doctest/scf/bintest/log2013.logtest/log2012.logtest/test3[root@localhost soft]# 说明： 1find [-path ..] [expression] 在路径列表的后面的是表达式 -path “test” -prune -o -print 是 -path “test” -a -prune -o -print 的简写表达式按顺序求值, -a 和 -o 都是短路求值，与 shell 的 &amp;&amp; 和 || 类似如果-path “test” 为真，则求值 -prune , -prune 返回真，与逻辑表达式为真；否则不求值 -prune，与逻辑表达式为假。如果 -path “test” -a -prune 为假，则求值 -print ，-print返回真，或逻辑表达式为真；否则不求值 -print，或逻辑表达式为真。 这个表达式组合特例可以用伪码写为: 1234if -path \"test\" then-pruneelse-print 例二：避开多个文件夹命令：find test ( -path test/test4 -o -path test/test3 ) -prune -o -print 输出： 123456789101112131415[root@localhost soft]# find test \\( -path test/test4 -o -path test/test3 \\) -prune -o -printtesttest/log2014.logtest/log2015.logtest/scftest/scf/libtest/scf/servicetest/scf/service/deploytest/scf/service/deploy/producttest/scf/service/deploy/infotest/scf/doctest/scf/bintest/log2013.logtest/log2012.log[root@localhost soft]# 说明： 圆括号表示表达式的结合。 \\ 表示引用，即指示 shell 不对后面的字符作特殊解释，而留给 find 命令去解释其意义。 例三：查找某一确定文件，-name等选项加在-o 之后命令：find test (-path test/test4 -o -path test/test3 ) -prune -o -name “*.log” -print 输出： 123456[root@localhost soft]# find test \\( -path test/test4 -o -path test/test3 \\) -prune -o -name &quot;*.log&quot; -printtest/log2014.logtest/log2015.logtest/log2013.logtest/log2012.log[root@localhost soft]# 使用user和nouser选项 按文件属主查找文件： 例一：在$HOME目录中查找文件属主为peida的文件命令：find ~ -user peida -print 例二：在/etc目录下查找文件属主为peida的文件命令：find /etc -user peida -print 说明： 例三：为了查找属主帐户已经被删除的文件，可以使用-nouser选项。在/home目录下查找所有的这类文件命令：find /home -nouser -print 说明： 这样就能够找到那些属主在/etc/passwd文件中没有有效帐户的文件。在使用-nouser选项时，不必给出用户名； find命令能够为你完成相应的工作。 使用group和nogroup选项 就像user和nouser选项一样，针对文件所属于的用户组， find命令也具有同样的选项，为了在/apps目录下查找属于gem用户组的文件，可以用： 1find /apps -group gem -print 要查找没有有效所属用户组的所有文件，可以使用nogroup选项。下面的find命令从文件系统的根目录处查找这样的文件: 1find / -nogroup-print 按照更改时间或访问时间等查找文件 如果希望按照更改时间来查找文件，可以使用mtime,atime或ctime选项。如果系统突然没有可用空间了，很有可能某一个文件的长度在此期间增长迅速，这时就可以用mtime选项来查找这样的文件。 用减号-来限定更改时间在距今n日以内的文件，而用加号+来限定更改时间在距今n日以前的文件。 希望在系统根目录下查找更改时间在5日以内的文件，可以用： 1find / -mtime -5 -print 为了在/var/adm目录下查找更改时间在3日以前的文件，可以用: 1find /var/adm -mtime +3 -print 查找比某个文件新或旧的文件 如果希望查找更改时间比某个文件新但比另一个文件旧的所有文件，可以使用-newer选项。 它的一般形式为： newest_file_name ! oldest_file_name 其中，！是逻辑非符号。 例一：查找更改时间比文件log2012.log新但比文件log2017.log旧的文件命令：find -newer log2012.log ! -newer log2017.log 输出： 123456789101112131415161718[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.log-rw-r--r-- 1 root root 0 11-13 06:06 log2015.log-rw-r--r-- 1 root root 0 11-16 14:41 log2016.log-rw-r--r-- 1 root root 0 11-16 14:43 log2017.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:08 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4[root@localhost test]# find -newer log2012.log ! -newer log2017.log../log2015.log./log2017.log./log2016.log./test3[root@localhost test]# 例二：查找更改时间在比log2012.log文件新的文件命令：find . -newer log2012.log -print 输出： 1234567[root@localhost test]# find -newer log2012.log../log2015.log./log2017.log./log2016.log./test3[root@localhost test]# 使用type选项： 例一：在/etc目录下查找所有的目录命令：find /etc -type d -print 例二：在当前目录下查找除目录以外的所有类型的文件命令：find . ! -type d -print 例三：在/etc目录下查找所有的符号链接文件命令：find /etc -type l -print 使用size选项： 可以按照文件长度来查找文件，这里所指的文件长度既可以用块（block）来计量，也可以用字节来计量。以字节计量文件长度的表达形式为N c；以块计量文件长度只用数字表示即可。 在按照文件长度查找文件时，一般使用这种以字节表示的文件长度，在查看文件系统的大小，因为这时使用块来计量更容易转换。 例一：在当前目录下查找文件长度大于1 M字节的文件命令：find . -size +1000000c -print 例二：在/home/apache目录下查找文件长度恰好为100字节的文件命令：find /home/apache -size 100c -print 例三：在当前目录下查找长度超过10块的文件（一块等于512字节）命令：find . -size +10 -print 使用depth选项： 在使用find命令时，可能希望先匹配所有的文件，再在子目录中查找。使用depth选项就可以使find命令这样做。这样做的一个原因就是，当在使用find命令向磁带上备份文件系统时，希望首先备份所有的文件，其次再备份子目录中的文件。 例一：find命令从文件系统的根目录开始，查找一个名为CON.FILE的文件。命令：find / -name “CON.FILE” -depth -print 说明： 它将首先匹配所有的文件然后再进入子目录中查找 使用mount选项： 在当前的文件系统中查找文件（不进入其他文件系统），可以使用find命令的mount选项。 例一：从当前目录开始查找位于本文件系统中文件名以XC结尾的文件命令：find . -name “*.XC” -mount -print","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"find","slug":"find","permalink":"ly2513.github.com/tags/find/"}]},{"title":"每天掌握一个Linux命令(21): find命令之xargs","date":"2017-03-15T15:42:43.000Z","path":"2017/03/15/每天掌握一个Linux命令-21-find命令之xargs/","text":"在使用 find命令的-exec选项处理匹配到的文件时， find命令将所有匹配到的文件一起传递给exec执行。但有些系统对能够传递给exec的命令长度有限制，这样在find命令运行几分钟之后，就会出现溢出错误。错误信息通常是“参数列太长”或“参数列溢出”。这就是xargs命令的用处所在，特别是与find命令一起使用。 find命令把匹配到的文件传递给xargs命令，而xargs命令每次只获取一部分文件而不是全部，不像-exec选项那样。这样它可以先处理最先获取的一部分文件，然后是下一批，并如此继续下去。 在有些系统中，使用-exec选项会为处理每一个匹配到的文件而发起一个相应的进程，并非将匹配到的文件全部作为参数一次执行；这样在有些情况下就会出现进程过多，系统性能下降的问题，因而效率不高； 而使用xargs命令则只有一个进程。另外，在使用xargs命令时，究竟是一次获取所有的参数，还是分批取得参数，以及每一次获取参数的数目都会根据该命令的选项及系统内核中相应的可调参数来确定。 实例 例一： 查找系统中的每一个普通文件，然后使用xargs命令来测试它们分别属于哪类文件命令：find . -type f -print | xargs file 输出： 12345678910111213[root@localhost test]# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4[root@localhost test]# find . -type f -print | xargs file./log2014.log: empty./log2013.log: empty./log2012.log: ASCII text[root@localhost test]# 例二：在整个系统中查找内存信息转储文件(core dump) ，然后把结果保存到/tmp/core.log 文件中命令：find / -name “core” -print | xargs echo “” &gt;/tmp/core.log 输出：12345678[root@localhost test]# find / -name \"core\" -print | xargs echo \"\" &gt;/tmp/core.log[root@localhost test]# cd /tmp[root@localhost tmp]# ll总计 16-rw-r--r-- 1 root root 1524 11-12 22:29 core.logdrwx------ 2 root root 4096 11-12 22:24 ssh-TzcZDx1766drwx------ 2 root root 4096 11-12 22:28 ssh-ykiRPk1815drwx------ 2 root root 4096 11-03 07:11 vmware-root 例三：在当前目录下查找所有用户具有读、写和执行权限的文件，并收回相应的写权限命令：find . -perm -7 -print | xargs chmod o-w 输出：123456789101112131415161718[root@localhost test]# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4[root@localhost test]# find . -perm -7 -print | xargs chmod o-w[root@localhost test]# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 19:32 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4[root@localhost test]# 说明： 执行命令后，文件夹scf、test3和test4的权限都发生改变 例四：用grep命令在所有的普通文件中搜索hostname这个词命令：find . -type f -print | xargs grep “hostname” 输出：1234[root@localhost test]# find . -type f -print | xargs grep \"hostname\"./log2013.log:hostnamebaidu=baidu.com./log2013.log:hostnamesina=sina.com./log2013.log:hostnames=true[root@localhost test]# 例五：用grep命令在当前目录下的所有普通文件中搜索hostnames这个词命令：find . -name * -type f -print | xargs grep “hostnames” 输出：123[root@peida test]# find . -name \\* -type f -print | xargs grep \"hostnames\"./log2013.log:hostnamesina=sina.com./log2013.log:hostnames=true[root@localhost test]# 说明： 注意，在上面的例子中， \\用来取消find命令中的*在shell中的特殊含义。 例六：使用xargs执行mv命令：find . -name “*.log” | xargs -i mv {} test4 输出：1234567891011121314151617181920212223[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:54 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4[root@localhost test]# cd test4/[root@localhost test4]# ll总计 0[root@localhost test4]# cd ..[root@localhost test]# find . -name \"*.log\" | xargs -i mv &#123;&#125; test4[root@localhost test]# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4[root@localhost test]# cd test4/[root@localhost test4]# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log[root@localhost test4]# 例七：find后执行xargs提示xargs: argument line too long解决方法：命令：find . -type f -atime +0 -print0 | xargs -0 -l1 -t rm -f 输出： 123[root@pd test4]# find . -type f -atime +0 -print0 | xargs -0 -l1 -t rm -frm -f[root@pdtest4]# 说明：-l1是一次处理一个；-t是处理之前打印出命令 例八：使用-i参数默认的前面输出用{}代替，-I参数可以指定其他代替字符，如例子中的[]命令：find . -name “file” | xargs -I [] cp [] .. 输出： 123456789101112131415161718192021[root@localhost test]# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4[root@localhost test]# cd test4[root@localhost test4]# find . -name \"file\" | xargs -I [] cp [] ..[root@localhost test4]# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log[root@localhost test4]# cd ..[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4[root@localhost test]# 说明： 使用-i参数默认的前面输出用{}代替，-I参数可以指定其他代替字符，如例子中的[] 例九：xargs的-p参数的使用命令：find . -name “*.log” | xargs -p -i mv {} .. 输出： 123456789101112131415161718192021222324252627[root@localhost test3]# ll总计 0-rw-r--r-- 1 root root 0 11-13 06:06 log2015.log[root@localhost test3]# cd ..[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:06 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4[root@localhost test]# cd test3[root@localhost test3]# find . -name \"*.log\" | xargs -p -i mv &#123;&#125; ..mv ./log2015.log .. ?...y[root@localhost test3]# ll总计 0[root@localhost test3]# cd ..[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.log-rw-r--r-- 1 root root 0 11-13 06:06 log2015.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:08 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4[root@localhost test]# 说明： -p参数会提示让你确认是否执行后面的命令,y执行，n不执行。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"find","slug":"find","permalink":"ly2513.github.com/tags/find/"}]},{"title":"laravel安装的坑","date":"2017-03-15T14:46:05.000Z","path":"2017/03/15/laravel安装的坑/","text":"在安装laravel框架的时候,遇到一些问题,本文主要讲的是怎么处理掉这些”坑”,说”坑”是我不知道什么原因导致出现这样的问题的。废话不说了,进入正题。这里的安装是在Mac OS X 系统下完成的。 安装 许多人被拦在了学习 Laravel 的第一步：安装。并不是因为安装有多复杂，而是因为【众所周知的原因】。在此我推荐一个 composer 全量中国镜像：http://pkg.phpcomposer.com/ 。启用 Composer 镜像服务作为本教程的第一项小作业请自行完成哦。 镜像配置完成后，在终端（iTerm）里切换到你想要放置该网站的目录下（如/usr/local/var/www），运行命令： 1composer create-project laravel/laravel laravel 然后，稍等片刻，当前目录下就会出现一个叫 laravel 的文件夹，安装完成啦~ 运行 为了尽可能地减缓学习曲线，推荐宝宝们使用 PHP 内置 web 服务器驱动我们的网站。运行以下命令： 123cd laravel/public# php的命令模式运行系统php -S 0.0.0.0:1024 然后去浏览器地址栏输入:127.0.0.0:1024,如果出现下面这个界面就表示安装成功了 如果出现这样的的Whoops, looks like something went wrong.话,就有问题的。经过Google搜索下,汇总了下解决方法 给bootstrap/cache 目录写的权限 重命名.env.example 文件命名为.env 假如还出现了RuntimeException... No supported encrypter found. The cipher and / or key length are invalid.的错误提示,你需要运行命令php artisan key:generate 如果出现OpenSSL extension is required这样的错误提示,说明php.ini文件中没开启php_openssl.so扩展。","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"laravel","slug":"laravel","permalink":"ly2513.github.com/tags/laravel/"}]},{"title":"每天掌握一个Linux命令(20): find命令之exec","date":"2017-03-05T11:19:53.000Z","path":"2017/03/05/每天掌握一个Linux命令-20-find命令之exec/","text":"find是我们很常用的一个Linux命令，但是我们一般查找出来的并不仅仅是看看而已，还会有进一步的操作，这个时候exec的作用就显现出来了。 exec解释： -exec 参数后面跟的是command命令，它的终止是以;为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。 {} 花括号代表前面find查找出来的文件名。 使用find时，只要把想要的操作写在一个文件里，就可以用exec来配合find查找，很方便的。在有些操作系统中只允许-exec选项执行诸如l s或ls -l这样的命令。大多数用户使用这一选项是为了查找旧文件并删除它们。建议在真正执行rm命令删除文件之前，最好先用ls命令看一下，确认它们是所要删除的文件。 exec选项后面跟随着所要执行的命令或脚本，然后是一对儿{ }，一个空格和一个\\，最后是一个分号。为了使用exec选项，必须要同时使用print选项。如果验证一下find命令，会发现该命令只输出从当前路径起的相对路径及文件名。 例一：ls -l命令放在find命令的-exec选项中命令：find . -type f -exec ls -l {} \\; 输出： 12345678910111213[root@localhost test]# find . -type f -exec ls -l &#123;&#125; \\;-rw-r--r-- 1 root root 127 10-28 16:51 ./log2014.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-2.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-3.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-1.log-rw-r--r-- 1 root root 33 10-28 16:54 ./log2013.log-rw-r--r-- 1 root root 302108 11-03 06:19 ./log2012.log-rw-r--r-- 1 root root 25 10-28 17:02 ./log.log-rw-r--r-- 1 root root 37 10-28 17:07 ./log.txt-rw-r--r-- 1 root root 0 10-28 14:47 ./test3/log3-2.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test3/log3-3.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test3/log3-1.log[root@localhost test]# 说明： 上面的例子中，find命令匹配到了当前目录下的所有普通文件，并在-exec选项中使用ls -l命令将它们列出。 例二：在目录中查找更改时间在n日以前的文件并删除它们命令：find . -type f -mtime +14 -exec rm {} \\; 输出： 1234567891011121314151617181920[root@localhost test]# ll总计 328-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 33 10-28 16:54 log2013.log-rw-r--r-- 1 root root 127 10-28 16:51 log2014.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.log-rw-r--r-- 1 root root 25 10-28 17:02 log.log-rw-r--r-- 1 root root 37 10-28 17:07 log.txtdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4[root@localhost test]# find . -type f -mtime +14 -exec rm &#123;&#125; \\;[root@localhost test]# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4[root@localhost test]# 说明： 在shell中用任何方式删除文件之前，应当先查看相应的文件，一定要小心！当使用诸如mv或rm命令时，可以使用-exec选项的安全模式。它将在对每个匹配到的文件进行操作之前提示你。 例三：在目录中查找更改时间在n日以前的文件并删除它们，在删除之前先给出提示命令：find . -name “*.log” -mtime +5 -ok rm {} \\; 输出： 1234567891011121314151617[root@localhost test]# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4[root@localhost test]# find . -name &quot;*.log&quot; -mtime +5 -ok rm &#123;&#125; \\;&lt; rm ... ./log_link.log &gt; ? y&lt; rm ... ./log2012.log &gt; ? n[root@localhost test]# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4[root@localhost test]# 说明： 在上面的例子中， find命令在当前目录中查找所有文件名以.log结尾、更改时间在5日以上的文件，并删除它们，只不过在删除之前先给出提示。 按y键删除文件，按n键不删除。 例四：-exec中使用grep命令命令：find /etc -name “passwd*” -exec grep “root” {} \\; 输出： 1234[root@localhost test]# find /etc -name &quot;passwd*&quot; -exec grep &quot;root&quot; &#123;&#125; \\;root:x:0:0:root:/root:/bin/bashroot:x:0:0:root:/root:/bin/bash[root@localhost test]# 说明： 任何形式的命令都可以在-exec选项中使用。 在上面的例子中我们使用grep命令。find命令首先匹配所有文件名为“ passwd*”的文件，例如passwd、passwd.old、passwd.bak，然后执行grep命令看看在这些文件中是否存在一个root用户。 例五：查找文件移动到指定目录命令：find . -name “*.log” -exec mv {} .. \\; 输出： 12345678910111213141516171819202122[root@localhost test]# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:49 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4[root@localhost test]# cd test3/[root@localhost test3]# ll总计 304-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.log[root@localhost test3]# find . -name &quot;*.log&quot; -exec mv &#123;&#125; .. \\;[root@localhost test3]# ll总计 0[root@localhost test3]# cd ..[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:50 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4[root@localhost test]# 例六：用exec选项执行cp命令命令：find . -name “*.log” -exec cp {} test3 \\; 输出： 123456789101112131415161718192021[root@localhost test3]# ll总计 0[root@localhost test3]# cd ..[root@localhost test]# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:50 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4[root@localhost test]# find . -name &quot;*.log&quot; -exec cp &#123;&#125; test3 \\;cp: “./test3/log2014.log” 及 “test3/log2014.log” 为同一文件cp: “./test3/log2013.log” 及 “test3/log2013.log” 为同一文件cp: “./test3/log2012.log” 及 “test3/log2012.log” 为同一文件[root@localhost test]# cd test3[root@localhost test3]# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log[root@localhost test3]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"find","slug":"find","permalink":"ly2513.github.com/tags/find/"}]},{"title":"每天掌握一个Linux命令(19): find命令概览","date":"2017-03-05T11:09:31.000Z","path":"2017/03/05/每天掌握一个Linux命令-19-find命令概览/","text":"Linux下find命令在目录结构中搜索文件，并执行指定的操作。Linux下find命令提供了相当多的查找条件，功能很强大。由于find具有强大的功能，所以它的选项也很多，其中大部分选项都值得我们花时间来了解一下。即使系统中含有网络文件系统( NFS)，find命令在该文件系统中同样有效，只你具有相应的权限。 在运行一个非常消耗资源的find命令时，很多人都倾向于把它放在后台执行，因为遍历一个大的文件系统可能会花费很长的时间(这里是指30G字节以上的文件系统)。 1.命令格式 find pathname -options [-print -exec -ok …] 2.命令功能 用于在文件树种查找文件，并作出相应的处理 3.命令参数 pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print： find命令将匹配的文件输出到标准输出。-exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为’command’ { } ;，注意{ }和；之间的空格。-ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 4.命令选项 -name 按照文件名查找文件。-perm 按照文件权限来查找文件。-prune 使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略。 -user 按照文件属主来查找文件。-group 按照文件所属的组来查找文件。-mtime -n +n 按照文件的更改时间来查找文件， - n表示文件更改时间距现在n天以内，+ n表示文件更改时间距现在n天以前。find命令还有-atime和-ctime 选项，但它们都和-m time选项。 -nogroup 查找无有效所属组的文件，即该文件所属的组在/etc/groups中不存在。-nouser 查找无有效属主的文件，即该文件的属主在/etc/passwd中不存在。-newer file1 ! file2 查找更改时间比文件file1新但比文件file2旧的文件。 -type 查找某一类型的文件，诸如：b - 块设备文件。d - 目录。c - 字符设备文件。p - 管道文件。l - 符号链接文件。f - 普通文件。 -size n：[c] 查找文件长度为n块的文件，带有c时表示文件长度以字节计。-depth：在查找文件时，首先查找当前目录中的文件，然后再在其子目录中查找。-fstype：查找位于某一类型文件系统中的文件，这些文件系统类型通常可以在配置文件/etc/fstab中找到，该配置文件中包含了本系统中有关文件系统的信息。 -mount：在查找文件时不跨越文件系统mount点。-follow：如果find命令遇到符号链接文件，就跟踪至链接所指向的文件。-cpio：对匹配的文件使用cpio命令，将这些文件备份到磁带设备中。 另外,下面三个的区别: -amin n 查找系统中最后N分钟访问的文件-atime n 查找系统中最后n24小时访问的文件-cmin n 查找系统中最后N分钟被改变文件状态的文件-ctime n 查找系统中最后n24小时被改变文件状态的文件-mmin n 查找系统中最后N分钟被改变文件数据的文件-mtime n 查找系统中最后n*24小时被改变文件数据的文件 5.使用实例 例一：查找指定时间内修改过的文件命令：find -atime -2 输出： 123456[root@localhost ~]# find -atime -2../logs/monitor./.bashrc./.bash_profile./.bash_history 说明：超找48小时内修改过的文件 例二：根据关键字查找命令：find . -name “*.log” 输出： 12345678910111213141516171819[root@localhost test]# find . -name &quot;*.log&quot;./log_link.log./log2014.log./test4/log3-2.log./test4/log3-3.log./test4/log3-1.log./log2013.log./log2012.log./log.log./test5/log5-2.log./test5/log5-3.log./test5/log.log./test5/log5-1.log./test5/test3/log3-2.log./test5/test3/log3-3.log./test5/test3/log3-1.log./test3/log3-2.log./test3/log3-3.log./test3/log3-1.log 说明：在当前目录查找 以.log结尾的文件。 “. “代表当前目录 例三：按照目录或文件的权限来查找文件命令：find /opt/soft/test/ -perm 777 输出： 12345[root@localhost test]# find /opt/soft/test/ -perm 777/opt/soft/test/log_link.log/opt/soft/test/test4/opt/soft/test/test5/test3/opt/soft/test/test3 说明： 查找/opt/soft/test/目录下 权限为 777的文件 例四：按类型查找命令：find . -type f -name “*.log” 输出： 12345678910111213141516171819[root@localhost test]# find . -type f -name &quot;*.log&quot;./log2014.log./test4/log3-2.log./test4/log3-3.log./test4/log3-1.log./log2013.log./log2012.log./log.log./test5/log5-2.log./test5/log5-3.log./test5/log.log./test5/log5-1.log./test5/test3/log3-2.log./test5/test3/log3-3.log./test5/test3/log3-1.log./test3/log3-2.log./test3/log3-3.log./test3/log3-1.log[root@localhost test]# 说明： 查找当目录，以.log结尾的普通文件 例五：查找当前所有目录并排序命令：find . -type d | sort 输出： 123456789101112131415[root@localhost test]# find . -type d | sort../scf./scf/bin./scf/doc./scf/lib./scf/service./scf/service/deploy./scf/service/deploy/info./scf/service/deploy/product./test3./test4./test5./test5/test3[root@localhost test]# 例六：按大小查找文件命令：find . -size +1000c -print 输出： 12345678910111213141516[root@localhost test]# find . -size +1000c -print../test4./scf./scf/lib./scf/service./scf/service/deploy./scf/service/deploy/product./scf/service/deploy/info./scf/doc./scf/bin./log2012.log./test5./test5/test3./test3[root@localhost test]# 说明： 查找当前目录大于1K的文件","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"find","slug":"find","permalink":"ly2513.github.com/tags/find/"}]},{"title":"每天掌握一个Linux命令(18): locate命令","date":"2017-03-05T10:57:28.000Z","path":"2017/03/05/每天掌握一个Linux命令-18-locate命令/","text":"locate 让使用者可以很快速的搜寻档案系统内是否有指定的档案。其方法是先建立一个包括系统内所有档案名称及路径的数据库，之后当寻找时就只需查询这个数据库，而不必实际深入档案系统之中了。在一般的 distribution 之中，数据库的建立都被放在 crontab 中自动执行。 1.命令格式 Locate [选择参数] [样式] 2.命令功能 locate命令可以在搜寻数据库时快速找到档案，数据库由updatedb程序来更新，updatedb是由cron daemon周期性建立的，locate命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是locate所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb每天会跑一次，可以由修改crontab来更新设定值。(etc/crontab) locate指定用在搜寻符合条件的档案，它会去储存档案与目录名称的数据库内，寻找合乎范本样式条件的档案或目录录，可以使用特殊字元（如”” 或”?”等）来指定范本样式，如指定范本为kcpaner, locate会找出所有起始字串为kcpa且结尾为ner的档案或目录，如名称为kcpartner若目录录名称为kcpa_ner则会列出该目录下包括 子目录在内的所有档案。 locate指令和find找寻档案的功能类似，但locate是透过update程序将硬盘中的所有档案和目录资料先建立一个索引数据库，在 执行loacte时直接找该索引，查询速度会较快，索引数据库一般是由操作系统管理，但也可以直接下达update强迫系统立即修改索引数据库。 3.命令参数 -e 将排除在寻找的范围之外。-1 如果 是 1．则启动安全模式。在安全模式下，使用者不会看到权限无法看到的档案。这会始速度减慢，因为 locate 必须至实际的档案系统中取得档案的权限资料。-f 将特定的档案系统排除在外，例如我们没有到理要把 proc 档案系统中的档案放在资料库中。-q 安静模式，不会显示任何错误讯息。-n 至多显示 n个输出。-r 使用正规运算式 做寻找的条件。-o 指定资料库存的名称。-d 指定资料库的路径-h 显示辅助讯息-V 显示程式的版本讯息 4.使用实例 例一:查找和pwd相关的所有文件命令：locate pwd 输出： 12345678910111213141516peida-VirtualBox ~ # locate pwd/bin/pwd/etc/.pwd.lock/sbin/unix_chkpwd/usr/bin/pwdx/usr/include/pwd.h/usr/lib/python2.7/dist-packages/twisted/python/fakepwd.py/usr/lib/python2.7/dist-packages/twisted/python/fakepwd.pyc/usr/lib/python2.7/dist-packages/twisted/python/test/test_fakepwd.py/usr/lib/python2.7/dist-packages/twisted/python/test/test_fakepwd.pyc/usr/lib/syslinux/pwd.c32/usr/share/help/C/empathy/irc-join-pwd.page/usr/share/help/ca/empathy/irc-join-pwd.page/usr/share/help/cs/empathy/irc-join-pwd.page/usr/share/help/de/empathy/irc-join-pwd.page/usr/share/help/el/empathy/irc-join-pwd.page 例二： 搜索etc目录下所有以sh开头的文件命令：locate /etc/sh 输出： 12345peida-VirtualBox ~ # locate /etc/sh/etc/shadow/etc/shadow-/etc/shellspeida-VirtualBox ~ # 例三：搜索etc目录下，所有以m开头的文件命令：locate /etc/m 输出： 1234567peida-VirtualBox ~ # locate /etc/m/etc/magic/etc/magic.mime/etc/mailcap/etc/mailcap.order/etc/manpath.config/etc/mate-settings-daemon","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"locate","slug":"locate","permalink":"ly2513.github.com/tags/locate/"}]},{"title":"每天掌握一个Linux命令(17): whereis命令","date":"2017-03-05T10:35:34.000Z","path":"2017/03/05/每天掌握一个Linux命令-17-whereis命令/","text":"whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 和find相比，whereis查找的速度非常快，这是因为linux系统会将 系统内的所有文件都记录在一个数据库文件中，当使用whereis和下面即将介绍的locate时，会从数据库中查找数据，而不是像find命令那样，通 过遍历硬盘来查找，效率自然会很高。 但是该数据库文件并不是实时更新，默认情况下时一星期更新一次，因此，我们在用whereis和locate 查找文件时，有时会找到已经被删除的数据，或者刚刚建立文件，却无法查找到，原因就是因为数据库文件没有被更新。 1.命令格式 whereis [-bmsu] [BMS 目录名 -f ] 文件名 2.命令功能 whereis命令是定位可执行文件、源代码文件、帮助文件在文件系统中的位置。这些文件的属性应属于原始代码，二进制文件，或是帮助文件。whereis 程序还具有搜索源代码、指定备用搜索路径和搜索不寻常项的能力。 3.命令参数 -b 定位可执行文件。-m 定位帮助文件。-s 定位源代码文件。-u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。-B 指定搜索可执行文件的路径。-M 指定搜索帮助文件的路径。-S 指定搜索源代码文件的路径。 4.使用实例 例一：将和**文件相关的文件都查找出来命令：whereis svn 输出：1234[root@localhost ~]# whereis tomcattomcat:[root@localhost ~]# whereis svnsvn: /usr/bin/svn /usr/local/svn /usr/share/man/man1/svn.1.gz 说明： tomcat没安装，找不出来，svn安装找出了很多相关文件 实例2：只将二进制文件 查找出来 命令：whereis -b svn 输出：1234567[root@localhost ~]# whereis -b svnsvn: /usr/bin/svn /usr/local/svn[root@localhost ~]# whereis -m svnsvn: /usr/share/man/man1/svn.1.gz[root@localhost ~]# whereis -s svnsvn:[root@localhost ~]# 说明： whereis -m svn 查出说明文档路径，whereis -s svn 找source源文件。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"whereis","slug":"whereis","permalink":"ly2513.github.com/tags/whereis/"}]},{"title":"每天掌握一个Linux命令(16): which命令","date":"2017-03-01T10:03:29.000Z","path":"2017/03/01/每天掌握一个Linux命令-16-which命令/","text":"我们经常在linux要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索： which 查看可执行文件的位置。 whereis 查看文件的位置。 locate 配合数据库查看文件位置。 find 实际搜寻硬盘查询文件名称。 which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 1.命令格式 which 可执行文件名称 2.命令功能 which指令会在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。 3.命令参数 -n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。-p 与-n参数相同，但此处的包括了文件的路径。-w 指定输出时栏位的宽度。-V 显示版本信息 4.使用实例 例一：查找文件、显示命令路径命令：which lsmod 输出： 12345[root@localhost ~]# which pwd/bin/pwd[root@localhost ~]# which adduser/usr/sbin/adduser[root@localhost ~]# 说明： which 是根据使用者所配置的 PATH 变量内的目录去搜寻可运行档的！所以，不同的 PATH 配置内容所找到的命令当然不一样的！ 例二：用 which 去找出 which命令： which which 输出： 1234[root@localhost ~]# which whichalias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde' /usr/bin/which[root@localhost ~]# 说明： 竟然会有两个 which ，其中一个是 alias 这就是所谓的『命令别名』，意思是输入 which 会等於后面接的那串命令！ 例三：找出 cd 这个命令命令：which cd 说明： cd 这个常用的命令竟然找不到啊！为什么呢？这是因为 cd 是bash 内建的命令！ 但是 which 默认是找 PATH 内所规范的目录，所以当然一定找不到的！","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"which","slug":"which","permalink":"ly2513.github.com/tags/which/"}]},{"title":"每天掌握一个Linux命令(15): tail命令","date":"2017-03-01T09:45:31.000Z","path":"2017/03/01/每天掌握一个Linux命令-15-tail命令/","text":"tail 命令从指定点开始将文件写到标准输出.使用tail命令的-f选项可以方便的查阅正在改变的日志文件,tail -f filename会把filename里最尾部的内容显示在屏幕上,并且不但刷新,使你看到最新的文件内容. 1.命令格式 tail[必要参数][选择参数][文件] 2.命令功能 用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。 3.命令参数 -f 循环读取-q 不显示处理信息-v 显示详细的处理信息-c&lt;数目&gt; 显示的字节数-n&lt;行数&gt; 显示行数–pid=PID 与-f合用,表示在进程ID,PID死掉之后结束.-q, –quiet, –silent 从不输出给出文件名的首部-s, –sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 4.使用实例 例一：显示文件末尾内容命令：tail -n 5 log2014.log 输出： 123456[root@localhost test]# tail -n 5 log2014.log2014-092014-102014-112014-12==============================[root@localhost test]# 说明： 显示文件最后5行内容 例二：循环查看文件内容命令：tail -f test.log 输出： 123456789101112131415[root@localhost ~]# ping 192.168.120.204 &gt; test.log &amp;[1] 11891[root@localhost ~]# tail -f test.logPING 192.168.120.204 (192.168.120.204) 56(84) bytes of data.64 bytes from 192.168.120.204: icmp_seq=1 ttl=64 time=0.038 ms64 bytes from 192.168.120.204: icmp_seq=2 ttl=64 time=0.036 ms64 bytes from 192.168.120.204: icmp_seq=3 ttl=64 time=0.033 ms64 bytes from 192.168.120.204: icmp_seq=4 ttl=64 time=0.027 ms64 bytes from 192.168.120.204: icmp_seq=5 ttl=64 time=0.032 ms64 bytes from 192.168.120.204: icmp_seq=6 ttl=64 time=0.026 ms64 bytes from 192.168.120.204: icmp_seq=7 ttl=64 time=0.030 ms64 bytes from 192.168.120.204: icmp_seq=8 ttl=64 time=0.029 ms64 bytes from 192.168.120.204: icmp_seq=9 ttl=64 time=0.044 ms64 bytes from 192.168.120.204: icmp_seq=10 ttl=64 time=0.033 ms64 bytes from 192.168.120.204: icmp_seq=11 ttl=64 time=0.027 ms[root@localhost ~]# 说明： ping 192.168.120.204 &gt; test.log &amp; //在后台ping远程主机。并输出文件到test.log；这种做法也使用于一个以上的档案监视。用Ctrl＋c来终止。 例三：从第5行开始显示文件命令：tail -n +5 log2014.log 输出： ```shell[root@localhost test]# cat log2014.log2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-11 2014-12[root@localhost test]# tail -n +5 log2014.log2014-052014-062014-072014-082014-092014-102014-11 2014-12```s","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"tail","slug":"tail","permalink":"ly2513.github.com/tags/tail/"}]},{"title":"每天掌握一个Linux命令(14): head命令","date":"2017-03-01T09:31:00.000Z","path":"2017/03/01/每天掌握一个Linux命令-14-head命令/","text":"head 与 tail 就像它的名字一样的浅显易懂，它是用来显示开头或结尾某个数量的文字区块，head 用来显示档案的开头至标准输出中，而 tail 想当然尔就是看档案的结尾。 1.命令格式 head [参数]… [文件]… 2.命令功能 head 用来显示档案的开头至标准输出中，默认head命令打印其相应文件的开头10行。 3.命令参数 -q 隐藏文件名-v 显示文件名-c&lt;字节&gt; 显示字节数-n&lt;行数&gt; 显示的行数 4.使用实例 例一：显示文件的前n行命令：head -n 5 log2014.log 输出： 1234567891011121314151617181920[root@localhost test]# cat log2014.log2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-12==============================[root@localhost test]# head -n 5 log2014.log2014-012014-022014-032014-042014-05[root@localhost test]# 例二：显示文件前n个字节命令：head -c 20 log2014.log 输出： 12345[root@localhost test]# head -c 20 log2014.log2014-012014-022014[root@localhost test]# 例三：文件的除了最后n个字节以外的内容命令：head -c -32 log2014.log 输出： 12345678910111213[root@localhost test]# head -c -32 log2014.log2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-12[root@localhost test]# 例四：输出文件除了最后n行的全部内容命令：head -n -6 log2014.log 输出： 12345678[root@localhost test]# head -n -6 log2014.log2014-012014-022014-032014-042014-052014-062014-07[root@localhost test]#","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"head","slug":"head","permalink":"ly2513.github.com/tags/head/"}]},{"title":"每天掌握一个Linux命令(13): less命令","date":"2017-03-01T09:14:28.000Z","path":"2017/03/01/每天掌握一个Linux命令-13-less命令/","text":"less 工具也是对文件或其它输出进行分页显示的工具，应该说是linux正统查看文件内容的工具，功能极其强大。less 的用法比起 more 更加的有弹性。 在 more 的时候，我们并没有办法向前面翻， 只能往后面看，但若使用了 less 时，就可以使用 [pageup] [pagedown] 等按键的功能来往前往后翻看文件，更容易用来查看一个文件的内容！除此之外，在 less 里头可以拥有更多的搜索功能，不止可以向下搜，也可以向上搜。 1.命令格式 less [参数] 文件 2.命令功能 less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。 3.命令参数 -b &lt;缓冲区大小&gt; 设置缓冲区的大小-e 当文件显示结束后，自动离开-f 强迫打开特殊文件，例如外围设备代号、目录和二进制文件-g 只标志最后搜索的关键词-i 忽略搜索时的大小写-m 显示类似more命令的百分比-N 显示每行的行号-o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来-Q 不使用警告音-s 显示连续空行为一行-S 行过长时间将超出部分舍弃-x &lt;数字&gt; 将“tab”键显示为规定的数字空格/字符串：向下搜索“字符串”的功能?字符串：向上搜索“字符串”的功能n：重复前一个搜索（与 / 或 ? 有关）N：反向重复前一个搜索（与 / 或 ? 有关）b 向后翻一页d 向后翻半页h 显示帮助界面Q 退出less 命令u 向前滚动半页y 向前滚动一行空格键 滚动一行回车键 滚动一页[pagedown]： 向下翻动一页[pageup]： 向上翻动一页 4.使用实例 例一：查看文件命令：less log2013.log 例二：ps查看进程信息并通过less分页显示命令：ps -ef |less 例三：查看命令历史使用记录并通过less分页显示命令：history | less 输出： 12345678910111213141516171819202122232425262728293031323334353637383940 [root@localhost test]# history | less 22 scp -r tomcat6.0.32 root@192.168.120.203:/opt/soft 23 cd .. 24 scp -r web root@192.168.120.203:/opt/ 25 cd soft 26 ls 27 scp -r jdk1.6.0_16/ root@192.168.120.203:/opt/soft 28 clear 29 vim /etc/profile 30 vim /etc/profile 31 cd tomcat6.0.32/bin/ 32 ls 33 ./shutdown.sh 34 ./startup.sh 35 vim startup.sh36 ls 37 echo $JAVA_HOME 38 java 39 ls40 ls 41 clear 42 cd /opt 43 ls 44 cp apache-tomcat-6.0.32.tar.gz soft/ 45 ls 46 rm -f apache-tomcat-6.0.32.tar.gz 47 ls 48 cd soft 49 ls 50 tar -vzf apache-tomcat-6.0.32.tar.gz 51 tar -vzfx apache-tomcat-6.0.32.tar.gz 52 tar -zxvf apache-tomcat-6.0.32.tar.gz 53 ls 54 cd apache-tomcat-6.0.32 55 ls 56 cd .. 57 mv apache-tomcat-6.0.32 tomcat6.0.32 58 ls 59 cd tomcat6.0.32/ 60 ls 例五：浏览多个文件命令：Less log2013.log log2014.log 说明： 输入 ：n后，切换到 log2014.log输入 ：p 后，切换到log2013.log 5.附加备注 全屏导航 ctrl + F - 向前移动一屏 ctrl + B - 向后移动一屏 ctrl + D - 向前移动半屏 ctrl + U - 向后移动半屏 单行导航 j - 向前移动一行 k - 向后移动一行 其它导航 G - 移动到最后一行 g - 移动到第一行 q / ZZ - 退出 less 命令 其它有用的命令 v - 使用配置的编辑器编辑当前文件 h - 显示 less 的帮助文档 &amp;pattern - 仅显示匹配模式的行，而不是整个文件 标记导航 当使用 less 查看大文件时，可以在任何一个位置作标记，可以通过命令导航到标有特定标记的文本位置： ma - 使用 a 标记文本的当前位置 ‘a - 导航到标记 a 处","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"less","slug":"less","permalink":"ly2513.github.com/tags/less/"}]},{"title":"每天掌握一个Linux命令(12): more命令","date":"2017-03-01T09:07:54.000Z","path":"2017/03/01/每天掌握一个Linux命令-12-more命令/","text":"more命令，功能类似 cat ，cat命令是整个文件的内容从上到下显示在屏幕上。 more会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示，而且还有搜寻字串的功能 。more命令从前向后读取文件，因此在启动时就加载整个文件。 1.命令格式 more [-dlfpcsu ] [-num ] [+/ pattern] [+ linenum] [file … ] 2.命令功能 more命令和cat的功能一样都是查看文件里的内容，但有所不同的是more可以按页来查看文件的内容，还支持直接跳转行等功能。 3.命令参数 +n 从笫n行开始显示-n 定义屏幕大小为n行+/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示-c 从顶部清屏，然后显示-d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能-l 忽略Ctrl+l（换页）字符-p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似-s 把连续的多个空行显示为一行-u 把文件内容中的下画线去掉 4.常用操作命令 Enter 向下n行，需要定义。默认为1行Ctrl+F 向下滚动一屏空格键 向下滚动一屏Ctrl+B 返回上一屏= 输出当前行的行号：f 输出文件名和当前行的行号V 调用vi编辑器!命令 调用Shell，并执行命令q 退出more 5.命令实例 例一：显示文件中从第3行起的内容命令：more +3 log2012.log 输出： 12345678910111213[root@localhost test]# cat log2012.log2012-012012-022012-032012-04-day12012-04-day22012-04-day3======[root@localhost test]# more +3 log2012.log2012-032012-04-day12012-04-day22012-04-day3======[root@localhost test]# 例二：从文件中查找第一个出现”day3”字符串的行，并从该处前两行开始显示输出命令：more +/day3 log2012.log 输出： 123456789[root@localhost test]# more +/day3 log2012.log...skipping2012-04-day12012-04-day22012-04-day32012-052012-05-day1======[root@localhost test]# 例三：设定每屏显示行数命令：more -5 log2012.log 输出： 123456[root@localhost test]# more -5 log2012.log2012-012012-022012-032012-04-day12012-04-day2 例四：列一个目录下的文件，由于内容太多，我们应该学会用more来分页显示。这得和管道 | 结合起来命令：ls -l | more -5 输出： 1234567891011[root@localhost test]# ls -l | more -5总计 36-rw-r--r-- 1 root root 308 11-01 16:49 log2012.log-rw-r--r-- 1 root root 33 10-28 16:54 log2013.log-rw-r--r-- 1 root root 127 10-28 16:51 log2014.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.log-rw-r--r-- 1 root root 25 10-28 17:02 log.log-rw-r--r-- 1 root root 37 10-28 17:07 log.txtdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4 说明： 每页显示5个文件信息，按 Ctrl+F 或者 空格键 将会显示下5条文件信息。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"more","slug":"more","permalink":"ly2513.github.com/tags/more/"}]},{"title":"每天掌握一个Linux命令(11): nl命令","date":"2017-03-01T08:59:18.000Z","path":"2017/03/01/每天掌握一个Linux命令-11-nl命令/","text":"nl 命令在linux系统中用来计算文件中行号。nl 可以将输出的文件内容自动的加上行号！其默认的结果与 cat -n 有点不太一样， nl 可以将行号做比较多的显示设计，包括位数与是否自动补齐 0 等等的功能。 1.命令格式 nl [选项]… [文件]… 2.命令参数 -b ：指定行号指定的方式，主要有两种： -b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)； -b t ：如果有空行，空的那一行不要列出行号(默认值)； -n ：列出行号表示的方法，主要有三种： -n ln ：行号在萤幕的最左方显示； -n rn ：行号在自己栏位的最右方显示，且不加 0 ； -n rz ：行号在自己栏位的最右方显示，且加 0 ； -w ：行号栏位的占用的位数。 -p 在逻辑定界符处不重新开始计算。 3.命令功能 nl 命令读取 File 参数（缺省情况下标准输入），计算输入中的行号，将计算过的行号写入标准输出。 在输出中，nl 命令根据您在命令行中指定的标志来计算左边的行。 输入文本必须写在逻辑页中。每个逻辑页有头、主体和页脚节（可以有空节）。 除非使用 -p 标志，nl 命令在每个逻辑页开始的地方重新设置行号。 可以单独为头、主体和页脚节设置行计算标志（例如，头和页脚行可以被计算然而文本行不能）。 4.使用实例 例一：用 nl 列出 log2012.log 的内容命令：nl log2012.log 输出： 1234[root@localhost test]# nl log2012.log 1 2012-01 2 2012-02 3 ======[root@localhost test]# 说明： 文件中的空白行，nl 不会加上行号 例二：用 nl 列出 log2012.log 的内容，空本行也加上行号命令：nl -b a log2012.log 输出： 123456[root@localhost test]# nl -b a log2012.log 1 2012-01 2 2012-02 3 4 5 ======[root@localhost test]# 例三：让行号前面自动补上0,统一输出格式 , 读取log2014.log内容,行号前面自动补0命令：nl -b a -n rz log2014.log 输出： 1234567891011121314151617181920212223242526272829[root@localhost test]# nl -b a -n rz log2014.log000001 2014-01000002 2014-02000003 2014-03000004 2014-04000005 2014-05000006 2014-06000007 2014-07000008 2014-08000009 2014-09000010 2014-10000011 2014-11000012 2014-12000013 =======[root@localhost test]# nl -b a -n rz -w 3 log2014.log001 2014-01002 2014-02003 2014-03004 2014-04005 2014-05006 2014-06007 2014-07008 2014-08009 2014-09010 2014-10011 2014-11012 2014-12013 ======= 说明： nl -b a -n rz 命令行号默认为六位，要调整位数可以加上参数 -w 3 调整为3位。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"nl","slug":"nl","permalink":"ly2513.github.com/tags/nl/"}]},{"title":"Redis百亿级Key存储方案","date":"2017-02-28T10:42:07.000Z","path":"2017/02/28/Redis百亿级Key存储方案/","text":"需求背景该应用场景为DMP缓存存储需求，DMP需要管理非常多的第三方id数据，其中包括各媒体cookie与自身cookie（以下统称supperid）的mapping关系，还包括了supperid的人口标签、移动端id（主要是idfa和imei）的人口标签，以及一些黑名单id、ip等数据。 在hdfs的帮助下离线存储千亿记录并不困难，然而DMP还需要提供毫秒级的实时查询。由于cookie这种id本身具有不稳定性，所以很多的真实用户的浏览行为会导致大量的新cookie生成，只有及时同步mapping的数据才能命中DMP的人口标签，无法通过预热来获取较高的命中，这就跟缓存存储带来了极大的挑战。 经过实际测试，对于上述数据，常规存储超过五十亿的kv记录就需要1T多的内存，如果需要做高可用多副本那带来的消耗是巨大的，另外kv的长短不齐也会带来很多内存碎片，这就需要超大规模的存储方案来解决上述问题。 存储何种数据人⼝标签主要是cookie、imei、idfa以及其对应的gender（性别）、age（年龄段）、geo（地域）等；mapping关系主要是媒体cookie对supperid的映射。以下是数据存储⽰示例： 1) PC端的ID： 媒体编号-媒体cookie=&gt;supperid supperid =&gt; { age=&gt;年龄段编码，gender=&gt;性别编码，geo=&gt;地理位置编码 } 2) Device端的ID： imei or idfa =&gt; { age=&gt;年龄段编码，gender=&gt;性别编码，geo=&gt;地理位置编码 } 显然PC数据需要存储两种key=&gt;value还有key=&gt;hashmap，⽽而Device数据需要存储⼀一种 key=&gt;hashmap即可。 数据特点短key短value：其中superid为21位数字：比如1605242015141689522；imei为小写md5：比如2d131005dc0f37d362a5d97094103633；idfa为大写带”-”md5：比如：51DFFC83-9541-4411-FA4F-356927E39D04；媒体自身的cookie长短不一；需要为全量数据提供服务，supperid是百亿级、媒体映射是千亿级、移动id是几十亿级；每天有十亿级别的mapping关系产生；对于较大时间窗口内可以预判热数据（有一些存留的稳定cookie）；对于当前mapping数据无法预判热数据，有很多是新生成的cookie； 存在的技术挑战1）长短不一容易造成内存碎片； 2）由于指针大量存在，内存膨胀率比较高，一般在7倍，纯内存存储通病； 3）虽然可以通过cookie的行为预判其热度，但每天新生成的id依然很多（百分比比较敏感，暂不透露）； 4）由于服务要求在公网环境（国内公网延迟60ms以下）下100ms以内，所以原则上当天新更新的mapping和人口标签需要全部in memory，而不会让请求落到后端的冷数据； 5）业务方面，所有数据原则上至少保留35天甚至更久； 6）内存至今也比较昂贵，百亿级Key乃至千亿级存储方案势在必行！ 解决方案5.1 淘汰策略存储吃紧的一个重要原因在于每天会有很多新数据入库，所以及时清理数据尤为重要。主要方法就是发现和保留热数据淘汰冷数据。 网民的量级远远达不到几十亿的规模，id有一定的生命周期，会不断的变化。所以很大程度上我们存储的id实际上是无效的。而查询其实前端的逻辑就是广告曝光，跟人的行为有关，所以一个id在某个时间窗口的（可能是一个campaign，半个月、几个月）访问行为上会有一定的重复性。 数据初始化之前，我们先利用hbase将日志的id聚合去重，划定TTL的范围，一般是35天，这样可以砍掉近35天未出现的id。另外在Redis中设置过期时间是35天，当有访问并命中时，对key进行续命，延长过期时间，未在35天出现的自然淘汰。这样可以针对稳定cookie或id有效，实际证明，续命的方法对idfa和imei比较实用，长期积累可达到非常理想的命中。 5.2 减少膨胀Hash表空间大小和Key的个数决定了冲突率（或者用负载因子衡量），再合理的范围内，key越多自然hash表空间越大，消耗的内存自然也会很大。再加上大量指针本身是长整型，所以内存存储的膨胀十分可观。先来谈谈如何把key的个数减少。 大家先来了解一种存储结构。我们期望将key1=&gt;value1存储在redis中，那么可以按照如下过程去存储。先用固定长度的随机散列md5(key)值作为redis的key，我们称之为BucketId，而将key1=&gt;value1存储在hashmap结构中，这样在查询的时候就可以让client按照上面的过程计算出散列，从而查询到value1。 过程变化简单描述为：get(key1) -&gt; hget(md5(key1), key1) 从而得到value1。 如果我们通过预先计算，让很多key可以在BucketId空间里碰撞，那么可以认为一个BucketId下面挂了多个key。比如平均每个BucketId下面挂10个key，那么理论上我们将会减少超过90%的redis key的个数。 具体实现起来有一些麻烦，而且用这个方法之前你要想好容量规模。我们通常使用的md5是32位的hexString（16进制字符），它的空间是128bit，这个量级太大了，我们需要存储的是百亿级，大约是33bit，所以我们需要有一种机制计算出合适位数的散列，而且为了节约内存，我们需要利用全部字符类型（ASCII码在0~127之间）来填充，而不用HexString，这样Key的长度可以缩短到一半。 下面是具体的实现方式 12345678910111213public static byte [] getBucketId(byte [] key, Integer bit) &#123; MessageDigest mdInst = MessageDigest.getInstance(&quot;MD5&quot;); mdInst.update(key); byte [] md = mdInst.digest(); byte [] r = new byte[(bit-1)/7 + 1];// 因为一个字节中只有7位能够表示成单字符 int a = (int) Math.pow(2, bit%7)-2; md[r.length-1] = (byte) (md[r.length-1] &amp; a); System.arraycopy(md, 0, r, 0, r.length); for(int i=0;i&lt;r.length;i++) &#123; if(r[i]&lt;0) r[i] &amp;= 127; &#125; return r;&#125; 参数bit决定了最终BucketId空间的大小，空间大小集合是2的整数幂次的离散值。这里解释一下为何一个字节中只有7位可用，是因为redis存储key时需要是ASCII（0~127），而不是byte array。如果规划百亿级存储，计划每个桶分担10个kv，那么我们只需2^30=1073741824的桶个数即可，也就是最终key的个数。 5.3 减少碎片碎片主要原因在于内存无法对齐、过期删除后，内存无法重新分配。通过上文描述的方式，我们可以将人口标签和mapping数据按照上面的方式去存储，这样的好处就是redis key是等长的。另外对于hashmap中的key我们也做了相关优化，截取cookie或者deviceid的后六位作为key，这样也可以保证内存对齐，理论上会有冲突的可能性，但在同一个桶内后缀相同的概率极低(试想id几乎是随机的字符串，随意10个由较长字符组成的id后缀相同的概率*桶样本数=发生冲突的期望值&lt;&lt;0.05,也就是说出现一个冲突样本则是极小概率事件，而且这个概率可以通过调整后缀保留长度控制期望值)。而value只存储age、gender、geo的编码，用三个字节去存储。 另外提一下，减少碎片还有个很low但是有效的方法，将slave重启，然后强制的failover切换主从，这样相当于给master整理的内存的碎片。 推荐Google-tcmalloc， facebook-jemalloc内存分配，可以在value不大时减少内存碎片和内存消耗。有人测过大value情况下反而libc更节约。 md5散列桶的方法需要注意的问题1）kv存储的量级必须事先规划好，浮动的范围大概在桶个数的十到十五倍，比如我就想存储百亿左右的kv，那么最好选择30bit~31bit作为桶的个数。也就是说业务增长在一个合理的范围（10~15倍的增长）是没问题的，如果业务太多倍数的增长，会导致hashset增长过快导致查询时间增加，甚至触发zip-list阈值，导致内存急剧上升。 2）适合短小value，如果value太大或字段太多并不适合，因为这种方式必须要求把value一次性取出，比如人口标签是非常小的编码，甚至只需要3、4个bit（位）就能装下。 3）典型的时间换空间的做法，由于我们的业务场景并不是要求在极高的qps之下，一般每天亿到十亿级别的量，所以合理利用CPU租值，也是十分经济的。 4）由于使用了信息摘要降低了key的大小以及约定长度，所以无法从redis里面random出key。如果需要导出，必须在冷数据中导出。 5）expire需要自己实现，目前的算法很简单，由于只有在写操作时才会增加消耗，所以在写操作时按照一定的比例抽样，用HLEN命中判断是否超过15个entry，超过才将过期的key删除，TTL的时间戳存储在value的前32bit中。 6）桶的消耗统计是需要做的。需要定期清理过期的key，保证redis的查询不会变慢。 测试结果人口标签和mapping的数据100亿条记录。 优化前用2.3T，碎片率在2左右；优化后500g，而单个桶的平均消耗在4左右。碎片率在1.02左右。查询时这对于cpu的耗损微乎其微。 另外需要提一下的是，每个桶的消耗实际上并不是均匀的，而是符合多项式分布的。 上面的公式可以计算桶消耗的概率分布。公式是唬人用的，只是为了提醒大家不要想当然的认为桶消耗是完全均匀的，有可能有的桶会有上百个key。但事实并不没有那么夸张。试想一下投硬币，结果只有两种正反面。相当于只有两个桶，如果你投上无限多次，每一次相当于一次伯努利实验，那么两个桶必然会十分的均匀。概率分布就像上帝施的魔咒一样，当你面对大量的桶进行很多的广义的伯努利实验。桶的消耗分布就会趋于一种稳定的值。接下来我们就了解一下桶消耗分布具体什么情况： 通过采样统计31bit（20多亿）的桶，平均4.18消耗 桶消耗 占比 1 14.9% 2 17.7% 3 15.7% 4 13.9% 5 11.4% 6 9.6% 7 7% 8 4.8% 9 3.2% 10 1.7% 11 0.8% 12 &lt;0.2% 100亿节约了1.8T内存。相当于节约了原先的78%内存，而且桶消耗指标远没有达到预计的底线值15。 对于未出现的桶也是存在一定量的，如果过多会导致规划不准确，其实数量是符合二项分布的，对于2^30桶存储2^32kv，不存在的桶大概有（百万级别，影响不大）： 1Math.pow((1 - 1.0 / Math.pow(2, 30)), Math.pow(2, 32)) * Math.pow(2, 30); 对于桶消耗不均衡的问题不必太担心，随着时间的推移，写入时会对HLEN超过15的桶进行削减，根据多项式分布的原理，当实验次数多到一定程度时，桶的分布就会趋于均匀（硬币投掷无数次，那么正反面出现次数应该是一致的），只不过我们通过expire策略削减了桶消耗，实际上对于每个桶已经经历了很多的实验发生。 总结：信息摘要在这种场景下不仅能节约key存储，对齐了内存，还能让Key按照多项式分布均匀的散列在更少量的key下面从而减少膨胀，另外无需在给key设置expire，也很大程度上节约了空间。 这也印证了时间换空间的基本理论，合理利用CPU租值也是需要考虑的。","tags":[{"name":"redis","slug":"redis","permalink":"ly2513.github.com/tags/redis/"}]},{"title":"sudo 命令情景分析","date":"2017-02-28T10:36:50.000Z","path":"2017/02/28/sudo-命令情景分析/","text":"Linux 下使用 sudo 命令，可以让普通用户也能执行一些或者全部的 root 命令。本文就对我们常用到 sudo 操作情景进行简单分析，通过一些例子来了解 sudo 命令相关的技巧。 情景一：用户无权限执行 root 命令普通用户登录 shell 之后，如果自身没有权限访问某个文件或执行某个命令时，若该用户获得root授权，那么就可以在需要执行的命令之前加上 sudo，临时切换到root用户的权限，完成相关的操作。在sudo于1980年前后被写出之前，一般用户管理系统的方式是利用su切换为超级用户。但是使用su的缺点之一在于必须要先告知超级用户的密码，而sudo使一般用户不需要知道超级用户的密码即可获得权限。那么哪些用户可以临时获得 root 权限呢？这就需要在 /etc/sudoers 文件中进行配置：授权给单个用户： 1liyong ALL=(ALL) ALL 上面这个例子中： liyong：允许使用 sudo 的用户名ALL：允许从任何终端（任何机器）使用 sudo(ALL)：允许以任何用户执行 sudo 命令ALL：允许 sudo 权限执行任何命令 如果我们想让用户 test 只能在本主机（主机名为liyong-pc）以 root 账户执行/bin/chown、/bin/chmod 两条命令，那么就应该这样配置： 1test liyong-pc=(root) /bin/chown,/bin/chmod 和授权给单个用户类似，只不过将用户名在这里换成%组名，所有在该组中的用户都按照此规则进行授权。对于该例，所有在 sudo 组内的用户都有在任何终端（第一个ALL）、以任何用户（第二个ALL）、执行任何命令（第三个ALL）的权限，查看 /etc/group 文件可以知道哪些用户属于 sudo 组。 举例: 如果当前帐号在 /etc/sudoers 文件中被授予 sudo 的权限，那么你就可以将任何 root 命令作为 sudo 命令的参数，使用 root 权限来执行该命令。举例来说，挂载一个文件系统只能由 root 来执行，但是一个普通用户也可以使用 sudo 来挂载： 首次使用会要求你输入当前用户的密码，系统确实输入正确即以 root 权限来执行 mount 命令，接下来一段时间（默认为5分钟）再次使用 sudo 命令就不需要输密码了。 1sudo mount 情景二：vim 编辑后发现忘记使用 sudo我们经常会遇到这样的一个囧境：使用 vim 对某个文件进行编辑，编辑完之后，按 ESC 之后回到普通模式，再按 :wq 准备保存退出时，发现没有权限对该文件进行修改，我们在使用 vim 命令时忘记在前面加 sudo 了。我就经常出现这种问题，之前的做法是只能不保存强退，再加上 sudo 重新编辑。 但是今后我们再也不需要用这么愚蠢的做法了，我们可以在 vim 的普通模式下，按 :w !sudo tee % ，这样就可以 root 权限来保存文件了，你也无需因为自己一时忘记加个 sudo 而沮丧懊恼了！ 情景三：执行 root 命令忘记加 sudo我们还会遇到这样稍微好一点的情形：输入一个长长的命令，按 Enter 之后出现无权限操作，因为我们忘记加 sudo 了。大多人的做法是按 ↑ 回到上一条命令，在该命令之前加上 sudo，再执行该命令。 以后，我们无需这样了，只要输入 sudo !! 即可，这里的 !! 代表上一条命令。如： 情景四：shell 内置命令如何使用 sudoshell 是一个交互式的应用程序，在执行外部命令时通过 fork 来创建一个子进程，再通过 exec 来加载外部命令的程序来执行，但是如果一个命令是 shell 内置命令，那么只能直接由 shell 来运行。sudo 的意思是，以别的用户（如root）的权限来 fork 一个进程，加载程序并运行，因此 sudo 后面不能跟 shell 的内置命令，如： 在这种情况，我们又没有 root 账户的密码，我们怎样执行该命令呢？有种办法就是使用 sudo 获得root shell 的权限，然后在root shell 中执行该命令。进入root shell 很简单，输入sudo bash 确认本用户的密码即可，此时你会发现命令提示符显示当前是 root。一旦获得root shell，你可以执行任何命令而不需要在每条命令前输入sudo了。 另外，常用的shell 内置命令在这里 有简单介绍，我们可以使用 type 命令来查看命令的类型，如： 情景五：sudo 操作记录日志 作为一个 Linux 系统的管理员，不仅可以让指定的用户或用户组作为root用户或其它用户来运行某些命令，还能将指定的用户所输入的命令和参数作详细的记录。而sudo的日志功能就可以用户跟踪用户输入的命令，这不仅能增进系统的安全性，还能用来进行故障检修。但是要记录sudo的日志还要一些简单的配置： 参考资料： sudo mannual7 Linux sudo Command Tips and Trickssudo 日志配置","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"}]},{"title":"Nginx性能调优","date":"2017-02-28T10:10:28.000Z","path":"2017/02/28/Nginx性能调优/","text":"Nginx以高性能的负载均衡器，缓存，和web服务器闻名，驱动了全球超过 40% 最繁忙的网站。在大多数场景下，默认的Nginx和Linux 设置可以很好的工作，但要达到最佳性能，有些时候必须做些调整。本文将讨论当调优系统时要考虑的一些Nginx和Linux设置。 有太多可以调优的设置，但本文只涵盖一小部分设置，这些设置对大多数使用者有优化的好处。本文不包括那些设置，那些需要必须深入理解Nginx和Linux，或者需要Nginx支持团队或专业服务团队指导才能做的设置。专业服务团队已经和很多全球热门网站共事，调优Nginx以达到最高性能，他们可以与你一起共事，充分利用Nginx或Nginx部署。 简介本文假设你已经对 Nginx 架构和配置的概念有一个基本的理解，本文不会重复 Nginx 文档，但会提供各种选项的概述和相关文档的链接。 调优的时候要遵循的一个准则：一次只改一个设置，如果对性能无有效提升，就改回默认值。 我们先从讨论Linux调优开始，因为一些操作系统配置的设置决定了如何优化Nginx的配置。 调优Linux 的配置Linux 内核（2.6以上）的设置已经适用于大多数场合，不过对一些设置的调整，会有更大的收益。如果系统配置太低，检查错误信息的内核日志则会提示建议升级。在这里我们只涉及最有可能在常规工作负载下调优很有收益的配置。调整配置的更多细节，请参考Linux文档。 缓冲区队列下面的设置与连接及其如何排队相关。如果传入连接速率很高，导致性能参次不齐（例如一些连接似乎停滞了），改变这些配置会有效。 net.core.somaxconn：排队等待连接的最大数目，由NGINX可接受的数目决定。默认值通常很低，但可以接受，因为NGINX 接收连接非常快，但如果网站流量大时，就应该增加这个值。内核日志中的错误消息会提醒这个值太小了，把值改大，直到错误提示消失。注意： 如果设置这个值大于512，相应地也要改变NGINX listen指令的backlog参数。net.core.netdev_max_backlog ： 在提交到CPU前网卡中数据包缓冲的速率，高带宽下提高这个值可提高性能。检查内核日志文件中有关这个设置的错误，根据网卡文档中的建议修改这个值。 文件描述符文件描述符是操作系统资源，用于表示连接、打开的文件，以及其他信息。NGINX 每个连接可以使用两个文件描述符。例如，如果NGINX充当代理时，通常一个文件描述符表示客户端连接，另一个连接到代理服务器，如果开启了HTTP 保持连接，这个比例会更低（译注：为什么更低呢）。对于有大量连接服务的系统，下面的设置可能需要调整一下： sys.fs.file_max —— 文件描述符系统级别的限制nofile —— 用户级别文件描述符限制，在 /etc/security/limits.conf 文件中修改。 临时端口当NGINX充当代理时，每个到上游服务器的连接都使用一个短暂或临时端口。可能需要修改这些设置： net.ipv4.ip_local_port_range —— 端口值的起止范围。如果你发现用尽端口号，可以增大端口范围。一般端口号设置是1024到65000。 调优NGINX配置以下是一些可以影响性能的NGINX指令。如上所述，我们只讨论自己能调整的指令。我们建议你在没有NGINX团队指导下，不要调整别的指令。 工作进程NGINX可以运行多个工作进程，每个都可处理大量并发连接。可以控制工作进程数，用下面的指令管理它们的连接： worker_processes —— NGINX工作进程数（默认值是1）。在大多数情况下，一个CPU内核运行一个工作进程最好，建议将这个指令设置成自动就可以。有时可能想增大这个值，比如当工作进程需要做大量的磁盘I/O。worker_connections —— 每个工作进程可以处理并发的最大连接数。默认值是512，但多数系统有充足的资源可以支撑更多的连接。合适的设置可以根据服务器的大小和流量的性质决定，可以通过测试修改。 长连接长连接对性能有很大的影响，通过减少CPU和网络开销需要开启或关闭连接。NGINX终止所有客户端连接，创建到上游服务器独立的连接。NGINx支持客户端和上游服务器两种长连接。下面是和客户端的长连接相关的指令： keepalive_requests-单个客户端长连接可以请求的数量，默认值是100，但是当使用压力测试工具从一个客户端发送多个请求测试时，这个值设更高些特别有用。keepalive_timeout—空闲长连接保持打开状态的时间。 下面是和上游服务器长连接的相关指令： keepalive –每个工作进程中空闲长连接到上游服务器保持开启的连接数量。没有默认值。 要使用连接到上游服务器的长连接，必须要配置文件中下面的指令。 12proxy_http_version 1.1;proxy_set_header Connection \"\"; 访问日志记录每个请求会消耗CPU和I/O周期，一种降低这种影响的方式是缓冲访问日志。使用缓冲，而不是每条日志记录都单独执行写操作，NGINX会缓冲一连串的日志记录，使用单个操作把它们一起写到文件中。 要启用访问日志的缓存，就涉及到在access_log指令中buffer=size这个参数。当缓冲区达到size值时，NGINX会把缓冲区的内容写到日志中。让NGINX在指定的一段时间后写缓存，就包含flush=time参数。当两个参数都设置了，当下个日志条目超出缓冲区值或者缓冲区中日志条目存留时间超过设定的时间值，NGINX都会将条目写入日志文件。当工作进程重新打开它的日志文件或退出时，也会记录下来。要完全禁用访问日志记录的功能，将access_log 指令设置成off参数。 Sendfile操作系统的sendfile()系统调用可以实现从一个文件描述符到另一个文件描述符的数据拷贝，通常实现零拷贝，这能加速TCP数据传输。要让NGINX使用它，在http或server或location环境中包含sendfile指令。NGINX可以不需要切换到用户态，就把缓存或磁盘上的内容写入套接字 ，而且写的速度非常快，消耗更少的CPU周期。注意，尽管使用sendfile()数据拷贝可以绕过用户态，这不适用于常规的NGINX处理改变内容的链和过滤器， 比如gzip。当配置环境下有sendfile指令和激活内容更改过滤器的指令时，NGINX会自动禁用sendfile。 限制你可以设置多个限制，防止用户消耗太多的资源，避免影响系统性能和用户体验及安全。 以下是相关的指令： limit_conn and limit_conn_zone—NGINX接受客户连接的数量限制，例如单个IP地址的连接。设置这些指令可以防止单个用户打开太多的连接，消耗超出自己的资源。limit_rate–传输到客户端响应速度的限制（每个打开多个连接的客户消耗更多的带宽）。设置这个限制防止系统过载，确保所有客户端更均匀的服务质量。limit_req and limit_req_zone– NGINX处理请求的速度限制，与limit_rate有相同的功能。可以提高安全性，尤其是对登录页面，通过对用户限制请求速率设置一个合理的值，避免太慢的程序覆盖你的应用请求（比如DDoS攻击)。max_conns上游配置块中服务器指令参数。在上游服务器组中单个服务器可接受最大并发数量。使用这个限制防止上游服务器过载。设置值为0（默认值）表示没有限制。queue (NGINX Plus) – 创建一个队列，用来存放在上游服务器中超出他们最大max_cons限制数量的请求。这个指令可以设置队列请求的最大值，还可以选择设置在错误返回之前最大等待时间（默认值是60秒）。如果忽略这个指令，请求不会放入队列。 缓存和压缩可以提高性能Nginx的一些额外功能可用于提高Web应用的性能，调优的时候web应用不需要关掉，但值得一提，因为它们的影响可能很重要。 它们包括缓存和压缩。 缓存一个启用Nginx缓存的情景，一组web或者应用服务器负载均衡，可以显著缩短对客户端的响应时间，同时大幅度降低后端服务器的负载。缓存本身就可以作个专题来讲，这里我们就不试图讲它了。参阅Nginx Plus管理手册的NGINX内容缓存。 压缩所以使用更小的网络带宽。然而尽管压缩数据会消耗CPU资源，但当需要减少网络带宽使用时这样做非常有效。需要注意的是，不能对已压缩的文件再压缩例如JPEG 文件。有关更多的信息，请参阅“Nginx Plus管理指南”中的压缩和解压缩。 了解更多信息，参阅以下： • Benchmarking NGINX: 4 Ways to Improve Accuracy (whitepaper)• NGINX documentation at nginx.org• NGINX and NGINX Plus Feature Matrix• NGINX Plus Technical Specifications","tags":[{"name":"nginx","slug":"nginx","permalink":"ly2513.github.com/tags/nginx/"}]},{"title":"PHP命令行下的世界","date":"2017-02-28T09:48:37.000Z","path":"2017/02/28/PHP命令行下的世界/","text":"PHP作为一门web开发语言，通常情况下我们都是在Web Server中运行PHP，使用浏览器访问，因此很少关注其命令行操作以及相关参数的使用，但是，特别是在类Unix操作系统上，PHP可以作为一门脚本语言执行与shell类似的处理任务。 PHP命令行(CLI)参数详解查看PHP的所有命令行参数，使用php -h命令。我们将会对大部分常用的命令行参数进行一一解释，以加深对PHP能力的认识，更加快捷的在服务端命令行下使用PHP或者调试各种因为对环境不熟悉而出现的问题。 -a 以交互式shell模式运行-c | 指定php.ini文件所在的目录-n 指定不使用php.ini文件-d foo[=bar] 定义一个INI实体，key为foo，value为’bar’-e 为调试和分析生成扩展信息-f 解释和执行文件. -h 打印帮助-i 显示PHP的基本信息-l 进行语法检查 (lint)-m 显示编译到内核的模块-r 运行PHP代码，不需要使用标签 ..?&gt;-B 在处理输入之前先执行PHP代码-R 对输入的没一行作为PHP代码运行-F Parse and execute for every input line-E Run PHP after processing all input lines-H Hide any passed arguments from external tools.-S : 运行内建的web服务器.-t 指定用于内建web服务器的文档根目录-s 输出HTML语法高亮的源码-v 输出PHP的版本号-w 输出去掉注释和空格的源码-z 载入Zend扩展文件 . args… 传递给要运行的脚本的参数. 当第一个参数以-开始或者是脚本是从标准输入读取的时候，使用–参数 –ini 显示PHP的配置文件名 –rf 显示关于函数 的信息.–rc 显示关于类 的信息.–re 显示关于扩展 的信息.–rz 显示关于Zend扩展 的信息.–ri 显示扩展 的配置信息. 上面列出了PHP命令所有的参数及其注释，接下来，我们将对其中比较常用的参数举例说明。 以交互式shell模式运行php 用过 Python 的朋友对Python的交互式shell比较熟悉，在命令行下，如果我们直接输入python命令，则会进入python的交互式shell程序，接下来就可以交互式的执行一些计算任务。 在PHP命令行中，同样提供了类似的功能，使用-a参数即可进入交互shell模式。 在该shell中，我们可以执行一些简单的任务，而不需要总是新建一个php文件。 更详细的使用说明，请参考官方文档 运行内建的Web服务器 从PHP 5.4.0开始，PHP的命令行模式提供了一个内建的web服务器。使用-S开始运行web服务。 假设当前我们处在目录/usr/local/var/www/php/demo，在该目录中，存在index.php文件。 1234$ lsindex.php$ cat index.php&lt;?php echo \"Hello, PHPER!\"; 在该目录中，执行以下命令可以启动内建web服务器，并且默认以当前目录为工作目录 12345$ php -S localhost:8000PHP 5.6.3 Development Server started at Wed Jun 10 15:49:41 2015Listening on http://localhost:8000Document root is /Users/mylxsw/codes/php/aicode/demoPress Ctrl-C to quit. 我们另外开启一个shell窗口，请求http://localhost:8000/即可看到脚本输出 123456789$ curl -is http://localhost:8000/HTTP/1.1 200 OKHost: localhost:8000Connection: closeX-Powered-By: PHP/5.6.3Content-type: text/html;Hello, PHPER! 在web服务运行的窗口，可以看到输出的日志信息 以上我们在启动内建服务器的时候，只指定了-S参数让PHP以web服务器的方式运行，这时，PHP会使用当前目录作为工作目录，因此回到当前目录下寻找请求的文件，我们还可以使用-t参数指定其它的目录作为工作目录（文档根目录）。 更多详细信息，请参考官方文档。 查找PHP的配置文件 在有的时候，由于服务器上软件安装比较混乱，我们可能安装了多个版本的PHP环境，这时候，如何定位我们的PHP程序使用的是那个配置文件就比较重要了。在PHP命令行参数中，提供了–ini参数，使用该参数，可以列出当前PHP的配置文件信息。 1234567891011$ php --iniConfiguration File (php.ini) Path: /usr/local/etc/php/5.6Loaded Configuration File: /usr/local/etc/php/5.6/php.iniScan for additional .ini files in: /usr/local/etc/php/5.6/conf.dAdditional .ini files parsed: (none)$ /usr/local/php/bin/php --iniConfiguration File (php.ini) Path: /usr/local/php/etc/Loaded Configuration File: /usr/local/php/etc/php.iniScan for additional .ini files in: (none)Additional .ini files parsed: (none) 上述的服务器上我们安装了两个版本的PHP，由上可以看到，使用php –ini命令可以很方便的定位当前PHP命令将会采用哪个配置文件。 查看类/函数/扩展信息 通常，我们可以使用php –info命令或者在在web服务器上的php程序中使用函数phpinfo()显示php的信息，然后再查找相关类、扩展或者函数的信息，这样做实在是麻烦了一些。 1234$ php --info | grep redisredisRegistered save handlers =&gt; files user redisThis program is free software; you can redistribute it and/or modify 我们可以使用下列参数更加方便的查看这些信息 –rf 显示关于函数 的信息.–rc 显示关于类 的信息.–re 显示关于扩展 的信息.–rz 显示关于Zend扩展 的信息.–ri 显示扩展 的配置信息. 例如，我们希望查看扩展redis的配置信息 123456$ php --ri redisredisRedis Support =&gt; enabledRedis Version =&gt; 2.2.7 查看redis类的信息 1234567891011$ php --rc redisClass [ class Redis ] &#123; - Constants [19] &#123; Constant [ integer REDIS_NOT_FOUND ] &#123; 0 &#125; ... - Methods [201] &#123; ... Method [ public method echo ] &#123; &#125; ... 查看函数printf的信息 12345678$ php --rf printfFunction [ function printf ] &#123; - Parameters [2] &#123; Parameter #0 [ $format ] Parameter #1 [ ...$args ] &#125;&#125; 语法检查 有时候，我们只需要检查php脚本是否存在语法错误，而不需要执行它，比如在一些编辑器或者IDE中检查PHP文件是否存在语法错误。 使用-l（–syntax-check）可以只对PHP文件进行语法检查。 12$ php -l index.phpNo syntax errors detected in index.php 假如此时我们的index.php中存在语法错误 12345$ php -l index.phpPHP Parse error: syntax error, unexpected 'echo' (T_ECHO) in index.php on line 3Parse error: syntax error, unexpected 'echo' (T_ECHO) in index.php on line 3Errors parsing index.php 命令行脚本开发 在使用PHP开发命令行脚本的时候，与开发web程序是明显不同的，在web程序中，我们可以通过改变url的参数，为PHP环境提供不同的输入，但是在命令行脚本程序中如何获取外部的输入呢？ 在使用C语言开发程序时，我们通常会在main函数中提供两个可选的参数int main(int argc, char *argv[])，这两个参数就是从命令行提供的输入参数。在PHP中，提供了两个全局变量$argc和$argv用于获取命令行输入。 $argc 包含了 $argv数组包含元素的数目$argv 是一个数组，包含了提供的参数，第一个参数总是脚本文件名称假设我们有一个名为console.php的命令行脚本文件 123456&lt;?phpecho '命令行参数个数: ' . $argc . \"n\";echo \"命令行参数:n\";foreach ($argv as $index =&gt; $arg) &#123; echo \" &#123;$index&#125; : &#123;$arg&#125;n\";&#125; 在命令行下执行该脚本 123456$ php console.php hello world命令行参数个数: 3命令行参数: 0 : console.php 1 : hello 2 : world 可以看到，第0个参数是我们执行的脚本名称。需要注意的是，如果提供的第一个参数是以-开头的话，需要在前面增加–，以告诉php这后面的参数是提供给我们的脚本的，而不是php执行文件的（php -r ‘var_dump($argv);’ — -h）。 另外，在脚本中，我们可以通过php_sapi_name()函数判断是否是在命令行下运行的 12$ php -r 'echo php_sapi_name(), PHP_EOL;'cli","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"}]},{"title":"最全的常用正则表达式大全","date":"2017-02-28T09:10:19.000Z","path":"2017/02/28/最全的常用正则表达式大全/","text":"很多不太懂正则的朋友，在遇到需要用正则校验数据时，往往是在网上去找很久，结果找来的还是不很符合要求。所以我最近把开发中常用的一些正则表达式整理了一下，包括校验数字、字符、一些特殊的需求等等。给自己留个底，也给朋友们做个参考。 一、校验数字的表达式 数字：^[0-9]*$ n位的数字：^\\d{n}$ 至少n位的数字：^\\d{n,}$ m-n位的数字：^\\d{m,n}$ 零和非零开头的数字：^(0|[1-9][0-9]*)$ 非零开头的最多带两位小数的数字：^([1-9][0-9]*)+(.[0-9]{1,2})?$ 带1-2位小数的正数或负数：^(-)?\\d+(.\\d{1,2})?$ 正数、负数、和小数：^(-|+)?\\d+(.\\d+)?$ 有两位小数的正实数：^[0-9]+(.[0-9]{2})?$ 有1~3位小数的正实数：^[0-9]+(.[0-9]{1,3})?$ 非零的正整数：^[1-9]\\d$ 或 ^([1-9][0-9]){1,3}$ 或 ^+?[1-9][0-9]*$ 非零的负整数：^-[1-9][]0-9″$ 或 ^-[1-9]\\d$ 非负整数：^\\d+$ 或 ^[1-9]\\d*|0$ 非正整数：^-[1-9]\\d*|0$ 或 ^((-\\d+)|(0+))$ 非负浮点数：^\\d+(.\\d+)?$ 或 ^[1-9]\\d.\\d|0.\\d[1-9]\\d|0?.0+|0$ 非正浮点数：^((-\\d+(.\\d+)?)|(0+(.0+)?))$ 或 ^(-([1-9]\\d.\\d|0.\\d[1-9]\\d))|0?.0+|0$ 正浮点数：^[1-9]\\d.\\d|0.\\d[1-9]\\d$ 或 ^(([0-9]+.[0-9][1-9][0-9])|([0-9][1-9][0-9].[0-9]+)|([0-9][1-9][0-9]))$ 负浮点数：^-([1-9]\\d.\\d|0.\\d[1-9]\\d)$ 或 ^(-(([0-9]+.[0-9][1-9][0-9])|([0-9][1-9][0-9].[0-9]+)|([0-9][1-9][0-9])))$ 浮点数：^(-?\\d+)(.\\d+)?$ 或 ^-?([1-9]\\d.\\d|0.\\d[1-9]\\d|0?.0+|0)$ 二、校验字符的表达式 汉字：^[\\u4e00-\\u9fa5]{0,}$ 英文和数字：^[A-Za-z0-9]+$ 或 ^[A-Za-z0-9]{4,40}$ 长度为3-20的所有字符：^.{3,20}$ 由26个英文字母组成的字符串：^[A-Za-z]+$ 由26个大写英文字母组成的字符串：^[A-Z]+$ 由26个小写英文字母组成的字符串：^[a-z]+$ 由数字和26个英文字母组成的字符串：^[A-Za-z0-9]+$ 由数字、26个英文字母或者下划线组成的字符串：^\\w+$ 或 ^\\w{3,20}$ 中文、英文、数字包括下划线：^[\\u4E00-\\u9FA5A-Za-z0-9_]+$ 中文、英文、数字但不包括下划线等符号：^[\\u4E00-\\u9FA5A-Za-z0-9]+$ 或 ^[\\u4E00-\\u9FA5A-Za-z0-9]{2,20}$ 可以输入含有^%&amp;’,;=?$\\”等字符：[^%&amp;’,;=?$\\x22]+ 禁止输入含有~的字符：[^~\\x22]+ 三、特殊需求表达式 Email地址：^\\w+([-+.]\\w+)@\\w+([-.]\\w+).\\w+([-.]\\w+)*$ 域名：[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(/.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+/.? InternetURL：[a-zA-z]+://[^\\s] 或 ^http://([\\w-]+\\.)+[\\w-]+(/[\\w-./?%&amp;=])?$ 手机号码：^(13[0-9]|14[5|7]|15[0|1|2|3|5|6|7|8|9]|18[0|1|2|3|5|6|7|8|9])\\d{8}$ 电话号码(“XXX-XXXXXXX”、”XXXX-XXXXXXXX”、”XXX-XXXXXXX”、”XXX-XXXXXXXX”、”XXXXXXX”和”XXXXXXXX)：^($$\\d{3,4}-)|\\d{3.4}-)?\\d{7,8}$ 国内电话号码(0511-4405222、021-87888822)：\\d{3}-\\d{8}|\\d{4}-\\d{7} 身份证号(15位、18位数字)：^\\d{15}|\\d{18}$ 短身份证号码(数字、字母x结尾)：^([0-9]){7,18}(x|X)?$ 或 ^\\d{8,18}|[0-9x]{8,18}|[0-9X]{8,18}?$ 帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]{4,15}$ 密码(以字母开头，长度在6~18之间，只能包含字母、数字和下划线)：^[a-zA-Z]\\w{5,17}$ 强密码(必须包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间)：^(?=.\\d)(?=.[a-z])(?=.*[A-Z]).{8,10}$ 日期格式：^\\d{4}-\\d{1,2}-\\d{1,2} 一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$ 一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$钱的输入格式： 有四种钱的表示形式我们可以接受:”10000.00″ 和 “10,000.00″, 和没有 “分” 的 “10000″ 和 “10,000″：^[1-9][0-9]*$ 这表示任意一个不以0开头的数字，但是，这也意味着一个字符”0″不通过，所以我们采用下面的形式：^(0|[1-9][0-9]*)$ 一个0或者一个不以0开头的数字.我们还可以允许开头有一个负号：^(0|-?[1-9][0-9]*)$ 这表示一个0或者一个可能为负的开头不为0的数字.让用户以0开头好了.把负号的也去掉，因为钱总不能是负的吧.下面我们要加的是说明可能的小数部分：^[0-9]+(.[0-9]+)?$ 必须说明的是，小数点后面至少应该有1位数，所以”10.”是不通过的，但是 “10″ 和 “10.2″ 是通过的：^[0-9]+(.[0-9]{2})?$ 这样我们规定小数点后面必须有两位，如果你认为太苛刻了，可以这样：^[0-9]+(.[0-9]{1,2})?$ 这样就允许用户只写一位小数。下面我们该考虑数字中的逗号了，我们可以这样：^[0-9]{1,3}(,[0-9]{3})*(.[0-9]{1,2})?$ 1到3个数字，后面跟着任意个 逗号+3个数字，逗号成为可选，而不是必须：^([0-9]+|[0-9]{1,3}(,[0-9]{3})*)(.[0-9]{1,2})?$ 备注：这就是最终结果了，别忘了”+”可以用”*”替代。如果你觉得空字符串也可以接受的话(奇怪，为什么?)最后，别忘了在用函数时去掉去掉那个反斜杠，一般的错误都在这里 xml文件：^([a-zA-Z]+-?)+[a-zA-Z0-9]+\\.[x|X][m|M][l|L]$ 中文字符的正则表达式：[\\u4e00-\\u9fa5] 双字节字符：[^\\x00-\\xff] (包括汉字在内，可以用来计算字符串的长度(一个双字节字符长度计2，ASCII字符计1)) 空白行的正则表达式：\\n\\s*\\r (可以用来删除空白行) HTML标记的正则表达式：&lt;(\\S?)[^&gt;]&gt;.?&lt;/\\1&gt;|&lt;.? /&gt; (网上流传的版本太糟糕，上面这个也仅仅能部分，对于复杂的嵌套标记依旧无能为力) 首尾空白字符的正则表达式：^\\s|\\s$或(^\\s)|(\\s$) (可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式) 腾讯QQ号：[1-9][0-9]{4,} (腾讯QQ号从10000开始) 中国邮政编码：[1-9]\\d{5}(?!\\d) (中国邮政编码为6位数字) IP地址：\\d+.\\d+.\\d+.\\d+ (提取IP地址时有用) IP地址：((?:(?:25[0-5]|2[0-4]\\d|[01]?\\d?\\d)\\.){3}(?:25[0-5]|2[0-4]\\d|[01]?\\d?\\d)) (由@飞龙三少 提供，感谢共享)","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"正则","slug":"正则","permalink":"ly2513.github.com/tags/正则/"}]},{"title":"每天掌握一个Linux命令(10): cat命令","date":"2017-02-28T08:53:06.000Z","path":"2017/02/28/每天掌握一个Linux命令-10-cat命令/","text":"cat命令的用途是连接文件或标准输入并打印。这个命令常用来显示文件内容，或者将几个文件连接起来显示，或者从标准输入读取内容并显示，它常与重定向符号配合使用。 1.命令格式 cat [选项] [文件]… 2.命令功能 cat主要有三大功能： 1.一次显示整个文件:cat filename2.从键盘创建一个文件:cat &gt; filename 只能创建新文件,不能编辑已有文件.3.将几个文件合并为一个文件:cat file1 file2 &gt; file 3.命令参数 -A, –show-all 等价于 -vET-b, –number-nonblank 对非空输出行编号-e 等价于 -vE-E, –show-ends 在每行结束处显示 $-n, –number 对输出的所有行编号,由1开始对所有输出的行数编号-s, –squeeze-blank 有连续两行以上的空白行，就代换为一行的空白行-t 与 -vT 等价-T, –show-tabs 将跳格字符显示为 ^I-u (被忽略)-v, –show-nonprinting 使用 ^ 和 M- 引用，除了 LFD 和 TAB 之外 4.使用实例 例一：把 log2012.log 的文件内容加上行号后输入 log2013.log 这个文件里命令：cat -n log2012.log log2013.log 例二：把 log2012.log 和 log2013.log 的文件内容加上行号（空白行不加）之后将内容附加到 log.log 里。命令：cat -b log2012.log log2013.log log.log 输出：12345678[root@localhost test]# cat -b log2012.log log2013.log log.log 1 2012-01 2 2012-02 3 ====== 4 2013-01 5 2013-02 6 2013-03 7 ======[root@localhost test]# 例三：把 log2012.log 的文件内容加上行号后输入 log.log 这个文件里命令：cat -n log2012.log &gt; log.log 输出: 12345678[root@localhost test]# cat -n log2012.log &gt; log.log[root@localhost test]# cat -n log.log 1 2012-01 2 2012-02 3 4 5 ======[root@localhost test]# 例四：使用here doc来生成文件输出： 1234567891011121314[root@localhost test]# cat &gt;log.txt &lt;&lt;EOF&gt; Hello&gt; World&gt; Linux&gt; PWD=$(pwd)&gt; EOF[root@localhost test]# ls -l log.txt-rw-r--r-- 1 root root 37 10-28 17:07 log.txt[root@localhost test]# cat log.txtHelloWorldLinuxPWD=/opt/soft/test[root@localhost test]# 说明： 注意粗体部分，here doc可以进行字符串替换。 备注：tac (反向列示) 命令：tac log.txt 输出： 12345[root@localhost test]# tac log.txtPWD=/opt/soft/testLinuxWorldHello 说明： tac 是将 cat 反写过来，所以他的功能就跟 cat 相反， cat 是由第一行到最后一行连续显示在萤幕上，而 tac 则是由最后一行到第一行反向在萤幕上显示出来！","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"cat","slug":"cat","permalink":"ly2513.github.com/tags/cat/"}]},{"title":"每天掌握一个Linux命令(9): touch命令","date":"2017-02-28T08:38:42.000Z","path":"2017/02/28/每天掌握一个Linux命令-9-touch命令/","text":"linux的touch命令不常用，一般在使用make的时候可能会用到，用来修改文件时间戳，或者新建一个不存在的文件。 1.命令格式 命令 touch [选项]… 文件… 2.命令参数 -a 或–time=atime或–time=access或–time=use 只更改存取时间。-c 或–no-create 不建立任何文档。-d 使用指定的日期时间，而非现在的时间。-f 此参数将忽略不予处理，仅负责解决BSD版本touch指令的兼容性问题。-m 或–time=mtime或–time=modify 只更改变动时间。-r 把指定文档或目录的日期时间，统统设成和参考文档或目录的日期时间相同。-t 使用指定的日期时间，而非现在的时间。 3.命令功能 touch命令参数可更改文档或目录的日期时间，包括存取时间和更改时间。 4.使用范例 例一：创建不存在的文件命令：touch log2012.log log2013.log 输出： 1234567[root@localhost test]# touch log2012.log log2013.log[root@localhost test]# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log 如果log2014.log不存在，则不创建文件 命令: touch -c log2014.log 1234567[root@localhost test]# touch -c log2014.log[root@localhost test]# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log 例二：更新log.log的时间和log2012.log时间戳相同命令：touch -r log.log log2012.log 输出： 1234567891011121314151617[root@localhost test]# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log[root@localhost test]# touch -r log.log log2012.log[root@localhost test]# ll-rw-r--r-- 1 root root 0 10-28 14:48 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log 例三：设定文件的时间戳命令：touch -t 201211142234.50 log.log 说明： -t time 使用指定的时间值 time 作为指定文件相应时间戳记的新值．此处的 time规定为如下形式的十进制数: [[CC]YY]MMDDhhmm[.SS] 这里，CC为年数中的前两位，即”世纪数”；YY为年数的后两位，即某世纪中的年数．如果不给出CC的值，则touch 将把年数CCYY限定在1969–2068之内．MM为月数，DD为天将把年数CCYY限定在1969–2068之内．MM为月数，DD为天数，hh 为小时数(几点)，mm为分钟数，SS为秒数．此处秒的设定范围是0–61，这样可以处理闰秒．这些数字组成的时间是环境变量TZ指定的时区中的一个时 间．由于系统的限制，早于1970年1月1日的时间是错误的","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"touch","slug":"touch","permalink":"ly2513.github.com/tags/touch/"}]},{"title":"每天掌握一个Linux命令(8): cp命令","date":"2017-02-28T08:29:02.000Z","path":"2017/02/28/每天掌握一个Linux命令-8-cp命令/","text":"cp命令用来复制文件或者目录，是Linux系统中最常用的命令之一。一般情下，shell会设置一个别名，在命令行下复制文件时，如果目标文件已经存在，就会询问是否覆盖，不管你是否使用-i参数。但是如果是在shell脚本中执行cp时，没有-i参数时不会询问是否覆盖。这说明命令行和shell脚本的执行方式有些不同。 1.命令格式 命令： cp [选项]… [-T] 源 目的或：cp [选项]… 源… 目录或：cp [选项]… -t 目录 源… 2.命令功能 将源文件复制至目标文件，或将多个源文件复制至目标目录。 3.命令参数 -a, –archive 等于-dR –preserve=all–backup[=CONTROL 为每个已存在的目标文件创建备份-b 类似–backup 但不接受参数–copy-contents 在递归处理是复制特殊文件内容-d 等于–no-dereference –preserve=links-f, –force 如果目标文件无法打开则将其移除并重试(当 -n 选项存在时则不需再选此项)-i, –interactive 覆盖前询问(使前面的 -n 选项失效)-H 跟随源文件中的命令行符号链接-l, –link 链接文件而不复制-L, –dereference 总是跟随符号链接-n, –no-clobber 不要覆盖已存在的文件(使前面的 -i 选项失效)-P, –no-dereference 不跟随源文件中的符号链接-p 等于–preserve=模式,所有权,时间戳–preserve[=属性列表 保持指定的属性(默认：模式,所有权,时间戳)，如果可能保持附加属性：环境、链接、xattr 等-R, -r, –recursive 复制目录及目录内的所有项目 4.命令实例 例一：复制单个文件到目标目录，文件在目标文件中不存在命令：cp log.log test5 说明： 在没有带-a参数时，两个文件的时间是不一样的。在带了-a参数时，两个文件的时间是一致的。 例二：目标文件存在时，会询问是否覆盖命令：cp log.log test5 说明： 目标文件存在时，会询问是否覆盖。这是因为cp是cp -i的别名。目标文件存在时，即使加了-f标志，也还会询问是否覆盖。 例三：复制整个目录,将整个test2目录下的文件复制到test3命令：cp -r test2 test3 说明： 注意目标目录存在与否结果是不一样的。目标目录存在时，整个源目录被复制到目标目录里面。 例四：复制的 log.log 建立一个连结档 log_link.log命令：cp -s log.log log_link.log 说明： 那个 log_link.log 是由 -s 的参数造成的，建立的是一个『快捷方式』，所以您会看到在文件的最右边，会显示这个文件是『连结』到哪里去的！","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"cp","slug":"cp","permalink":"ly2513.github.com/tags/cp/"}]},{"title":"每天掌握一个Linux命令(7): mv命令","date":"2017-02-28T08:18:42.000Z","path":"2017/02/28/每天掌握一个Linux命令-7-mv命令/","text":"mv命令是move的缩写，可以用来移动文件或者将文件改名（move (rename) files），是Linux系统下常用的命令，经常用来备份文件或者目录。 1.命令格式 mv [选项] 源文件或目录 目标文件或目录 2.命令功能 视mv命令中第二个参数类型的不同（是目标文件还是目标目录），mv命令将文件重命名或将其移至一个新的目录中。当第二个参数类型是文件时，mv命令完成文件重命名，此时，源文件只能有一个（也可以是源目录名），它将所给的源文件或目录重命名为给定的目标文件名。当第二个参数是已存在的目录名称时，源文件或目录参数可以有多个，mv命令将各参数指定的源文件均移至目标目录中。在跨文件系统移动文件时，mv先拷贝，再将原有文件删除，而链至该文件的链接也将丢失。 3.命令参数 -b ：若需覆盖文件，则覆盖前先行备份。-f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖；-i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！-u ：若目标文件已经存在，且 source 比较新，才会更新(update)-t ： –target-directory=DIRECTORY move all SOURCE arguments into DIRECTORY，即指定mv的目标目录，该选项适用于移动多个源文件到一个目录的情况，此时目标目录在前，源文件在后。 4.命令实例 例一：文件改名,将文件test.log重命名为test1.txt命令：mv test.log test1.txt 例二：移动文件,将test1.txt文件移到目录test3中命令：mv test1.txt test3 例三：将文件log1.txt,log2.txt,log3.txt移动到目录test3中。命令： mv log1.txt log2.txt log3.txt test3 mv -t /opt/soft/test/test4/ log1.txt log2.txt log3.txt 说明： mv log1.txt log2.txt log3.txt test3 命令将log1.txt ，log2.txt， log3.txt 三个文件移到 test3目录中去，mv -t /opt/soft/test/test4/ log1.txt log2.txt log3.txt 命令又将三个文件移动到test4目录中去 例四：将文件file1改名为file2，如果file2已经存在，则询问是否覆盖命令：mv -i log1.txt log2.txt 例五：将文件log3.txt改名为log2.txt，即使log2.txt存在，也是直接覆盖掉。命令：mv -f log3.txt log2.txt 例六：目录的移动,如果目录dir2不存在，将目录dir1改名为dir2；否则，将dir1移动到dir2中。命令：mv dir1 dir2 例七：移动当前文件夹下的所有文件到上一级目录命令：mv * ../ 例八：把当前目录的一个子目录里的文件移动到另一个子目录里命令：mv test3/*.txt test5 例九：文件被覆盖前做简单备份，前面加参数-b命令：mv log1.txt -b log2.txt 输出： 12345678910111213[root@localhost test5]# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-25 17:56 test5-1[root@localhost test5]# mv log1.txt -b log2.txtmv：是否覆盖“log2.txt”? y[root@localhost test5]# ll-rw-r--r-- 1 root root 25 10-28 07:02 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt~-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-25 17:56 test5-1[root@localhost test5]# 说明： -b 不接受参数，mv会去读取环境变量VERSION_CONTROL来作为备份策略。–backup该选项指定如果目标文件存在时的动作，共有四种备份策略： 1.CONTROL=none或off : 不备份。 2.CONTROL=numbered或t：数字编号的备份 3.CONTROL=existing或nil：如果存在以数字编号的备份，则继续编号备份m+1…n： 执行mv操作前已存在以数字编号的文件log2.txt.~1~，那么再次执行将产生log2.txt~2~，以次类推。如果之前没有以数字编号的文件，则使用下面讲到的简单备份。 4.CONTROL=simple或never：使用简单备份：在被覆盖前进行了简单备份，简单备份只能有一份，再次被覆盖时，简单备份也会被覆盖。","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"mv","slug":"mv","permalink":"ly2513.github.com/tags/mv/"}]},{"title":"每天掌握一个Linux命令(6): rmdir命令","date":"2017-02-27T02:00:03.000Z","path":"2017/02/27/每天掌握一个Linux命令-6-rmdir命令/","text":"今天学习一下linux中命令： rmdir命令。rmdir是常用的命令，该命令的功能是删除空目录，一个目录被删除之前必须是空的。（注意，rm – r dir命令可代替rmdir，但是有很大危险性。）删除某目录时也必须具有对父目录的写权限。 1.命令格式 rmdir [选项]… 目录… 2.命令功能 该命令从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对父目录的写权限。 3.命令参数 -p 递归删除目录dirname，当子目录删除后其父目录为空时，也一同被删除。 如果整个路径被删除或者由于某种原因保留部分路径，则系统在标准输出上显示相应的信息。 -v, –verbose 显示指令执行过程 4命令实例 例一：rmdir 不能删除非空目录命令：rmdir doc 输出：1234567891011121314151617181920212223242526272829303132[root@localhost scf]# tree.|-- bin|-- doc| |-- info| `-- product|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product12 directories, 0 files[root@localhost scf]# rmdir docrmdir: doc: 目录非空[root@localhost scf]# rmdir doc/info[root@localhost scf]# rmdir doc/product[root@localhost scf]# tree.|-- bin|-- doc|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product10 directories, 0 files 说明： rmdir 目录名 命令不能直接删除非空目录 例二：rmdir -p 当子目录被删除后使它也成为空目录的话，则顺便一并删除命令：rmdir -p logs 输出：12345678910111213141516171819202122232425262728293031323334353637[root@localhost scf]# tree.|-- bin|-- doc|-- lib|-- logs| `-- product`-- service `-- deploy |-- info `-- product10 directories, 0 files[root@localhost scf]# rmdir -p logsrmdir: logs: 目录非空[root@localhost scf]# tree.|-- bin|-- doc|-- lib|-- logs| `-- product`-- service `-- deploy |-- info `-- product9 directories, 0 files[root@localhost scf]# rmdir -p logs/product[root@localhost scf]# tree.|-- bin|-- doc|-- lib`-- service`-- deploy |-- info `-- product7 directories, 0 files – 原文链接","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"rmdir","slug":"rmdir","permalink":"ly2513.github.com/tags/rmdir/"}]},{"title":"每天掌握一个Linux命令(5): rm命令","date":"2017-02-26T14:42:49.000Z","path":"2017/02/26/每天掌握一个Linux命令-5-rm命令/","text":"昨天学习了创建文件和目录的命令mkdir ，今天学习一下linux中删除文件和目录的命令： rm命令。rm是常用的命令，该命令的功能为删除一个目录中的一个或多个文件或目录，它也可以将某个目录及其下的所有文件及子目录均删除。对于链接文件，只是删除了链接，原有文件均保持不变。 rm是一个危险的命令，使用的时候要特别当心，尤其对于新手，否则整个系统就会毁在这个命令（比如在/（根目录）下执行rm * -rf）。所以，我们在执行rm之前最好先确认一下在哪个目录，到底要删除什么东西，操作时保持高度清醒的头脑。 1.命令格式 rm [选项] 文件… 2.命令功能 删除一个目录中的一个或多个文件或目录，如果没有使用- r选项，则rm不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。 3.命令参数： -f, –force 忽略不存在的文件，从不给出提示。 -i, –interactive 进行交互式删除 -r, -R, –recursive 指示rm将参数中列出的全部目录和子目录均递归地删除。 -v, –verbose 详细显示进行的步骤 –help 显示此帮助信息并退出 –version 输出版本信息并退出 4.命令实例： 例一：删除文件file，系统会先询问是否删除。命令：rm 文件名 输入rm log.log命令后，系统会询问是否删除，输入y后就会删除文件，不想删除则数据n。 例二：强行删除file，系统不再提示。命令：rm -f log1.log 例三：删除任何.log文件；删除前逐一询问确认命令：rm -i *.log 例四：将 test1子目录及子目录中所有档案删除命令：rm -r test1 例五：rm -rf test2命令会将 test2 子目录及子目录中所有档案删除,并且不用一一确认命令：rm -rf test2 例六：删除以 -f 开头的文件命令：rm – -f 例七：自定义回收站功能命令：myrm(){ D=/tmp/$(date +%Y%m%d%H%M%S); mkdir -p $D; mv “$@” $D &amp;&amp; echo “moved to $D ok”; } 输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@localhost test]# myrm()&#123; D=/tmp/$(date +%Y%m%d%H%M%S); mkdir -p $D; mv \"$@\" $D &amp;&amp; echo \"moved to $D ok\"; &#125;[root@localhost test]# alias rm='myrm'[root@localhost test]# touch 1.log 2.log 3.log[root@localhost test]# ll总计 16-rw-r--r-- 1 root root 0 10-26 15:08 1.log-rw-r--r-- 1 root root 0 10-26 15:08 2.log-rw-r--r-- 1 root root 0 10-26 15:08 3.logdrwxr-xr-x 7 root root 4096 10-25 18:07 scfdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5[root@localhost test]# rm [123].logmoved to /tmp/20121026150901 ok[root@localhost test]# ll总计 16drwxr-xr-x 7 root root 4096 10-25 18:07 scfdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5[root@localhost test]# ls /tmp/20121026150901/1.log 2.log 3.log[root@localhost test]# 说明： 上面的操作过程模拟了回收站的效果，即删除文件的时候只是把文件放到一个临时目录中，这样在需要的时候还可以恢复过来。 – 原文链接","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"rm","slug":"rm","permalink":"ly2513.github.com/tags/rm/"}]},{"title":"每天掌握一个Linux命令(4):mkdir命令","date":"2017-02-26T14:27:59.000Z","path":"2017/02/26/每天掌握一个Linux命令-4-mkdir命令/","text":"linux mkdir 命令用来创建指定的名称的目录，要求创建目录的用户在当前目录中具有写权限，并且指定的目录名不能是当前目录中已有的目录。 1.命令格式 mkdir [选项] 目录… 2.命令功能 通过 mkdir 命令可以实现在指定位置创建以 DirName(指定的文件名)命名的文件夹或目录。要创建文件夹或目录的用户必须对所创建的文件夹的父文件夹具有写权限。并且，所创建的文件夹(目录)不能与其父目录(即父文件夹)中的文件名重名，即同一个目录下不能有同名的(区分大小写)。 3.命令参数 -m, –mode=模式，设定权限&lt;模式&gt; (类似 chmod)，而不是 rwxrwxrwx 减 umask -p, –parents 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后,系统将自动建立好那些尚不存在的目录,即一次可以建立多个目录; -v, –verbose 每次创建新目录都显示信息 –help 显示此帮助信息并退出 –version 输出版本信息并退出 4.命令实例 例一：创建一个空目录命令：mkdir test1 例二：递归创建多个目录命令：mkdir -p test2/test22 实例三：创建权限为777的目录命令：mkdir -m 777 test3 例四：创建新目录都显示信息命令：mkdir -v test4 例五：一个命令创建项目的目录结构命令：mkdir -vp scf/{lib/,bin/,doc/{info,product},logs/{info,product},service/deploy/{info,product}} 输出： 1234567891011121314[root@localhost test]# mkdir -vp scf/&#123;lib/,bin/,doc/&#123;info,product&#125;,logs/&#123;info,product&#125;,service/deploy/&#123;info,product&#125;&#125;mkdir: 已创建目录 “scf”mkdir: 已创建目录 “scf/lib”mkdir: 已创建目录 “scf/bin”mkdir: 已创建目录 “scf/doc”mkdir: 已创建目录 “scf/doc/info”mkdir: 已创建目录 “scf/doc/product”mkdir: 已创建目录 “scf/logs”mkdir: 已创建目录 “scf/logs/info”mkdir: 已创建目录 “scf/logs/product”mkdir: 已创建目录 “scf/service”mkdir: 已创建目录 “scf/service/deploy”mkdir: 已创建目录 “scf/service/deploy/info”mkdir: 已创建目录 “scf/service/deploy/product” – 原文链接","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"mkdir","slug":"mkdir","permalink":"ly2513.github.com/tags/mkdir/"}]},{"title":"每天掌握一个Linux命令(3):pwd命令","date":"2017-02-26T14:17:08.000Z","path":"2017/02/26/每天掌握一个Linux命令-3-pwd命令/","text":"Linux中用 pwd 命令来查看”当前工作目录“的完整路径。 简单得说，每当你在终端进行操作时，你都会有一个当前工作目录。 在不太确定当前位置时，就会使用pwd来判定当前目录在文件系统内的确切位置。 1.命令格式 pwd [选项] 2.命令功能 查看”当前工作目录“的完整路径 3.常用参数 一般情况下不带任何参数 如果目录是链接时： -P 显示出实际路径，而非使用连接（link）路径。 4.常用实例 例一：用 pwd 命令查看默认工作目录的完整路径命令：pwd 例三：目录连接链接时，pwd -P 显示出实际路径，而非使用连接（link）路径；pwd显示的是连接路径命令：pwd -P 例四：/bin/pwd命令：/bin/pwd [选项] 选项： -L 目录连接链接时，输出连接路径-P 输出物理路径 例五：当前目录被删除了，而pwd命令仍然显示那个目录– 原文链接","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"pwd","slug":"pwd","permalink":"ly2513.github.com/tags/pwd/"}]},{"title":"每天掌握一个Linux命令(2): cd命令","date":"2017-02-26T14:03:12.000Z","path":"2017/02/26/每天掌握一个Linux命令-2-cd命令/","text":"Linux cd 命令可以说是Linux中最基本的命令语句，其他的命令语句要进行操作，都是建立在使用 cd 命令上的。所以，学习Linux 常用命令，首先就要学好 cd 命令的使用方法技巧。 1.命令格式 cd [目录名] 2.命令功能 切换当前目录至dirName 3.常用范例 例一：进入系统根目录命令：cd / 例二：使用 cd 命令进入当前用户主目录“当前用户主目录”和“系统根目录”是两个不同的概念。进入当前用户主目录有两个方法。 命令： cd cd ~ 例三：跳转到指定目录命令：cd /opt/soft 例四：返回进入此目录之前所在的目录命令：cd – 例五：把上个命令的参数作为cd参数使用。命令：cd !$ – 原文链接","tags":[{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"},{"name":"cd","slug":"cd","permalink":"ly2513.github.com/tags/cd/"}]},{"title":"RAP接口文档导入到postman","date":"2017-02-22T07:49:08.000Z","path":"2017/02/22/RAP接口文档导入到postman/","text":"我司采用阿里开源的RAP系统管理API文档。但实际中我们后端为了调试所写的API是否正确,常用Google提供的postman(Paw、fireFox的httpRequester)等这类工具模拟浏览器发送http请求所写API。之前是手工一个一个的写API地址,这样的做法非常耗时,为了提高开发效率,就想能否用Rap导出的API json文件生成相应的postman的相应的请求的API目录,这样一来就不需要手工的一个一个写API地址了。为了实现,我将导出的RapAPI的json文件进行研究发现,这文件里的API很有规律。于是写了一段PHP代码对这文件进行处理,最后生成postman想要的格式的json文件。以下就是将rap的接口文档导出的API生成的json文件处理后成为postman的json文件的代码, php代码如下： 12345678910111213141516171819202122232425262728293031323334$data = file_get_contents('rap备份文件路径');$data = json_decode($data,TRUE);$data['modelJSON'] = str_replace(\"\\'\",'\\\"',$data['modelJSON']);$data = json_decode($data['modelJSON'],TRUE);$postMan = [];$postMan['info']['name'] = $data['name'];$postMan['info']['_postman_id'] = md5(microtime(TRUE));$postMan['info']['schema'] = \"https://schema.getpostman.com/json/collection/v2.0.0/collection.json\";$postMan['item'] = [];foreach ($data['moduleList'] as &amp;$module) &#123; foreach ($module['pageList'] as &amp;$page) &#123; $p = []; $p['name'] = $page['name']; foreach ($page['actionList'] as &amp;$action) &#123; $a = []; $a['name'] = $action['name']; $a['request']['url'] = '&#123;&#123;url&#125;&#125;/'.trim($action['requestUrl'],'/'); $a['request']['method'] = ($action['requestType']==1) ? 'GET' : 'POST'; $a['request']['header']['token'] = '&#123;&#123;token&#125;&#125;'; $a['request']['body']['mode'] = 'urlencoded'; foreach ($action['requestParameterList'] as &amp;$requestParameter) &#123; $r['key'] = $requestParameter['identifier']; $r['value'] = \"\"; $r['type'] = \"text\"; $r['enabled'] = TRUE; $a['request']['body']['urlencoded'][] = $r; &#125; $p['item'][] = $a; &#125; $postMan['item'][] = $p; &#125;&#125;file_put_contents(\"输出目录/postman.json\", json_encode($postMan));#echo json_encode($postMan);die; 处理后得到的postman的json文件,然后导入到postman。这样就减少了之前一个一个API手写的功夫了。 如对你有用,尽管拿去,不用谢。。。。。。","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"postman","slug":"postman","permalink":"ly2513.github.com/tags/postman/"},{"name":"rap","slug":"rap","permalink":"ly2513.github.com/tags/rap/"}]},{"title":"每日掌握一个Linux命令(1): ls命令","date":"2017-02-21T14:32:14.000Z","path":"2017/02/21/每天掌握一个Linux命令-1-ls命令/","text":"ls命令是linux系统中常用的命令之一。ls命令是list缩写,该命令用于列出当前目录下所有的文件夹及文件.如果ls指定某个目录那么就会显示指定目录下的文件夹及文件。通过ls命令不仅可以查看linux下文件夹所包含的文件及文件夹而且可以查看文件的权限(包括目、文件夹、文件权限)查看目录信息等。 1.命令格式 ls [选项] [目录名] 2.命令功能 列出当前目录的所有文件夹及文件 3.常用参数 -a, -all 列出目录下的所有文件,包括以.开头的隐藏文件及目录 -A同-a,但不列出”.”(表示当前目录)和”..”(表示当前目录的父目录) -c 配合 -lt: 根据ctime排序及显示ctime(文件状态最后更改的时间)配合 -l:显示ctime但根据名称排序否则:根据ctime排序及显示 ctime (文件状态最后更改的时间)配合 -l：显示 ctime 但根据名称排序否则：根据 ctime 排序 -C 每栏由上至下列出项目 –color[=WHEN] 控制是否使用色彩分辨文件。WHEN 可以是’never’、’always’或’auto’其中之一 -d, –directory 将目录象文件一样显示，而不是显示其下的文件。 -D, –dired 产生适合 Emacs 的 dired 模式使用的结果 -f 对输出的文件不进行排序，-aU 选项生效，-lst 选项失效 -g 类似 -l,但不列出所有者 -G, –no-group 不列出任何有关组的信息 -h, –human-readable 以容易理解的格式列出文件大小 (例如 1K 234M 2G) -t 以文件修改时间排序 -u 配合 -lt:显示访问时间而且依访问时间排序 配合 -l:显示访问时间但根据名称排序否则：根据访问时间排序 -U 不进行排序;依文件系统原有的次序列出项目 -v 根据版本进行排序 -w, –width=COLS 自行指定屏幕宽度而不使用目前的数值 -x 逐行列出项目而不是逐栏列出 -X 根据扩展名排序 -1 每行只列出一个文件 –help 显示此帮助信息并离开 –version 显示版本信息并离开 –si 类似 -h,但文件大小取 1000 的次方而不是 1024 -H, –dereference-command-line 使用命令列中的符号链接指示的真正目的地–indicator-style=方式 指定在每个项目名称后加上指示符号&lt;方式&gt;：none (默认)，classify (-F)，file-type (-p) -i, –inode 印出每个文件的 inode 号 -I, –ignore=样式 不印出任何符合 shell 万用字符&lt;样式&gt;的项目 -k 即 –block-size=1K,以 k 字节的形式表示文件的大小。 -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来。 -L, –dereference 当显示符号链接的文件信息时，显示符号链接所指示的对象而并非符号链接本身的信息 -m 所有项目以逗号分隔，并填满整行行宽 -o 类似 -l,显示文件的除组信息外的详细信息。 -r, –reverse 依相反次序排列 -R, –recursive 同时列出所有子目录层 -s, –size 以块大小为单位列出所有文件的大小 -S 根据文件大小排序 4.常用范例 例一：列出/home/liyong文件夹下的所有文件和目录的详细资料命令：ls -l -R /home/liyong 例二：列出当前目录中所有以“t”开头的目录的详细内容，可以使用如下命令：命令：ls -l t* 例三：只列出文件下的子目录命令：ls -F /opt/soft |grep /$ 例四：列出目前工作目录下所有名称是s 开头的档案，愈新的排愈后面，可以使用如下命令：命令：ls -ltr s* 例五：列出目前工作目录下所有档案及目录;目录于名称后加”/”, 可执行档于名称后加”*”命令：ls -AF 例六：计算当前目录下的文件数和目录数命令： ls -l |grep “^-“|wc -l —文件个数ls -l |grep “^d”|wc -l —目录个数 例七: 在ls中列出文件的绝对路径命令：ls | sed “s:^:pwd/:” 例九：列出当前目录下的所有文件（包括隐藏文件）的绝对路径， 对目录不做递归命令：find $PWD -maxdepth 1 | xargs ls -ld 例十：递归列出当前目录下的所有文件（包括隐藏文件）的绝对路径命令：find $PWD | xargs ls -ld 例十一：指定文件时间输出格式命令：ls -tl –time-style=full-iso – 原文链接","tags":[{"name":"shell","slug":"shell","permalink":"ly2513.github.com/tags/shell/"},{"name":"linux","slug":"linux","permalink":"ly2513.github.com/tags/linux/"}]},{"title":"mac常用的命令汇总","date":"2016-12-28T15:04:30.000Z","path":"2016/12/28/mac常用的命令汇总/","text":"Mac 终端命令大全目录操作 命令名 功能描述 使用举例 mkdir 创建一个目录 mkdir dirname rmdir 删除一个目录 rmdir dirname mvdir 移动或重命名一个目录 mvdir dir1 dir2 cd 改变当前目录 cd dirname pwd 显示当前目录的路径名 pwd ls 显示当前目录的内容 ls -la dircmp 比较两个目录的内容 dircmp dir1 dir2 文件操作 命令名 功能描述 使用举例 cat 显示或连接文件 cat filename pg 分页格式化显示文件内容 pg filename more 分屏显示文件内容 more filename od 显示非文本文件的内容 od -c filename cp 复制文件或目录 cp file1 file2 rm 删除文件或目录 rm filename mv 改变文件名或所在目录 mv file1 file2 ln 联接文件 ln -s file1 file2 find 使用匹配表达式查找文件 find . -name “*.c” -print file 显示文件类型 file filename open 使用默认的程序打开文件 open filename 选择操作 命令名 功能描述 使用举例 head 显示文件的最初几行 head -20 filename tail 显示文件的最后几行 tail -15 filename cut 显示文件每行中的某些域 cut -f1,7 -d: /etc/passwd colrm 从标准输入中删除若干列 colrm 8 20 file2 paste 横向连接文件 paste file1 file2 diff 比较并显示两个文件的差异 diff file1 file2 sed 非交互方式流编辑器 sed “s/red/green/g” filename grep 在文件中按模式查找 grep “^[a-zA-Z]” filename awk 在文件中查找并处理模式 awk ‘{print $1 $1}’ filename sort 排序或归并文件 sort -d -f -u file1 uniq 去掉文件中的重复行 uniq file1 file2 comm 显示两有序文件的公共和非公共行 comm file1 file2 wc 统计文件的字符数、词数和行数 wc filename nl 给文件加上行号 nl file1 &gt;file2 安全操作 命令名 功能描述 使用举例 passwd 修改用户密码 passwd chmod 改变文件或目录的权限 chmod ug+x filename umask 定义创建文件的权限掩码 umask 027 chown 改变文件或目录的属主 chown newowner filename chgrp 改变文件或目录的所属组 chgrp staff filename xlock 给终端上锁 xlock -remote 编程操作 命令名 功能描述 使用举例 make 维护可执行程序的最新版本 make touch 更新文件的访问和修改时间 touch -m 05202400 filename dbx 命令行界面调试工具 dbx a.out xde 图形用户界面调试工具 xde a.out 进程操作 命令名 功能描述 使用举例 ps 显示进程当前状态 ps u kill 终止进程 kill -9 30142 nice 改变待执行命令的优先级 nice cc -c *.c renice 改变已运行进程的优先级 renice +20 32768 时间操作 命令名 功能描述 使用举例 date 显示系统的当前日期和时间 date cal 显示日历 cal 8 1996 time 统计程序的执行时间 time a.out 网络与通信操作 命令名 功能描述 使用举例 telnet 远程登录 telnet hpc.sp.net.edu.cn rlogin 远程登录 rlogin hostname -l username rsh 在远程主机执行指定命令 rsh f01n03 date ftp 在本地主机与远程主机之间传输文件 ftp ftp.sp.net.edu.cn rcp 在本地主机与远程主机 之间复制文件 rcp file1 host1:file2 ping 给一个网络主机发送 回应请求 ping hpc.sp.net.edu.cn mail 阅读和发送电子邮件 mail write 给另一用户发送报文 write username pts/1 mesg 允许或拒绝接收报文 mesg n Korn Shell 命令 命令名 功能描述 使用举例 history 列出最近执行过的 几条命令及编号 history r 重复执行最近执行过的 某条命令 r -2 alias 给某个命令定义别名 alias del=rm -i unalias 取消对某个别名的定义 unalias del 其它命令 命令名 功能描述 使用举例 uname 显示操作系统的有关信息 uname -a clear 清除屏幕或窗口内容 clear env 显示当前所有设置过的环境变量 env who 列出当前登录的所有用户 who whoami 显示当前正进行操作的用户名 whoami tty 显示终端或伪终端的名称 tty stty 显示或重置控制键定义 stty -a du 查询磁盘使用情况 du -k subdir df 显示文件系统的总空间和可用空间 df /tmp w 显示当前系统活动的总信息 w","tags":[{"name":"mac","slug":"mac","permalink":"ly2513.github.com/tags/mac/"},{"name":"shell","slug":"shell","permalink":"ly2513.github.com/tags/shell/"}]},{"title":"mac包管理工具 -- brew ","date":"2016-12-28T03:45:31.000Z","path":"2016/12/28/mac包管理工具-brew/","text":"熟悉centOS的同学很清楚yum、rpm这类工具,他们都是centOS下的包管理工具,熟悉Ubuntu的同学也对apt很了解,这就不多说了。使用mac的同学就会问了,mac有类似的的工具吗?它在mac下是怎么安装的,该工具又是怎样使用的呢?接下来就这几个问题进行”报道”。 问题一、它是谁了答案: 它就是brew,它的全称是Homebrew 问题二、它怎么安装了答案: 查看你的mac有没有安装ruby,没装的的话,自己google啊,在此就不介绍了,如果安装使用如下命令 1ruby -e \"$(curl -fsSL https://raw.github.com/mxcl/homebrew/go)\" 由于Homebrew的安装地址可能变化，请到官方网站查看最新的安装方法。 问题三、它怎么使用了 其基本使用方法如下(以mysql为例) 查找软件包 1brew search mysql 安装软件包 1brew install mysql 删除软件包 1brew remove mysql 查看软件包信息 1brew info mysql 列出软件包的依赖关系 1brew deps mysql 列出已安装的软件包 1brew list 更新brew 1brew update 列出所有安装的软件包里可以升级的那些 1brew outdated 升级全部的软件包 1brew upgrade 升级某个软件包 1brew upgrade mysql 清理不需要的版本极其 1brew cleanup 你还可以定制自己的软件包如果自己需要的软件包并不能在Homebrew中找到，怎么办呢，毕竟Homebrew是一个新生项目，不可能满足所有人的需求。当然，我们可以自行编译安装，但手工安装的软件包游离于Homebrew之外，管理起来不是很方便。 前文说过，Homebrew使用Ruby实现的软件包配置非常方便，下面简单谈一谈软件包的定制（假定软件包名称是bar，来自foo站点）。 首先找到待安装软件的源码下载地址 1http://foo.com/bar-1.0.tgz 建立自己的formula 1brew create http://foo.com/bar-1.0.tgz 编辑formula，上一步建立成功后，Homebrew会自动打开新建的formula进行编辑，也可用如下命令打开formula进行编辑。 1brew edit bar Homebrew自动建立的formula已经包含了基本的configure和make install命令，对于大部分软件，不需要进行修改，退出编辑即可。 输入以下命令安装自定义的软件包1brew install bar 以上就是brew常用的命令了,关于Homebrew的其它功能，比如将自定义软件包提交到官方发布等，请参考Homebrew项目的主页及其Man Page。你将发现Homebrew不仅是“家酿”，更是“佳酿”。","tags":[{"name":"mac","slug":"mac","permalink":"ly2513.github.com/tags/mac/"},{"name":"brew","slug":"brew","permalink":"ly2513.github.com/tags/brew/"}]},{"title":"mac上那些好用的tools","date":"2016-12-27T14:32:37.000Z","path":"2016/12/27/mac上那些好用的tools/","text":"Awesome Mac  这个仓库主要是收集非常好用的Mac应用程序、软件以及工具，主要面向开发者和设计师。有这个想法是因为我最近发了一篇较为火爆的涨粉儿微信公众号文章《工具武装的前端开发工程师》，于是建了这么一个仓库，持续更新作为补充，搜集更多好用的软件工具。请Star、Pull Request或者使劲搓它 issues 给我推荐优秀好用的Mac应用，很显然我是一个资深Mac用户，我需要它们帮助我快乐、高效的工作，同时也分享给你。格式参照awesome的清单。 在这里非常感谢 @GitHubDaily 的推荐 说明 表示 开源软件 ，点击进入 开源 仓库； 表示 免费 使用，或者个人 免费 ； 表示 热门 的软件； 表示 推荐 的软件； 表示 装机必备 的软件； 表示 App store 连接地址; 表示 强烈推荐 的必装神器，数量来表达强烈的程度； 目录 编辑器 开发者工具 测试工具 命令行工具 版本控制 数据库 设计和产品 虚拟机 通信 数据恢复 音频和视频 书签阅读写作 FTP客户端 软件打包工具 制作电子书 下载工具 网盘 输入法 浏览器 翻译工具 科学上网 其它实用工具 远程协助 第三方应用市场APP Mac软件下载网站 编辑器一种用于编辑纯文本文件的程序，建议使用免费开源的编辑器 Atom - GitHub推出的开源编辑器，Atom常用插件。 Sublime Text - 一个比较简洁大方带插件管理系统的流行编辑器，Sublime常用插件。 Brackets - Adobe推出的Brackets免费/开源编辑器。 Visual Studio Code - 微软推出的免费/开源编辑器，TypeScript支持杠杠的。 Emacs - Emacs是基于控制台的编辑器和高度可定制的。 LightTable - 下一代代码编辑器。 TextMate - 文本编辑器软件，与BBedit一起并称苹果机上的emacs和vim，这是以前。 BBEdit - 强大的文件编辑器，用于编辑文件，文本文件及程序源代码。 Coda2 - 编写Web应用长得漂亮的编辑器。 Vim - Vim古老命令行中使用的编辑器。 Vundle.vim - Vim插件管理工具。 vim-plug - 一个简约的vim插件管理器。 WebVim - 倾向于开发JavaScript和Web的vim。 vim-web - 一个前端开发工程师的vim。 Neovim - 提高Vim可扩张灵活性。 Spacevim - 模仿Spacemacs的使用方式。 Spf13 - 一套全方位的配置项目。 Vimr - Vim客户端，升级Vim体验。 HBuilder - HBuilder是DCloud（数字天堂）推出的一款支持HTML5的Web开发IDE。 Tincta - 一个免费的文本编辑器。 IntelliJ IDEA - 一款Java开发集成环境。(学生免费) Webstorm - 是jetbrains公司旗下一款JavaScript开发工具。 学生免费，点击这里 查看更多。 一些很好的插件 NodeJS - 集成Node.js，你肯定需要它，很多功能需要它。 EditorConfig - 帮助开发者在不同的编辑器和IDE之间定义和维护一致的代码风格。 Material Theme UI - Google为React开发的主题。 Deco IDE - React Native IDE 支持控件拖拽界面实时变更。 Xamarin Studio - 免费的跨平台的C# IDE。支持iOS、Android和.net开发。 NetBeans IDE - 免费、开源的IDE，主要用于java开发，可支持多种语言和框架。 Eclipse - 流行的开源IDE，主要用于Java，但为多种语言和平台的插件支持。 开发者工具 WeFlow - 一个基于 tmt-workflow 前端工作流的开发工具。 Koala - 预处理器语言图形编译工具，支持Less、Sass、CoffeeScript、Compass framework 的即时编译。 CodeKit - 自动编译Less、Sass、Stylus、CoffeeScript、Jade &amp; Haml等文件。 Hosts.prefpane - 编辑 hosts 文件的工具。 iHosts - 唯一上架 Mac App Store 的 /etc/hosts 编辑神器。 SwitchHosts - 一个管理、切换多个 hosts 方案的工具。 Gas Mask - 编辑 hosts 文件的工具，更简单方便。 DiffMerge - 可视化的文件比较（也可进行目录比较）与合并工具。 Beyond Compare - 对比两个文件夹或者文件，并将差异以颜色标示。 Kaleidoscope - 一款很强大的文本文件和图像比较工具，同时和 git、svn 等版本控制工具能够完美的结合。 Fanvas - 把swf转为HTML5 canvas动画的系统。 EnvPane - 图形终端查看环境变量的应用工具。 Dash - 强大到你无法想象的API离线文档软件。 Dlite - 简单的使用Docker的一个软件。 SnippetsLab - 管理和组织你的代码片段。 StarUML - 强大的软件建模软件。 Vagrant Manager - 管理你本地服务。 zeplin - 前端与设计协同工作专用工具。 Go2Shell - 从Finder打开命令行。 SecureCRT - 一款支持SSH、Telnet等多种协议的终端仿真程序。 MJML - 简化设计回应电子邮件的方式。 TeXstudio - 集成创建LaTeX文档的写作环境。 Vagrant - 用来构建虚拟开发环境的工具。 正则编辑器 Patterns - 正则表达式编辑器。 Reginald - 正则表达式测试应用程序，使用 RegexKitLite。 Regex - 感觉是用过最漂亮的正则表达式测试工具。 Reggy - 正则表达式编辑器。 RegExRX - 正则表达式的开发工具。 测试工具 Charles - 一个代理工具，允许你查看所有的HTTP和HTTPS流量。 Insomnia 3.0 - 漂亮的HTTP请求测试工具。 Cocoa Rest Client - 比Postman看起来漂亮的客户端，测试HTTP/REST endpoints。 Paw - 先进的 HTTP 客户端。 Cellist - HTTP调试客户端。 Integrity - 轻松找到无效链接。 Postman - Postman 帮助我们快速测试API。 命令行工具 iTerm2 - 免费的终端工具，直接替代自带的Terminal，有非常多惊人的特性。 cool-retro-term - 怀旧的命令行终端。 Oh my zsh - 拥有大量的有用的功能，助手，插件，主题，等特性的命令行工具插件。 Glances - 在命令行中查看你系统运行状态的工具。 Cakebrew - Homebrew 的客户端软件。摆脱命令方便安装、查看、卸载软件。 ndm - 查看本地NPM安装的包客户端软件。摆脱命令方便安装、查看、卸载软件。 Black Screen - 来自 21 世纪的强大终端。 Fish Shell - 智能且用户友好的命令行终端。 oh-my-fish - 和Oh my zsh相似，建立在Fish Shell框架上。 bash-it - 一个社区的bash的框架。 color-retro-term - 一款复古风格的终端，非常酷炫。 HyperTerm - 一款基于Node开发的终端软件，逼格很高。 itunes-remote - 通过终端控制您的iTunes。 mycli - 为MySQL命令行客户端，提供语法高亮和提示功能的工具！ LNav - 日志文件阅读器. tmux 终端登录远程主机的命令行工具。 mas - 一个简单的命令行界面的苹果应用商店。 版本控制 Git - 版本控制工具，官网提供数十种GUI客户端 for Mac。 SVN - 版本控制工具。 GUI Gitbar - 开源，在你的菜单栏上显示GitHub贡献统计。 GitHub Desktop - 使用GitHub的GUI应用。 Oh My Star - GitHub的star功能弱，比如分类，本地组织你的GitHub star! GitUp - 一个简单功能强大的git客户端。 Hub - 将GitHub接口和git命令进行包装。 SourceTree - 强大的Git跨平台客户端。 Tower2 - 最强大的Git客户端。 Versions - Mac上最好的SVN管理工具。 Cornerstone - Mac上最佳的SVN管理工具。 版本控制系统 GitLab - 一个用于仓库管理系统的开源项目。 GitHub GitHub代码托管，项目管理，演示部署，瞧，您现在就访问GitHub。 Coding.net - 代码托管，项目管理，WebIDE，演示部署，开启云端开发模式，让开发更简单。 phabricator phabricator支持Git、SVN、HG 基于 PHP + Mysql 的开放源代码软件开发平台。 Gogs - 一款极易搭建的自助 Git 服务Golang版本。 Gerrit Gerrit 是一个免费、开放源代码的代码审查软件，使用网页界面。 Gitblit Java版本Git代码托管，项目管理。 数据库 Sequel Pro - 一个MySQL数据库管理软件。 MySQL Workbench - MySQL数据库官方管理软件。 Postico - 现代PostgreSQL客户端，漂亮功能多。 ElectroCRUD - MySQL数据库CRUD应用程序。 Base 2 - 一个用于管理SQLite数据库的软件。 Postgres.app - Mac上最简单的方法的使用PostgreSQL关系型数据库管理系统。 PSequel - PostgreSQL数据库GUI软件。 Robomongo - 基于Shell的MongoDB图形化客户端管理软件。 MongoBooster - MongoDB图形化管理软件，内嵌MongoShell，ES6语法，流畅查询及智能感知。 Mongo Management Studio - MongoDB图形化客户端管理软件。 MongoChef - MongoDB图形化客户端管理软件。 Chrome MySQL Admin - 一个Chrome插件，是MySQL开发的跨平台、可视化数据库工具。 JackDB - 直接的SQL访问你所有的数据，无论在哪里。 medis - 漂亮的Redis管理软件。 RedisDesktopManager - Redis跨平台的GUI管理工具。 MDB Explorer - Mac上查看编辑Access数据库的工具。 Datum - SQLite - Sqlite3数据库管理软件。 mongoDB.app - 在Mac 上最简单的使用MongoDB DataGrip - 是jetbrains公司旗下一款数据库管理工具。点击这里 学生免费。 设计和产品设计工具 Acorn - 一个像PS，全面的功能集的图像编辑器。 Affinity Designer - 矢量图像设计工具，可能的Adobe Illustrator的替代。 Affinity Photo - 光栅图像设计工具，可以替代Adobe PS图象处理软件。 Blender - 全功能可扩展的跨平台3D内容套件。 Pixelmator - 强大的图像编辑器，可能PS图像处理软件的选择。 Sketch - 混合矢量/位图布局应用，特别适用于用户界面，Web和移动设计。 Sketch Toolbox - 一个超级简单的Sketch插件管理器。 inklet - 将Mac上的触摸板变成绘图板。 Monodraw - 一款文本图像编辑器。 SketchBook - 出众的绘图软件。 Tayasui Sketches - 专业的绘图软件。 Nik Collection - 专业照片后期制作工具Google收购后免费。 Paintbrush - 位图图像编辑器。 Krita - 一个开源的位图形编辑软件，包含一个绘画程式和照片编辑器。 Vectr - 免费图形编辑器。这是一个简单而强大的Web和桌面跨平台工具，把你的设计变成现实。 Principle - 使用它很容易设计动画和交互式用户界面。 MagicaVoxel - 轻量级的8位像素编辑和交互路径追踪渲染器。 ScreenToLayers - 轻松导出桌面分层文件PSD文件。 原型流程 Justinmind - 功能更丰富团队协作方便。 MockFlow - 用于网页设计和可用性测试的在线原型设计套件。 Axure RP 8 - 画原型图工具，团队协作SVN方便好用。 Mockplus - 更快更简单的原型设计工具。 OmniGraffle - 可用来绘制图表、流程图、组织结构图、思维导图以及插图或原型。 XMind - 一款实用的思维导图软件。 Scapple - 一款实用的思维导图软件。 Framer - 做交互原型的工具。 Balsamiq Mockups - 一个快速的网页设计原型工具，帮助你更快、更聪明的工作。 Marvel - 简单设计，原型设计和协作。 MindNode - 简洁的风格与人性化的操作，绘制思维脑图。 其它工具 Notes - 简洁的笔记应用。 TinyPNG4Mac - 图片压缩专用开源工具。 Image2icon - 将你的图片转换成图标。 ImageAlpha - 压缩PNG图片，去掉无效的透明。 ImageOptim - 压缩图片，删除EXIF信息。 Sip - 收集，整理和分享你的颜色拾色器。 Frank DeLoupe - 支持 Retina 的屏幕拾色器。 ColorSchemer - 专业的配色、调色软件。 IconKit - App图标自动生成器。 Licecap - 是一款屏幕录制工具输出GIF，录制过程中可以随意改变录屏范围。 Kap - 轻量 GIF 录屏小工具。 GIPHY Capture - 免费软件的捕捉和分享图片在桌面上。 Skitch - 截图附带强大的标注功能。 截图(Jietu) - 截图附带强大的标注功能，腾讯作品。 Snip - 高效的截图工具，支持滚动截屏，腾讯作品。 iPic - 上传插图链接分享。 Iconjar - 图标管理软件，带组织和搜索功能。 RightFont - 字体管理工具。 Solarized - 干净清爽的颜色主题，支持iTerm、Intellij IDEA、Vim等。 虚拟机 Parallels Desktop - 虽然好用但是收费机制，更新花钱、花钱、花钱。 Virtual Box - 免费、免费、免费，带NTFS读写，不用买ParagonNTFS，省100块。 VMWare Fusion - 强大的虚拟机，商业软件。 Veertu - Mac上最轻便的虚拟机，只有跑Windows才会收费。比较新，很多功能不支持。 通信推荐一些通信工具，沟通，团队协同。 Franz - 一个使用 Electron开发的，可以同时登录23个平台的即时通讯软件。 QQ - QQ for Mac App。 WeChat - 微信 for Mac App。 Electronic WeChat - 调用微信接口，使用 Electron 开发的第三方漂亮开源微信应用。 Skype - Skype共享、跨平台的短信和电话。 WeiboX - 微博第三方Mac应用。 御飯 - 饭否第三方Mac应用。 ChitChat - WhatsApp非官方。 Telegram - 通讯新时代。 Messenger - Facebook第三方聊天工具。 Adium - 呃这个是老的集成多个平台的聊天客户端。 Textual - 最受欢迎的世界与我们相关的KPI应用 for OS X。 Gitter - 关于GitHub的项目交流，支持 Markdown，对开发者极为友好。 简聊 - 企业级即时沟通工具，已经下线了，可以自己搭建一套系统玩儿。 钉钉 - 企业级办公通讯免费平台。 Slack - 团队协作，沟通工具。 零信 - 随时随地工作，跨平台。 今目标 - 一款面向中小企业的互联网工作平台。 BearyChat - 互联网团队协作，沟通工具。 Bitpost - 私人分散消息。它是一个p2p的去中心化和无须第三方提供信用担保协议。 Teambition - 团队协作。提供管理任务、安排日程、查找文件、即时讨论等团队所需要的一切协作功能。 日事清 - 个人日程管理，团队协作工具。日程安排，计划分配，笔记总结等。 Coding.net - 代码托管，项目管理，WebIDE，演示部署，开启云端开发模式，让开发更简单。 WeeChat - 一个命令行聊天客户端。 Email Spark - 新推出的快速邮件客户端支持Mac和iPhone。 Airmail - 快速的邮件客户端支持Mac和iPhone。 Foxmail - 快速的邮件客户端。 MailTags - 管理和组织邮件，日程和标签进行分类邮件。 N1 - 可以扩展的开源收费邮件客户端。 Postbox - 这个貌似也非常强大哦，关键是简洁漂亮的收费邮件客户端。 Polymail - 简单，功能强大，长得好看的新晋邮件客户端。 CloudMagic Email - 界面非常简洁的一个邮件客户端。 ThunderBird - Mozilla 公司出品的强大的Email客户端程序。 数据恢复 DiskWarrior - 恢复文件系统损坏时，磁盘工具进行选择。 Data Rescue - 多种情况下的全面和专业的数据恢复。 Stellar Phoenix Mac Data Recovery - 一个功能强大的恢复文件面向Mac的工具。 R-Studio for Mac - 可恢复分区被格式化、损坏或被删除的文件。 音频和视频 Kodi - 一款一流的免费开源媒体中心软件，可用于播放视频、音乐，查看图片，玩游戏等. MPV - 一个免费、开源和跨平台的媒体播放器。 VOX Player - 免费全能音乐播放器，撸码之余听听歌是一种享受。 Radiant Player - Google Play音乐播放器。 Sonora - 一个很小的音乐播放器。 Audacity - 免费开源的编辑音频的软件。 Audio Hijack - 一个记录任何应用程序的音频，包括网络电话Skype，网络流从Safari，以及更多。 Stringed 2 - 音频编辑处理工具。 Mixxx - 免费的DJ软件，给你一切你需要的表演组合，名副其实的替代Traktor。 Cog - 一个免费的开源音频播放器。 VLC - 开源的跨平台多媒体播放器及框架，可播放大多数多媒体文件。 XLD - 解码/解码/转换/播放各种“无损”音频文件。 HandBrake - 高性能的视频编码和转换工具，具有很好的图形用户界面。 MPlayerX - 媒体播放器。 ScreenFlow - 屏幕和视频编辑软件。 ArcTime - 跨平台字幕制作软件。 Perian - 让QuickTime播放所有常见格式的免费插件。 书签阅读写作 OpenOffice - 是一套跨平台的办公室软件套件。 Spillo - 功能强大，美观、快速网络书签网页阅读。 iChm - 读chm文件的软件。 Chmox - 读chm文件的软件。 CHM Reader - 读chm文件的软件。 Kindle App - 亚马逊 Kindle App 电子书阅读器。 RSS Feeds 2 - 监控任何RSS。 ReadKit - 书签RSS管理客户端。 Reeder 3 - RSS 服务订阅。 Leaf - RSS 客户端程序。 Vienna - RSS/Atom 新闻阅读客户端。 Markdown Mou - 免费 Markdown 编辑神器。 Marp - Markdown 制作幻灯片编辑器。 TextNut - Markdown编辑器，富文本之间自由切换。 MWeb - 专业的 Markdown 写作、记笔记、静态博客生成软件。 Typora - 基于 Electron 的“读写一体” Markdown 编辑器。 MacDown - 一款开源的Markdown编辑器，深受Mou的影响。 EME - 最近刚出的一款Markdown编辑器，界面很像Chrome浏览器的界面，很简约。 LightPaper - 简单的Markdown文本编辑器。 Cmd Markdown - Cmd Markdown 编辑阅读器，支持实时同步预览，区分写作和阅读模式，支持在线存储，分享文稿网址。 笔记 Quiver - 程序猿的笔记本。 有道云笔记 - 支持多目录，Markdown，iWork/Office预览。 为知笔记 - 支持Markdown，搜集整理图片链接导入文档。 leanote - 支持Markdown的一款开源笔记软件，支持直接成为个人博客。 Inkdrop - Markdown爱好者的笔记本应用程序。 制作电子书 Calibre - 丑陋的软件，但强大的软件电子书管理和转换。 Sigil - 多平台EPUB编辑器 FTP客户端 Transmit - 一个FTP客户端，支持FTP + SFTP + S3。 Flow - 支持简单的 FTP + SFTP 客户端。 Yummy FTP - 专业快速，可靠的FTP客户端。 Cyberduck - 免费FTP，SFTP，S3和WebDAV客户端 &amp; OpenStack Swift Client。 FileZilla - 跨平台的FTP，FTPS和SFTP客户端。 软件打包工具 Finicky - Web应用程序转化为苹果的应用程序。 nw.js - 使用HTML和JavaScript来制作桌面应用。 Electron - 前身是 AtomShell，使用 JS、HTML 和CSS 构建跨平台的桌面应用程序。 react-desktop - 为 macOS Sierra带来React UI组件。 React Native Desktop for Mac - 用 React Native 技术构建 OS X 下的桌面应用程序。 React Native Desktop for Ubuntu - 用 React Native 技术构建 Ubuntu 下的桌面应用程序。 AppJS - 使用 JS、HTML 和CSS 构建跨平台的桌面应用程序。 HEX - 使用 JS、HTML 和CSS 构建跨平台的桌面应用程序，有道出品。 AlloyDesktop - 同上，腾讯出品，给个差评。 MacGap - 桌面WebKit打包HTML、CSS、JS应用。 ionic - 一个用来开发混合手机应用的，开源的，免费的代码库。 下载工具 Transmission - 免费的BitTorrent客户端 aria2 - 一款支持多种协议的轻量级命令行下载工具。 JDownloader - 下载工具，下载文件的一键式托管。 You-Get - 网络富媒体命令行下载工具。 Free Download Manager - 功能强大的下载加速器。 FOLX - Folx 是一个Mac osx 系统风格界面的下载管理工具。 网盘推荐一些有Mac客户端的网盘。 Dropbox - 非常好用的免费网络文件同步工具，提供在线存储服务。 百度云 - 百度云客户端。 腾讯微云 - 腾讯云客户端。 坚果云 - 坚果云客户端。 115 - 115云客户端。 360 - 360云客户端。 快盘 - 金山快盘。 owncloud - 私有云网盘。 Mega - 免费的云服务，提供50GB的免费存储空间。 亿方云 - 硅谷团队打造，个人免费。 Seafile - 是由国内团队开发的国际化的开源云存储软件项目。 输入法 QQ输入法 - 腾讯出品的输入法。 搜狗输入法 - 搜狗输入法。 百度输入法 - 支持拼音五笔输入。 清歌五笔输入法 - 为 iOS 和 Mac 专门打造的五笔输入法。 WBIM - 五笔输入法。 Rocket - Emoji标签输入。 颜文字 - 颜文字输入。 RIME - 中州韻輸入法引擎。 哈利路亚英文输入法 - 智能英文输入法，具备自动补全，自动纠错功能。 浏览器这里放Mac的浏览器应用 Safari - Mac预装自带浏览器。 Chrome - Chrome浏览器谷歌出品。 Firefox - 火狐浏览器。 Opera - Opera 浏览器。 QQ浏览器 - QQ浏览器－腾讯出品。 傲游云浏览器 - 傲游云浏览器。 Vivaldi - Opera开发商出品新的浏览器。 Ōryōki - 小的web浏览器。这是一个试验性的项目，目前正在开发中 翻译工具 有道翻译 - 有道词典桌面版。 辞海词典 - 学单词、背单词、辞海词典。 eudic - 欧路词典词典。 iTranslate - 支持全世界超过 80 种语言发音和输出。 科学上网假设你是个勤奋的同学，你总有一天会强烈需要它们，上帝保佑他们吧。 SpechtLite - 支持 Shadowsocks 及规则管理的高效率代理。 ShadowsocksX - 一个快速的隧道代理，可以帮助你绕过防火墙。 ShadowsocksX-NG - 一款ShadowsocksX客户端软件。 Lantern - 科学上网。 鱼摆摆 - 科学上网。 Tunnelblick - OpenVPN的免费软件。 GoAgentX - 科学上网。 Surge - 科学上网。 云梯 - 在圈内小有名气的VPN服务提供商。 srocket - 开启科学上网。 LoCoVPN - 每天签到可获得2小时免费VPN加速。 二师兄VPN - 提供无限流量、无限续期免费VPN账号。 GTX加速器 - 每天签到领取500M流量。 GreenVPN - 注册激活送免费VPN加速流量。 风驰VPN - 无限流量、无限续期的免费VPN加速服务。 开眼 - Chrome插件免费的科学上网利器。 其它实用工具 12306ForMac - Mac版12306 订票/捡票 助手。 AirServer - 将手机投影到电脑上。 CheatSheet - CheatSheet 是一款Mac上的非常实用的快捷键快速提醒工具。 WWDC - Mac OS的非官方的WWDC APP。 xScope - 测量、检查和测试屏幕上的图形和布局的工具。搜索你的苹果和网络，快速打开应用程序。 f.lux - 自动调整您的电脑屏幕，以匹配亮度。 Todoist - 跨平台的任务管理器与移动应用程序。 TaskPaper - 漂亮的纯文本任务列表。 Wunderlist - 奇妙清单跨平台的任务管理器与移动应用程序。 Ukelele - Unicode键盘布局编辑器。 Karabiner - 一个强大的和稳定的OS X的键盘定制。 Keytty - 让你通过键盘使用鼠标。 AppCleaner - 一个小应用程序，让你彻底卸载不需要的应用程序。 BetterZip 3 - 压缩解压缩工具支持格式 ZIP、TAR、TGZ、TBZ、TXZ (new)、7-ZIP、RAR。 Numi - 漂亮的计算器应用。 Fantastical - 日历应用程序，你将管理好生活。 OnyX - 多功能实用工具来验证磁盘和文件，运行清洁和系统维护任务，配置隐藏选项等。 SSH Tunnel - 管理你的SSH。 Mounty - NTFS 分区读写组件。 Paragon NTFS - 在Mac OS X中完全读写、修改、访问Windows NTFS硬盘、U盘等外接设备的文件。 Tuxera NTFS - Mac上的NTFS文件系统驱动。 gfxCardStatus - 控制Mac独立显卡与集成显卡之间的切换。 openEmu - 模拟器，可以玩魂斗罗之类，放松回到小时候。 Alfred - 效率神器。 Hammerspoon - 功能强大的自动化工具，Lua 脚本驱动，支持窗口管理。 DaisyDisk - 磁盘空间使用扫描工具。 iStat pro - 免费的Mac OS电脑硬件信息检测软件。 BitBar - 支持使用各种语言将信息展示到Mac OS的菜单栏。 ClipMenu - 一个剪贴板操作的管理器。 ControlPlane - 自定义Mac情景模式。比如你到了公司后可以让Mac自动静音或是自动打开Mail客户端，晚上回到家后自动打开iTunes听歌，到了公共场所自行修改网络设置等等。 Caffeine - 实用工具，菜单栏按钮，点击休眠。 Itsycal - 一款简洁实用的开源日历工具。 HTML5 Player - Chrome插件解决中国视频网站播放视频电脑发热的情况。 Monity - 帮助用户实时监控系统的一款非常漂亮的软件。 BetterTouchTool - 代替默认的系统操作方式（组合键、修饰键、手势等）。 iStats - iStats 是一个可以让你快速查看电脑 CPU 温度，磁盘转速和电池等信息的命令行工具。 InsomniaX - 合上盖子不眠不休，继续听歌下载。 NoSleep - 合上盖子不休眠，可根据是否连接电源单独设置。 窗口管理 ShiftIt - 窗口位置和大小管理软件。 Moom - 多任务多窗口的软件。 Slate - 窗口管理器，可用JavaScript写配置。 Amethyst - 窗口管理器（自动保持窗口大小的窗口）。 Spectacle - 简单的移动和调整大小的窗口，和可定制的键盘快捷键。 密码管理 1password - 跨平台帐号密码管理软件。 LastPass - 密码管理器和安全的数字笔记。 KeePassX - 一个免费的，开源的，体积小的密码管理器。 MacPass - 密码管理器。 Finder Quicklook-Plugins - Finder快速预览文件插件。 Path Finder - 强大的Finder替代者，拥有很多特性。 TotalFinder - 强大的Finder替代者，界面风格像Chrome。 XtraFinder - 给Finder添加有用的新特性。 远程协助 TeamViewer - 远程协助及在线协作和会议功能的软件，商业软件个人使用免费。 RealVNC 是一款免费的远程控制跨多平台的程序。 第三方应用市场APP这里讨论盗版问题或者提供黑名单？，拒绝盗版从我做起，欢迎大家监督。 正版这里只提供正版软件购买下载的应用商店。 HackStore - 一个类似于cydia的第三方Mac应用市场，平台拒绝盗版。 MacUpdate Desktop - 管理/更新/下载App，跟踪优惠信息。 homebrew-cask - 体验通过命令行安装Mac软件的工具。 应用商店黑名单第三方应用市场APP黑名单，存在盗版软件传播和下载，拒绝盗版从我做起，欢迎大家监督它们。 腾讯电脑管家 - 电脑管家for Mac 带应用市场。 迅雷Thunder Store - 迅雷Thunder for Mac 带应用市场。 Mac软件宝箱 - Macx推出软件宝箱。 MacHunter - Mac应用市场。 Mac软件下载网站这里主要是推荐一些软件下载的网站，还有一些Mac OSX软件分享网站 正版/介绍 MacUpdate：https://www.macupdate.com/ App Shopper：http://appshopper.com/ 类似于iOS上Cydia一样的第三方软件商店：http://hack-store.com 少数派：http://sspai.com/tag/Mac Mac玩儿法：http://www.waerfa.com 盗版软件下载网站黑名单上面有大量的开源软件或者免费软件，拒绝盗版从我做起，下面被删除的网站提供大量破解软件下载，欢迎大家监督它们。 玩转苹果：http://www.ifunmac.com Mac软件下载站：http://www.pshezi.com MacPeers：http://www.macpeers.com Mac志：http://www.isofts.org Mac软件分享：http://www.waitsun.com AppKed：http://www.macbed.com 苹果软件园：http://www.maczapp.com Mac精品软件：http://xclient.info/ Macx：http://www.macx.cn/ 腾牛网：http://www.qqtn.com/mac/r_17_1.html ⬆ 返回顶部 本文转载自: 原文github","tags":[{"name":"mac","slug":"mac","permalink":"ly2513.github.com/tags/mac/"}]},{"title":"PHP-CLI环境变量的设置和读取","date":"2016-12-27T14:32:37.000Z","path":"2016/12/27/PHP-CLI环境变量的设置和读取/","text":"场景 通常我们在维护PHP线上项目的时候，为了隔离配置和代码，会使用fastcgi_param的形式将环境变量定义在Nginx的配置文件中（Apache可以使用SetEnv指令）。这样在PHP-FPM运行过程中就可以使用getenv函数获取到环境变量的值了。 那对于PHP-CLI，我们又应该怎么设置它的环境变量呢？做法也很简单。 在终端直接执行： 1$ export ART_ENV=production 后续直接使用PHP-CLI命令的时候自然可以获取到环境变量ART_ENV的值： 12$ php -r \"var_dump(getenv('ART_ENV'));\"string(10) \"production\" 但是通常还有这种情况：我们当前登录的用户并不是合适的用来运行PHP-CLI脚本的用户，比如我们期待使用www-data用户来运行PHP-CLI脚本，通常我们会这样做： 12$ sudo -u www-data php -r \"var_dump(getenv('ART_ENV'));\"bool(false) 这时候就会发现无法获取到环境变量了。查看sudo –help可以发现我们还需要设置-E参数： 123$ sudo --help...-E, --preserve-env preserve user environment when running command 按照文档说明补上即可： 12$ sudo -E -u www-data php -r \"var_dump(getenv('ART_ENV'));\"string(10) \"production\" 或者更直接点直接指定环境变量的值： 12$ sudo -u www-data ART_ENV=testing php -r \"var_dump(getenv('ART_ENV'));\"string(7) \"testing\" 需要注意的是：这里的PHP代码都必须使用getenv函数获取环境变量，不能单纯依赖全局变量$_ENV。全局变量$_ENV并不总是可用的，除非在php.ini文件中显式设置了variables_order的值包含E，例如variables_order = “EGPCS”。更多的详细信息可以参考PHP文档的全局变量部分。","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"}]},{"title":"PHPExcel库使用手册","date":"2016-10-22T13:27:08.000Z","path":"2016/10/22/PHPExcel库使用手册/","text":"前言(说在前面的话) 作为开发人员,我们使用数据库来管理数据,但是作为公司的运营同事、销售同事。。不一定会使用一些客户端的数据库软件(sequel Pro、Navicat等),他们当中大多数习惯使用Excel来对数据进行操作。为了让公司的工作效率提高,作为研发的我需要将数据通过程序导入到特定的Excel中供其他同事使用。由于以上所谓的”理由”,本博客就这针对于PHPExcel库使用进行讲解。PHPExcel可是个好东东，功能强大，下面这是一个phpExcel简易中文帮助手册，列举了各种属性，以及常用的操作方法，是每一个都用实例加以说明，希望对大家有所帮助。 也许也有人不知道PHPExcel是什么 PHPExcel: 是用来操作Office Excel 文档的一个PHP类库，它基于微软的OpenXML标准和PHP语言。可以使用它来读取、写入不同格式的电子表格. 由于最近本人在实际的项目开发中需要使用这库做一些报表数据的录入,所以对这库进行了稍微深入的学习,并用此博客进行说明。 话就不多说,我们直接进入主题–怎么使用PHPExcel这扩展库 怎么使用PHPExcel这扩展库 首先你得先引用并实例化一个PHPExcel对象去操作Excel吧 12345678# 引用PHPExcelinclude 'PHPExcel.php';# 用于输出格式为.xlsxinclude 'PHPExcel/Writer/Excel2007.php';# 用于输出格式.xls的// include 'PHPExcel/Writer/Excel5.php';# 创建一个实例$objPHPExcel = new PHPExcel(); 设置属性 123456789101112131415# 创建人$objPHPExcel-&gt;getProperties()-&gt;setCreator(\"Maarten Balliauw\");# 最后修改人$objPHPExcel-&gt;getProperties()-&gt;setLastModifiedBy(\"Maarten Balliauw\");# 标题$objPHPExcel-&gt;getProperties()-&gt;setTitle(\"Office 2007 XLSX Test Document\");# 题目$objPHPExcel-&gt;getProperties()-&gt;setSubject(\"Office 2007 XLSX Test Document\");# 描述$objPHPExcel-&gt;getProperties()-&gt;setDescription(\"Test document for Office 2007 XLSX, generated using PHP classes.\");# 关键字$objPHPExcel-&gt;getProperties()-&gt;setKeywords(\"office 2007 openxml php\");# 种类$objPHPExcel-&gt;getProperties()-&gt;setCategory(\"Test result file\"); 同样你也可以这样设置,”一步到位”,”链式”设置 1234567$objPHPExcel-&gt;getProperties()-&gt;setCreator(\"ctos\") -&gt;setLastModifiedBy(\"ctos\") -&gt;setTitle(\"Office 2007 XLSX Test Document\") -&gt;setSubject(\"Office 2007 XLSX Test Document\") -&gt;setDescription(\"Test document for Office 2007 XLSX, generated using PHP classes.\") -&gt;setKeywords(\"office 2007 openxml php\") -&gt;setCategory(\"Test result file\"); 设置当前的sheet 1$objPHPExcel-&gt;setActiveSheetIndex(0); 设置sheet的标题 1$objPHPExcel-&gt;getActiveSheet()-&gt;setTitle('Simple'); 设置单元格宽度 1$objPHPExcel-&gt;getActiveSheet()-&gt;getColumnDimension('A')-&gt;setWidth(20); 设置单元格高度 1$objPHPExcel-&gt;getActiveSheet()-&gt;getRowDimension($i)-&gt;setRowHeight(40); 合并单元格 1$objPHPExcel-&gt;getActiveSheet()-&gt;mergeCells('A18:E22'); 拆分单元格 1$objPHPExcel-&gt;getActiveSheet()-&gt;unmergeCells('A28:B28'); 设置保护cell,保护工作表 123$objPHPExcel-&gt;getActiveSheet()-&gt;getProtection()-&gt;setSheet(true);$objPHPExcel-&gt;getActiveSheet()-&gt;protectCells('A3:E13', 'PHPExcel'); 设置格式 123$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('E4')-&gt;getNumberFormat()-&gt;setFormatCode(PHPExcel_Style_NumberFormat::FORMAT_CURRENCY_EUR_SIMPLE);$objPHPExcel-&gt;getActiveSheet()-&gt;duplicateStyle( $objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('E4'), 'E5:E13' ); 设置字体加粗 1$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('B1')-&gt;getFont()-&gt;setBold(true); 设置水平对齐方式（HORIZONTAL_RIGHT，HORIZONTAL_LEFT，HORIZONTAL_CENTER，HORIZONTAL_JUSTIFY） 1$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('D11')-&gt;getAlignment()-&gt;setHorizontal(PHPExcel_Style_Alignment::HORIZONTAL_RIGHT); 设置垂直居中 1$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('A18')-&gt;getAlignment()-&gt;setVertical(PHPExcel_Style_Alignment::VERTICAL_CENTER); 设置字号 1$objPHPExcel-&gt;getActiveSheet()-&gt;getDefaultStyle()-&gt;getFont()-&gt;setSize(10); 设置边框 1$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('A1:I20')-&gt;getBorders()-&gt;getAllBorders()-&gt;setBorderStyle(\\PHPExcel_Style_Border::BORDER_THIN); 设置边框颜色12345678# 左边框$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('D13')-&gt;getBorders()-&gt;getLeft()-&gt;getColor()-&gt;setARGB('FF993300');# 上边框$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('D13')-&gt;getBorders()-&gt;getTop()-&gt;getColor()-&gt;setARGB('FF993300');# 下边框$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('D13')-&gt;getBorders()-&gt;getBottom()-&gt;getColor()-&gt;setARGB('FF993300');# 右边框$objPHPExcel-&gt;getActiveSheet()-&gt;getStyle('E13')-&gt;getBorders()-&gt;getRight()-&gt;getColor()-&gt;setARGB('FF993300'); 插入图像1234567891011121314151617$objDrawing = new PHPExcel_Worksheet_Drawing();# 设置图片路径 切记：只能是本地图片$objDrawing-&gt;setPath('图像地址');# 设置图片高度$objDrawing-&gt;setHeight(180);# 设置照片宽度$objDrawing-&gt;setWidth(150);# 设置图片要插入的单元格$objDrawing-&gt;setCoordinates('E2');# 设置X(水平)偏移量(相当于设置的单元格水平偏移,正数:水平右平移,负数:水平左平移)$objDrawing-&gt;setOffsetX(5);# 设置Y(垂直)偏移量(相当于设置的单元格垂直偏移,正数:垂直上平移,负数:垂直下平移)$objDrawing-&gt;setOffsetY(5);$objDrawing-&gt;setRotation(5);$objDrawing-&gt;getShadow()-&gt;setVisible(true);$objDrawing-&gt;getShadow()-&gt;setDirection(50);$objDrawing-&gt;setWorksheet($objPHPExcel-&gt;getActiveSheet()); 设置单元格背景色 12$objPHPExcel-&gt;getActiveSheet(0)-&gt;getStyle('A1')-&gt;getFill()-&gt;setFillType(\\PHPExcel_Style_Fill::FILL_SOLID);$objPHPExcel-&gt;getActiveSheet(0)-&gt;getStyle('A1')-&gt;getFill()-&gt;getStartColor()-&gt;setARGB('FFCAE8EA'); 数据写入 1$objPHPExcel-&gt;setActiveSheetIndex(0)-&gt;setCellValue('A1',$data); 最后输出到浏览器，导出Excel1234567891011121314$savename='导出Excel示例';$ua = $_SERVER[\"HTTP_USER_AGENT\"];$datetime = date('Y-m-d', time());if (preg_match(\"/MSIE/\", $ua)) &#123; $savename = urlencode($savename); //处理IE导出名称乱码&#125;# excel头参数header('Content-Type: application/vnd.ms-excel');# 日期为文件名后缀header('Content-Disposition: attachment;filename=\"'.$savename.'.xls\"');header('Cache-Control: max-age=0');#excel5为xls格式，excel2007为xlsx格式$objWriter = \\PHPExcel_IOFactory::createWriter($objPHPExcel, 'Excel5');$objWriter-&gt;save('php://output'); 输出到本地电脑123456# 输出到本地目录路径$path = /usr/local/;# 文件名称$fileName = $path.'测试Excel.xls';$objWriter = \\PHPExcel_IOFactory::createWriter($objPHPExcel, 'Excel5');$objWriter-&gt;save($path); 以上是本人在工作中用到的PHPExcel类库的方法,以上方法掌握了,对基本的Excel操作足矣。。。。如果要更深的操作就去看看PHPExcel原文的,稍微需要有点英语基础语言。","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"},{"name":"PHPExcel","slug":"PHPExcel","permalink":"ly2513.github.com/tags/PHPExcel/"}]},{"title":"她(三)","date":"2016-10-12T14:07:51.000Z","path":"2016/10/12/她(三)/","text":"她 –记录着我和她最近的一些事情,总想和她发生些故事,哈哈。。你别可歪想哦 ! ! ! 我只想和她在一起做一些我们都喜欢的事,举个栗子(旅行,吃饭,电影。。。)这篇博客接下来就是描述我们一起旅行—-西藏之旅,可以说这次旅行就像钱钟书所说的:”如果你爱一个人，那就带她去旅行吧”,而且是一次说走就走的旅行,其实我跟她说了是有准备攻略的,就是让她安心。事情发生在今年神圣的祖国的生日的第二天,她从广州飞往西宁,我从北京飞往西宁,我们在西宁会合,她比我提前一个小时到达,不知道是她知道我是那趟航班还是。。。我的飞机刚着地滑行几分钟,我就接到她打给我的电话,问我是否到了。我说:”到了,待会见啊”。我高兴的下飞机,恨不得立刻出现在她面前,吓她下,哈哈.. 到了西宁我们赶紧乘坐去西宁火车站的机场大巴,因为离火车发车还有2个小时,时间比较紧。。。。。她在中途跟我说,这地方确实比较贫穷,山上树都不长,而且气候都很干燥,根本没有田地。庆幸自己没出身在这,不然皮肤很不好。她又说:”有点困了,想睡觉”。我说:”你睡吧!!!”。我得说说那个大巴司机了,人生地不熟的我们,到哪下也不告诉我们,差点就在中途下车了。大概行驶了1个多小时,到了西宁火车站,当时的天已经变黑了,但是还是有点亮度,就是日落不久的那种","tags":[{"name":"情感","slug":"情感","permalink":"ly2513.github.com/tags/情感/"}]},{"title":"她(二️)","date":"2016-10-10T13:26:36.000Z","path":"2016/10/10/她(二)/","text":"2016年2月11日一个兴奋的日子,这天是农历正月初五,是我认识她以来第一次去她家拜年。那天我依稀记得我买了很多礼品,这是我们这边的习俗,去的别人家不能空手去,还必须带足的礼品才像样。前一天她跟我说:”你明天来我家,你得提前跟我说,不然我还没起床哦”。为了能让她多睡睡,我到了也没跟她说,直到她妈把她叫起,说:”李勇来了,你还在不起床啊”。 不久,她从她家楼上下来,跟我说:”你咋不提前跟我说,搞得还让我妈来叫”。我告诉她没有提前叫她的原因,还好她没生我气。。。。那天她家来了好多亲朋好友,搞得我有点不好意思,毕竟是第一次去她家,有点紧张。。。过年期间每家都很热闹,她家今天格外的热闹(自我感觉啊)。。。过年的情景就是,她家的亲朋好友聚在一起一边吃着过年的食物一边聊天。。。很快就到中午了,大家接下来就坐在一起吃饭了,她把她外公外婆叫过一起吃饭,我记得当时的情景是她用双手一边搀着外公一边搀着外婆,他们三人的脸上洋溢着🌞般的笑容,和谐至极。我跟她外公外婆并不陌生,之前有些接触,她外公和蔼,性格爽快,关键对我很好。。。还有一点就是她外公酒量惊人,排行榜首(厉害。。)。吃饭的时候她爸就坐我旁边,她爸说过的一句话到现在我还记得,他说:”李勇,你这小伙我满意,我就直说了吧,我对你没什么过多的要求,但你要在县城买套房子,你家先付个首付,如果首付钱还不够我借你点凑足付首付的钱。。。”,听完这句话我不知道说什么,因为我跟她爸这是第一次见面,她爸就直接进入话题了,说道房子是应该要买套,毕竟我也不希望我和她结婚后没有一个新家。我当时就点着头:”嗯。。”她爸妈对我的映像都还不错,我心里很高兴(偷着乐着了。。嘿嘿。。)饭桌上我和她家人聊得还好,都是一群性格直爽的人,没有太多的规矩,她爸妈都很客气。。。特别是她外公,他真心希望我俩好好的在一起生活,特别看好我俩。。那晚,我和她还有我姐、我姐夫、她哥、她嫂子、她弟一起去县城看电影《美人鱼》也是第一次和她看电影。看完电影我们还去吃了夜宵。。之后就回家了。。。第二天,我说服她去我家玩玩,也就是这天,不愉快的事情出来了。。。。。我们从她家出发,到县城的时候,我带她去我舅那拜年,我顺便在这说下,我舅在安城那边开了家超市–世纪华联,欢迎安福的亲朋好友前去购物。。。之后就坐车去我家了,同行的还有她姐、她哥(我姐夫)、她嫂子(我姐),我父母还有我奶奶都喜欢她。。。。她特别有气质,我没说谎话啊,我说真的。待续。。。。","tags":[{"name":"情感","slug":"情感","permalink":"ly2513.github.com/tags/情感/"}]},{"title":"PHP中使用cURL实现Get和Post请求的方法","date":"2016-05-04T14:42:49.000Z","path":"2016/05/04/PHP中使用cURL实现Get和Post请求的方法/","text":"今天主要讲下cURL，cURL是什么了？cURL有什么作用了？它在PHP中又是怎么运用的了？这些问题，都将在本博客中一一为你解惑。 问题一 cURL是什么 curl是利用URL语法在命令行方式下工作的开源文件传输工具，它支持很多协议，如HTTP、FTP、TELNET等。最爽的是，PHP也支持cURL库。 问题二 PHP中又是怎么运用cURL cURL可以使用URL的语法模拟浏览器来传输数据，因为它是模拟浏览器，因此它同样支持多种协议，FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE 以及 LDAP等协议都可以很好的支持，包括一些：HTTPS认证，HTTP POST方法，HTTP PUT方法，FTP上传，keyberos认证，HTTP上传，代理服务器，cookies，用户名/密码认证，下载文件断点续传，上传文件断点续传，http代理服务器管道，甚至它还支持IPv6，scoket5代理服务器，通过http代理服务器上传文件到FTP服务器等等。这就是我们为什么要使用cURL的原因！使用cURL完成简单的请求主要分为以下四步： 1.初始化，创建一个新cURL资源 2.设置URL和相应的选项 3.抓取URL并把它传递给浏览器 4.关闭cURL资源，并且释放系统资源 我们来采集一个页面，通常情况下，我们会使用file_get_contents()函数来获取： 像这样：123456789&lt;?php $str = file_get_contents('http://bbs.baidu.net'); //或者是： $str = file(\"http://bbs.baidu.net\"); //或者是： readfile(\"http://bbs.baidu.net\"); ?&gt; 这样我们会发现，我们没有办法有效地进行错误处理，更重要的是我们没有办法完成一些高难度的任务： 如：处理cookies，验证，表单提交，文件上传等等。 好，现在我们来用代码完成上述cURL的四步：123456789101112// get方式实现// 1.初始化，创建一个新cURL资源$ch = curl_init();// 2.设置URL和相应的选项curl_setopt($ch, CURLOPT_URL, \"http://www.baidu.com/\");curl_setopt($ch, CURLOPT_HEADER, 0);// 3.抓取URL并把它传递给浏览器$output = curl_exec($ch);// 4.关闭cURL资源，并且释放系统资源curl_close($ch);// 打印获得的数据print_r($output); 以下是用POST方式实现12345678910111213141516171819202122&lt;?php // post方式 // 定义声明一个url $url = \"http://localhost/demoCurl.php\"; // 定义的post数据 $post_data = array (\"username\" =&gt; \"bob\",\"key\" =&gt; \"12345\"); // 1.初始化，创建一个新cURL资源 $ch = curl_init(); // 2.设置URL和相应的选项 curl_setopt($ch, CURLOPT_URL, $url); curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1); // 设置post请求 curl_setopt($ch, CURLOPT_POST, 1); // 设置post的数据 curl_setopt($ch, CURLOPT_POSTFIELDS, $post_data); // 3.抓取URL获得数据 $output = curl_exec($ch); // 4.关闭cURL资源，并且释放系统资源 curl_close($ch); // 打印获得的数据 print_r($output);?&gt; 上述四步中，其中第二步最为关键，可以设置一些高级选项： 选项 可选value值 备注 CURLOPT_AUTOREFERER 当根据Location:重定向时，自动设置header中的Referer:信息。 CURLOPT_BINARYTRANSFER 在启用CURLOPT_RETURNTRANSFER的时候，返回原生的（Raw）输出。 CURLOPT_COOKIESESSION 启用时curl会仅仅传递一个session cookie，忽略其他的cookie，默认状况下cURL会将所有的cookie返回给服务端。session cookie是指那些用来判断服务器端的session是否有效而存在的cookie。 CURLOPT_CRLF 启用时将Unix的换行符转换成回车换行符。 CURLOPT_DNS_USE_GLOBAL_CACHE 启用时会启用一个全局的DNS缓存，此项为线程安全的，并且默认启用。 CURLOPT_FAILONERROR 显示HTTP状态码，默认行为是忽略编号小于等于400的HTTP信息。 CURLOPT_FILETIME 启用时会尝试修改远程文档中的信息。结果信息会通过curl_getinfo()函数的CURLINFO_FILETIME选项返回。 curl_getinfo(). CURLOPT_FOLLOWLOCATION 启用时会将服务器服务器返回的”Location: “放在header中递归的返回给服务器，使用CURLOPT_MAXREDIRS可以限定递归返回的数量。 CURLOPT_FORBID_REUSE 在完成交互以后强迫断开连接，不能重用。 CURLOPT_FRESH_CONNECT 强制获取一个新的连接，替代缓存中的连接。 CURLOPT_FTP_USE_EPRT 启用时当FTP下载时，使用EPRT (或 LPRT)命令。设置为FALSE时禁用EPRT和LPRT，使用PORT命令 only. CURLOPT_FTP_USE_EPSV 启用时，在FTP传输过程中回复到PASV模式前首先尝试EPSV命令。设置为FALSE时禁用EPSV命令。 CURLOPT_FTPAPPEND 启用时追加写入文件而不是覆盖它。 CURLOPT_FTPASCII CURLOPT_TRANSFERTEXT的别名。 CURLOPT_FTPLISTONLY 启用时只列出FTP目录的名字。 CURLOPT_HEADER 启用时会将头文件的信息作为数据流输出。 CURLINFO_HEADER_OUT 启用时追踪句柄的请求字符串。 从 PHP 5.1.3 开始可用。CURLINFO_前缀是故意的(intentional)。 CURLOPT_HTTPGET 启用时会设置HTTP的method为GET，因为GET是默认是，所以只在被修改的情况下使用。 CURLOPT_HTTPPROXYTUNNEL 启用时会通过HTTP代理来传输。 CURLOPT_MUTE 启用时将cURL函数中所有修改过的参数恢复默认值。 CURLOPT_NETRC 在连接建立以后，访问~/.netrc文件获取用户名和密码信息连接远程站点。 CURLOPT_NOBODY 启用时将不对HTML中的BODY部分进行输出。 CURLOPT_NOPROGRESS 启用时关闭curl传输的进度条，此项的默认设置为启用。 Note: PHP自动地设置这个选项为TRUE，这个选项仅仅应当在以调试为目的时被改变。 CURLOPT_NOSIGNAL 启用时忽略所有的curl传递给php进行的信号。在SAPI多线程传输时此项被默认启用。 cURL 7.10时被加入。 CURLOPT_POST 启用时会发送一个常规的POST请求，类型为：application/x-www-form-urlencoded，就像表单提交的一样。 CURLOPT_PUT 启用时允许HTTP发送文件，必须同时设置CURLOPT_INFILE和CURLOPT_INFILESIZE。 CURLOPT_RETURNTRANSFER 将curl_exec()获取的信息以文件流的形式返回，而不是直接输出。 CURLOPT_SSL_VERIFYPEER 禁用后cURL将终止从服务端进行验证。使用CURLOPT_CAINFO选项设置证书使用CURLOPT_CAPATH选项设置证书目录 如果CURLOPT_SSL_VERIFYPEER(默认值为2)被启用，CURLOPT_SSL_VERIFYHOST需要被设置成TRUE否则设置为FALSE。 自cURL 7.10开始默认为TRUE。从cURL 7.10开始默认绑定安装。 CURLOPT_TRANSFERTEXT 启用后对FTP传输使用ASCII模式。对于LDAP，它检索纯文本信息而非HTML。在Windows系统上，系统不会把STDOUT设置成binary模式。 CURLOPT_UNRESTRICTED_AUTH 在使用CURLOPT_FOLLOWLOCATION产生的header中的多个locations中持续追加用户名和密码信息，即使域名已发生改变。 CURLOPT_UPLOAD 启用后允许文件上传。 CURLOPT_VERBOSE 启用时会汇报所有的信息，存放在STDERR或指定的CURLOPT_STDERR中。 对于下面的这些option的可选参数，value应该被设置一个integer类型的值： 选项 可选value值 备注 CURLOPT_BUFFERSIZE 每次获取的数据中读入缓存的大小，但是不保证这个值每次都会被填满。 在cURL 7.10中被加入。 CURLOPT_CLOSEPOLICY 不是CURLCLOSEPOLICY_LEAST_RECENTLY_USED就是CURLCLOSEPOLICYOLDEST，还存在另外三个CURLCLOSEPOLICY，但是cURL暂时还不支持。 CURLOPT_CONNECTTIMEOUT 在发起连接前等待的时间，如果设置为0，则无限等待。 CURLOPT_CONNECTTIMEOUT_MS 尝试连接等待的时间，以毫秒为单位。如果设置为0，则无限等待。 在cURL 7.16.2中被加入。从PHP 5.2.3开始可用。 CURLOPT_DNS_CACHE_TIMEOUT 设置在内存中保存DNS信息的时间，默认为120秒。 CURLOPT_FTPSSLAUTH FTP验证方式：CURLFTPAUTH_SSL (首先尝试SSL)，CURLFTPAUTH_TLS (首先尝试TLS)或CURLFTPAUTH_DEFAULT (让cURL自动决定)。 在cURL 7.12.2中被加入。 CURLOPT_HTTP_VERSION CURL_HTTP_VERSION_NONE (默认值，让cURL自己判断使用哪个版本)，CURL_HTTP_VERSION_1_0 (强制使用 HTTP/1.0)或CURL_HTTP_VERSION_1_1 (强制使用 HTTP/1.1)。 CURLOPT_HTTPAUTH 使用的HTTP验证方法，可选的值有：CURLAUTH_BASIC、CURLAUTH_DIGEST、CURLAUTH_GSSNEGOTIATE、CURLAUTH_NTLM、CURLAUTH_ANY和CURLAUTH_ANYSAFE。 可以使用位域(或)操作符分隔多个值，cURL让服务器选择一个支持最好的值。 CURLAUTH_ANY等价于CURLAUTH_BASIC 或 CURLAUTH_DIGEST 或 CURLAUTH_GSSNEGOTIATE 或 CURLAUTH_NTLM. CURLAUTH_ANYSAFE等价于CURLAUTH_DIGEST 或 CURLAUTH_GSSNEGOTIATE 或 CURLAUTH_NTLM. CURLOPT_INFILESIZE 设定上传文件的大小限制，字节(byte)为单位。 CURLOPT_LOW_SPEED_LIMIT 当传输速度小于CURLOPT_LOW_SPEED_LIMIT时(bytes/sec)，PHP会根据CURLOPT_LOW_SPEED_TIME来判断是否因太慢而取消传输。 CURLOPT_LOW_SPEED_TIME 当传输速度小于CURLOPT_LOW_SPEED_LIMIT时(bytes/sec)，PHP会根据CURLOPT_LOW_SPEED_TIME来判断是否因太慢而取消传输。 CURLOPT_MAXCONNECTS 允许的最大连接数量，超过是会通过CURLOPT_CLOSEPOLICY决定应该停止哪些连接。 CURLOPT_MAXREDIRS 指定最多的HTTP重定向的数量，这个选项是和CURLOPT_FOLLOWLOCATION一起使用的。 CURLOPT_PORT 用来指定连接端口。（可选项） CURLOPT_PROTOCOLS CURLPROTO_*的位域指。如果被启用，位域值会限定libcurl在传输过程中有哪些可使用的协议。这将允许你在编译libcurl时支持众多协议，但是限制只是用它们中被允许使用的一个子集。默认libcurl将会使用全部它支持的协议。参见CURLOPT_REDIR_PROTOCOLS. 可用的协议选项为：CURLPROTO_HTTP、CURLPROTO_HTTPS、CURLPROTO_FTP、CURLPROTO_FTPS、CURLPROTO_SCP、CURLPROTO_SFTP、CURLPROTO_TELNET、CURLPROTO_LDAP、CURLPROTO_LDAPS、CURLPROTO_DICT、CURLPROTO_FILE、CURLPROTO_TFTP、CURLPROTO_ALL 在cURL 7.19.4中被加入。 CURLOPT_PROXYAUTH HTTP代理连接的验证方式。使用在CURLOPT_HTTPAUTH中的位域标志来设置相应选项。对于代理验证只有CURLAUTH_BASIC和CURLAUTH_NTLM当前被支持。 在cURL 7.10.7中被加入。 CURLOPT_PROXYPORT 代理服务器的端口。端口也可以在CURLOPT_PROXY中进行设置。 CURLOPT_PROXYTYPE 不是CURLPROXY_HTTP (默认值) 就是CURLPROXY_SOCKS5。 在cURL 7.10中被加入。 CURLOPT_REDIR_PROTOCOLS CURLPROTO_*中的位域值。如果被启用，位域值将会限制传输线程在CURLOPT_FOLLOWLOCATION开启时跟随某个重定向时可使用的协议。这将使你对重定向时限制传输线程使用被允许的协议子集默认libcurl将会允许除FILE和SCP之外的全部协议。这个和7.19.4预发布版本种无条件地跟随所有支持的协议有一些不同。关于协议常量，请参照CURLOPT_PROTOCOLS。 在cURL 7.19.4中被加入。 CURLOPT_RESUME_FROM 在恢复传输时传递一个字节偏移量（用来断点续传）。 CURLOPT_SSL_VERIFYHOST 1 检查服务器SSL证书中是否存在一个公用名(common name)。译者注：公用名(Common Name)一般来讲就是填写你将要申请SSL证书的域名 (domain)或子域名(sub domain)。2 检查公用名是否存在，并且是否与提供的主机名匹配。 CURLOPT_SSLVERSION 使用的SSL版本(2 或 3)。默认情况下PHP会自己检测这个值，尽管有些情况下需要手动地进行设置。 CURLOPT_TIMECONDITION 如果在CURLOPT_TIMEVALUE指定的某个时间以后被编辑过，则使用CURL_TIMECOND_IFMODSINCE返回页面，如果没有被修改过，并且CURLOPT_HEADER为true，则返回一个”304 Not Modified”的header， CURLOPT_HEADER为false，则使用CURL_TIMECOND_IFUNMODSINCE，默认值为CURL_TIMECOND_IFUNMODSINCE。 CURLOPT_TIMEOUT 设置cURL允许执行的最长秒数。 CURLOPT_TIMEOUT_MS 设置cURL允许执行的最长毫秒数。 在cURL 7.16.2中被加入。从PHP 5.2.3起可使用。 CURLOPT_TIMEVALUE 设置一个CURLOPT_TIMECONDITION使用的时间戳，在默认状态下使用的是CURL_TIMECOND_IFMODSINCE。 对于下面的这些option的可选参数，value应该被设置一个string类型的值： 选项 可选value值 备注 CURLOPT_CAINFO 一个保存着1个或多个用来让服务端验证的证书的文件名。这个参数仅仅在和CURLOPT_SSL_VERIFYPEER一起使用时才有意义。 CURLOPT_CAPATH 一个保存着多个CA证书的目录。这个选项是和CURLOPT_SSL_VERIFYPEER一起使用的。 CURLOPT_COOKIE 设定HTTP请求中”Cookie: “部分的内容。多个cookie用分号分隔，分号后带一个空格(例如， “fruit=apple; colour=red”)。 CURLOPT_COOKIEFILE 包含cookie数据的文件名，cookie文件的格式可以是Netscape格式，或者只是纯HTTP头部信息存入文件。 CURLOPT_COOKIEJAR 连接结束后保存cookie信息的文件。 CURLOPT_CUSTOMREQUEST 使用一个自定义的请求信息来代替”GET”或”HEAD”作为HTTP请求。这对于执行”DELETE” 或者其他更隐蔽的HTTP请求。有效值如”GET”，”POST”，”CONNECT”等等。也就是说，不要在这里输入整个HTTP请求。例如输入”GET /index.html HTTP/1.0 “是不正确的。Note: 在确定服务器支持这个自定义请求的方法前不要使用。 CURLOPT_EGDSOCKET 类似CURLOPT_RANDOM_FILE，除了一个Entropy Gathering Daemon套接字。 CURLOPT_ENCODING HTTP请求头中”Accept-Encoding: “的值。支持的编码有”identity”，”deflate”和”gzip”。如果为空字符串””，请求头会发送所有支持的编码类型。 在cURL 7.10中被加入。 CURLOPT_FTPPORT 这个值将被用来获取供FTP”POST”指令所需要的IP地址。”POST”指令告诉远程服务器连接到我们指定的IP地址。这个字符串可以是纯文本的IP地址、主机名、一个网络接口名（UNIX下）或者只是一个’-‘来使用默认的IP地址。 CURLOPT_INTERFACE 网络发送接口名，可以是一个接口名、IP地址或者是一个主机名。 CURLOPT_KRB4LEVEL KRB4 (Kerberos 4) 安全级别。下面的任何值都是有效的(从低到高的顺序)：”clear”、”safe”、”confidential”、”private”.。如果字符串和这些都不匹配，将使用”private”。这个选项设置为NULL时将禁用KRB4 安全认证。目前KRB4 安全认证只能用于FTP传输。 CURLOPT_POSTFIELDS 全部数据使用HTTP协议中的”POST”操作来发送。要发送文件，在文件名前面加上@前缀并使用完整路径。这个参数可以通过urlencoded后的字符串类似’para1=val1&amp;para2=val2&amp;…’或使用一个以字段名为键值，字段数据为值的数组。如果value是一个数组，Content-Type头将会被设置成multipart/form-data。 CURLOPT_PROXY HTTP代理通道。 CURLOPT_PROXYUSERPWD 一个用来连接到代理的”[username]:[password]”格式的字符串。 CURLOPT_RANDOM_FILE 一个被用来生成SSL随机数种子的文件名。 CURLOPT_RANGE 以”X-Y”的形式，其中X和Y都是可选项获取数据的范围，以字节计。HTTP传输线程也支持几个这样的重复项中间用逗号分隔如”X-Y,N-M”。 CURLOPT_REFERER 在HTTP请求头中”Referer: “的内容。 CURLOPT_SSL_CIPHER_LIST 一个SSL的加密算法列表。例如RC4-SHA和TLSv1都是可用的加密列表。 CURLOPT_SSLCERT 一个包含PEM格式证书的文件名。 CURLOPT_SSLCERTPASSWD 使用CURLOPT_SSLCERT证书需要的密码。 CURLOPT_SSLCERTTYPE 证书的类型。支持的格式有”PEM” (默认值), “DER”和”ENG”。 在cURL 7.9.3中被加入。 CURLOPT_SSLENGINE 用来在CURLOPT_SSLKEY中指定的SSL私钥的加密引擎变量。 CURLOPT_SSLENGINE_DEFAULT 用来做非对称加密操作的变量。 CURLOPT_SSLKEY 包含SSL私钥的文件名。 CURLOPT_SSLKEYPASSWD 在CURLOPT_SSLKEY中指定了的SSL私钥的密码。 Note: 由于这个选项包含了敏感的密码信息，记得保证这个PHP脚本的安全。 CURLOPT_SSLKEYTYPE CURLOPT_SSLKEY中规定的私钥的加密类型，支持的密钥类型为”PEM”(默认值)、”DER”和”ENG”。 CURLOPT_URL 需要获取的URL地址，也可以在curl_init()函数中设置。 CURLOPT_USERAGENT 在HTTP请求中包含一个”User-Agent: “头的字符串。 CURLOPT_USERPWD 传递一个连接中需要的用户名和密码，格式为：”[username]:[password]”。 对于下面的这些option的可选参数，value应该被设置为一个回调函数名： 选项 可选value值 CURLOPT_HEADERFUNCTION 设置一个回调函数，这个函数有两个参数，第一个是cURL的资源句柄，第二个是输出的header数据。header数据的输出必须依赖这个函数，返回已写入的数据大小。 CURLOPT_PASSWDFUNCTION 设置一个回调函数，有三个参数，第一个是cURL的资源句柄，第二个是一个密码提示符，第三个参数是密码长度允许的最大值。返回密码的值。 CURLOPT_PROGRESSFUNCTION 设置一个回调函数，有三个参数，第一个是cURL的资源句柄，第二个是一个文件描述符资源，第三个是长度。返回包含的数据。 CURLOPT_READFUNCTION 拥有两个参数的回调函数，第一个是参数是会话句柄，第二是HTTP响应头信息的字符串。使用此函数，将自行处理返回的数据。返回值为数据大小，以字节计。返回0代表EOF信号。 CURLOPT_WRITEFUNCTION 拥有两个参数的回调函数，第一个是参数是会话句柄，第二是HTTP响应头信息的字符串。使用此回调函数，将自行处理响应头信息。响应头信息是整个字符串。设置返回值为精确的已写入字符串长度。发生错误时传输线程终止。","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"}]},{"title":"前端组件库","date":"2016-04-27T08:39:21.000Z","path":"2016/04/27/前端组件库/","text":"搭建web app常用的样式/组件等收集列表(移动优先) 一个常用的PHP类库, 资源以及技巧列表 awesome-php 推荐 0. 前端自动化(Workflow) 前端构建工具 Webpack - module bundler Yeoman - a set of tools for automating development workflow gulp - The streaming build system grunt - the JavaScript Task Runner F.I.S - 前端集成解决方案 前端模块管理器 Bower - A package manager for the web Browserify Component Duo RequireJS Sea.js LABjs - 文件加载器 css-modulesify - CSS模块加载器 css预处理器 Less - Less is More , Than CSS Sass - Syntactically Awesome Style Sheets Stylus - Expressive, dynamic, robust CSS 前端性能分析工具 analyze-css - CSS 选择器的复杂度和性能分析器 1. 前端框架(Frameworks) Bootstrap Foundation Amaze UI Semantic UI Pure CSS topcoat UIkit Material UI Materialize - 一个基于 Material Design 的 CSS 框架 Framework7 mui ionic framework Fries jQuery Mobile App.js Office UI Fabric - 微软 Office 前端团队的框架 2. JavaScript 框架汇总 JavaScript 框架 react Angular jQuery Backbone.js ember.js Ractive.js KISSY Zepto.js Vanilla JS Vue.js - 数据驱动的组件化MVVM库(用于创建web交互界面和数据双向绑定) Can.js Avalon T3 JavaScript Framework - 一个用于搭建大型Web应用的客户端JS框架 jsblocks - Better MV-ish Framework 从简单 UI 到复杂单页应用都适用 轻量级JavaScript框架 Min.js - Super minimal selector and event library skel.js - A lightweight responsive framework Sprint.js - 一个高性能、小体积的 DOM 操作库 knockout PhantomJS - 一个基于 WebKit 的服务器端 JavaScript API 函数式反应型编程框架 FRP (Functional Reactive Programming) library Bacon.js - A small functional reactive programming lib for JavaScript. Flapjax - 一个支持FRP的JavaScript框架 JavaScript 工具库 underscore.js Underscore.string.js字符串操作库 functional.js Lo-Dash - A JavaScript utility library Lazy.js - a functional utility library for JavaScript Fn.js Way.js - 双向数据绑定库 boiler - a utility library that makes tasks in JavaScript easier 快捷键操作 Keys.js - 应用快捷键 simple-hotkeys jquery.hotkeys Mousetrap - 键盘快捷键操作 3. 前端游戏框架 cocos2d-html5 Egret Engine LimeJS EaselJS three.js AlloyStick The-Best-JS-Game-Framework CanvasEngine Quintus Stage.js phaser - 一个快速、免费、开源的 HTML5 游戏框架 4. ui组件库(CSS Framework) WeUI - 微信官方UI库 GMU - 基于zepto的ui组件库，适用于移动端 FrozenUI - 腾讯移动端组件库 NEC - 更好的CSS方案 NEJ - 更好的JS解决方案 Pure CSS Components magic-of-css Primer - The CSS toolkit and guidelines that power GitHub light7 - A Light and easy to use UI Lib Spectre.css - 轻量、响应式的现代 CSS 框架 5. 基础模版 浏览器统一(Cross Browser) HTML5 BOILERPLATE Modernizr Normalize.css HTML5 Shiv - 让ie浏览器成为支持html5的浏览器的解决方法 cssFx - 为CSS3自动生成浏览器前缀 -prefix-free - Break free from CSS prefix hell ieBetter.js - make ie browser like a morden browser main for ie6~ie8 es6-promise - Promise 对象的兼容 响应式 Responsive - 响应式布局 Enquire.js - Awesome Media Queries in JavaScript Free Wall - 创建桌面，移动和平板的动态网格布局 适配方案 MetaHandler.js：移动端适配各种屏幕无痛工具脚本 lib-flexible: 移动端自适应方案 参考: 使用Flexible实现手淘H5页面的终端适配 flexible demo 1 flexible demo 2 pageResponse responsive-page 6. 排版 yue.css typo.css chinese-copywriting-guidelines - 中文文案排版指南 7. 网格系统 grid Flexbox Grid MasonJS - creating a perfect grid 8. 路由和链接(Routing And URLs) History.js - gracefully supports the HTML5 History/State APIs jquery-pjax - pushState+ajax jquery-address - Deep Linking page.js - Micro client-side router crossroads.js hash.js path.js director.js - 前端路由库(通过#符号进行路径组织,结合vue的component可进行单页的局部模块刷新) URI.js Roadcrew.js SpeakingURL uparams - An URL params parser 9. UA 识别 detector 10. 表单处理10.1 表单验证(Form Validator)/表单提示 Validator Parsley jquery.form.js - jQuery Form Plugin Validform validator.js jquery-validation - jQuery Validation Plugin formvalidator.js Fort.js – 表单填写进度提示 mailcheck - 用于检测email地址的域名 Floatlable.js - 输入时显示placeholder文本 jQuery Label Better validator.js 10.2 &lt; select &gt; 相关 Chosen Select2 bootstrap-select 10.3 单选框/复选框相关 iCheck - 增强复选框和单选按钮 Switchery - iOS 7 style switches for your checkboxes 10.4 上传组件 jQuery File Upload Plugin 百度 Web Uploader uploadify Plupload Fine Uploader arale-upload - 轻量级 iframe and html5 file uploader Dropzone.js - 文件上传库(可获取文件mime, 文件大小等; 针对图片可生成缩略图,获取图片宽度,高度) flow.js localResizeIMG - 前端本地客户端压缩图片，兼容IOS，Android，PC simple-uploader 10.5 日期选择 Both Date and Time picker widget based on twitter bootstrap GMU 日历组件 Mobiscroll Pikaday - 日期选择器 PriceCalendar - 酒店价格日历 API文档 DEMO 10.6 取色 Colorpicker plugin for Twitter Bootstrap 10.7 标签插件(Tag) TaggingJS – 可以灵活定制的 jQuery 标签系统插件 selectize.js Bootstrap Tags Input 10.8 自动完成插件 At.js - 一个Twitter/微博样式的@自动完成插件 jquery-textcomplete - 智能搜索提示框/自动补全 typeahead.js - a fast and fully-featured autocomplete library 10.9 样式修正 autosize - 使文本框自动适应所输入的内容 11. 图表绘制/图形库(Graphics) Highcharts Chart.js - Simple HTML5 Charts using Canvas 百度 ECharts Chartist.js D3.js - A JavaScript visualization library for HTML and SVG. intro-to-d3 - a D3.js tutorial Bonsai - 一个功能强大的JavaScript图形库 epoch - 数据图表可视化 Vis.js Coutour.js zrender - 一个轻量级的Canvas类库，MVC封装，数据驱动，提供类Dom事件模型，让canvas绘图大不同！ 12. 日期格式化 &amp; 时间轴 Moment.js - 日期处理 Smart Time Ago - 显示相对时间 timeline.js 13. 页面交互13.1 Slider slick - the last carousel you’ll ever need Swipe - the most accurate touch slider Swiper - Most modern mobile touch slider iscroll - Smooth scrolling for the web iSlider - 移动端滑动组件 OwlCarousel - create beautiful responsive carousel slider jquery-mousewheel - jQuery鼠标滚轮滚动侦测插件 Glide.js - 轻量级滑块组件 PhotoSwipe TouchSlide - 触屏滑动特效(焦点图,Tab切换) 13.2 瀑布流 Masonry Isotope - Filter &amp; sort magical layouts 13.3 懒加载/加载监听/预加载 imagesLoaded Echo.js lazySizes jquery_lazyload BttrLazyLoading lazyload.js layzr.js - 一个小巧快速的图片懒加载库 waitForImages - 图片加载监听库 PxLoader - JS预加载库:实现图片、声音等各种文件的预加载功能 bindWithDelay - jQuery Plugin For Delayed Event Execution TypeWatch - 停止输入时调用 13.4 图片轮播(幻灯片)/图片展示 FlexSlider unslider - 小而美的轮播库 prettyPhoto FlickerPlate - A cool jQuery plugin that lets you flick through content. Holder.js - Client-side image placeholders. RowGrid.js - 在径直的行里放置图片 ImageLightbox.js - 灯箱效果 JQuery Panorama Viewer - 全景视图 Intense Images - 全屏查看图片 Picturefill - 一个响应式图片 JS 插件 zoom.js - 一个 jQuery 图片放大插件 watermarkjs - 一个在浏览器中添加图片水印的 JS 库 responsive-images.js 13.5 图片剪裁/图片处理 Jcrop - Image Cropping Plugin for jQuery croppic - an image cropping jquery plugin smartcrop.js - 智能图片裁剪库 jQuery.eraser - 图像擦除插件 DD_belatedPNG.js - 让IE6支持透明PNG图片 FocusPoint.js 实现图片的响应式裁剪 imgareaselect CSSgram - CSS 实现的 Instagram 滤镜库 antimoderate - 图片模糊库 13.6 进度条/加载动画(Loading) NProgress.js progress.js Pace - Automatic page load progress bar jquery-ajax-progress nanobar - Very lightweight progress bars. waitMe - 很漂亮的loading效果 spin.js sonic.js fakeLoader.js loaders.css - 一个为性能优化的实现加载动画效果的 CSS 框架 css-loaders 13.7 侧滑插件(offcancas) pushy - a responsive off-canvas navigation menu Slideout.js - 一个用于移动 Web 应用的触摸滑出式导航菜单 13.8 菜单(Menu) SuperFish - 基于jQuery的级联下拉菜单 Responsive Nav - 响应式导航 13.9 滚动侦测(ScrollSpy) jquery-scrollspy(1) jquery-scrollspy(2) Waypoints ScrollMagic - 像进度条一样使用滚动条 13.10 滚动加载更多/下拉刷新(Pull to Refresh) jScroll web-pull-to-refresh pulltorefresh RubberBand.js - add pull-to-refresh functionality to any page. 13.11 平滑滚动插件(Smooth Scroll) jquery-smooth-scroll jquery.scrollTo - 平滑滚动到页面指定位置 smooth-scroll scrollUp elevator.js - 一个模拟电梯运行“返回顶部”的 JS 插件 13.12 全屏滚动/全屏切换 pagePiling.js - 全屏滚动效果 fullPage.js onepage-scroll zepto.fullpage - 专注于移动端的fullPage.js screenfull.js - 切换全屏模式 13.13 分屏滚动 multiscroll.js - 分屏滚动效果 13.14 转场效果 Animsition - 页面切换时的过渡效果 13.15 固定元素(Sticky) sticky - jQuery Plugin for Sticky Objects jquery.pin - 固定页面元素 stickUp Slinky.js - 堆叠头部创建滑动导航列表 13.16 触控事件 Hammer.js jquery.event.move.js 13.17 拖拽组件 Draggabilly - 专注于拖拽功能的 JS 库 dragula - 一个让拖放操作变简单的 JS 库 GridList - 可拖拉的响应式列表库 13.18 隐藏或展示页面元素 Headroom.js - 在不需要页头时将其隐藏 Readmore.js - 内容显示与隐藏插件 oriDomi - 像指一样折叠Dom元素 13.19 滚动条(Scrollbar) jScrollPane jquery.scrollbar perfect-scrollbar nanoScrollerJS tinyscrollbar 13.20 视差滚动(Parallax Scrolling) parallax.js jparallax skrollr 14. 代码高亮插件/代码编辑器 google-code-prettify highlight.js Rainbow ACE CodeMirror Crayon Syntax Highlighter prism - Lightweight, robust, elegant syntax highlighting. 15. UI Icon 组件 Font Awesome Glyphter: The SVG Font Machine Perfect Icons iconizr Cikonss - 纯CSS实现的响应式Icon Simple Icons 16. 动画(Animate) animate.css - A cross-browser library of CSS animations. Transit - CSS transitions and transformations for jQuery WOW - 在滚动过程中展示CSS动画效果(默认触发animate.css动画) AniJS - A Library to Raise your Web Design without Coding Move.js - 简化CSS3动画的JS库 ScrollMe – 在网页中加入各种滚动动画效果 Effeckt.css - A Performant Transitions and Animations Library NEC动画库 csshake - CSS classes to move your DOM magic - CSS3 Animations with special effects Hover.css - 很多鼠标Hover态的效果 SpinKit Velocity.js - 加速JavaScript动画 lenticular.js - 响应倾斜或鼠标事件创建图片动画 jQuery Interactive 3D - Create a 3D interactive object using images AnimateScroll - A Simple jQuery Plugin for Animating Scroll Blast.js - 把动画和样式注入到文本中 Bounce.js - 一个用于制作漂亮 CSS3 关键帧动画的 JS 库 Sticker.js - create a Sticker Effect scrollReveal.js - 元素进入可视区域自动触发设置好的动画 stroll.js - CSS3 list scroll effects jQuery Easing - 动画效果扩展 animations - CSS3 ANIMATION CHEAT SHEET iconate.js：将 icons 增加动画效果的 JS 库 17. 本地存储 cross-storage - Cross domain local storage localForage pouchdb basil.js Neurosync - JavaScript 本地离线 ORM 库 18. 模板引擎 mustache.js Handlebars.js artTemplate baiduTemplate JSRender EJS - JavaScript Templates Juicer - A Light Javascript Templete Engine. Tempo json2html Hogan.js - JavaScript templating from Twitter. Dust.js - Linkedin维护的项目 19. 通知组件/弹框组件/模态窗口 Notify.js(Web Notifications API) alertify.js AlertifyJS SweetAlert Messenger - 非常酷的弹框组件 PNotify Notify.js - A simple, versatile notification library Remodal - 模态窗口插件 action.js - 极简的tip和Modal弹窗效果 20. 提示控件(Tooltips) hint.css - 一款非常小巧的提示框效果 qTip2 - Pretty powerful tooltips tooltip - CSS Tooltips tooltipster - A jQuery tooltip plugin grumble.js - 气泡形状的提示（Tooltip）控件 Ouibounce - 离站提示控件 intro.js - 一个创建引导式网站介绍功能的 JS 库 data-tip.css - 纯 CSS 实现的工具提示 21. 对话框/遮罩层/弹出层(lightbox) fancyBox - Fancy jQuery lightbox jquery-lightbox - The popular lightbox script, ported to jQuery Colorbox - a jQuery lightbox artDialog - 经典的网页对话框组件 DialogEffects jQuery blockUI - Page or element overlay layer - web弹出窗/层 22. 文档/表格/PDF Backgrid.js - 强大的表格组件 handsontable - 在线可编辑excel表格 jQuery Bootgrid - 用于ajax生成动态表格 DataTables - Table plug-in for jQuery PDF.js - 一个 JavaScript 编写的 PDF 阅读器 jsPDF - Generate PDF files in JavaScript Recline.js - 灵活操作和展示数据 Dynatable - 交互式表格插件 fattable - 创建无限滚动无限行列数的表格 23. 目录树插件 zTree_v3 - jQuery Tree Plugin jstree - jQuery Tree Plugin fancytree - Tree plugin for jQuery 24. Ajax模块 fetch - A window.fetch JavaScript polyfill reqwest - browser asynchronous http requests ajax - Standalone AJAX library then-request browser-request superagent minAjax.js 25. 音频/视频 jPlayer - HTML5 Audio &amp; Video for jQuery video.js - HTML5 &amp; Flash video player Accessible HTML5 Video Player - PayPal 开源的 HTML5 视频播放器 Clappr - 开源的Web视频播放器 Plyr - A simple HTML5 media player FitVids.js - A lightweight, easy-to-use jQuery plugin for fluid width video embeds. BigVideo.js - The jQuery Plugin for Big Background Video BigScreen - A simple library for using the JavaScript Full Screen API Vide - 视频背景 winamp2-js Buzz - A Javascript HTML5 Audio library MediaElement.js 26. 按钮 Buttons - A CSS button library ButtonComponentMorph ProgressButtonStyles CreativeButtons CSS3 buttons jquery.onoff - Interactive, accessible toggle switches for the web. 27. 富文本编辑器/Markdown编辑器/Markdown解析器 Simditor - 简单快速的富文本编辑器 BachEditor - 一个有情怀的编辑器 TinyMCE bootstrap-markdown marked - markdown解析器 Markdown Plus Editor.md - 开源在线Markdown编辑器 stackedit Redactor Text Editor micromarkdown.js - 轻量级的md解析器 wangEditor - 支持移动端的轻量级web富文本框 28. 内容提取(Readability) Readability json.human.js - Json Formatting for Human Beings 29. 颜色(CSS Colors)/SVG/Canvas CSS Colours SVGeneration SVGMagic - 自动的创建PNG来兼容不支持SVG的浏览器 Adaptive Backgrounds - 从图片抽取主要颜色和应用到父元素 Seen.js - 渲染3D场景为SVG或者HTML Canvas 30. 选项卡(Tabs) Easy Responsive Tabs to Accordion Responsive-Tabs ion.tabs - jQuery tabs plugin jQuery-EasyTabs tabulous.js 31. 文本处理 ZeroClipboard - 文本复制插件 clipboard.js Bigfoot - 点击文章中的脚注弹窗显示 Annotator - 文本注解插件，可以包括注释、标签、用户等 Succinct - 用作截断多行文本,后面添加省略号 Flowtype.js - 自动调整字体大小和行号 flat-shadow FitText - A jQuery plugin for inflating web type shine.js - 实现漂亮阴影 Type Rendering Mix - 文本渲染引擎 jquery-expander - 阅读更多 Typed.js - 输入模拟插件 jQuery.dotdotdot - 多行文本溢出显示省略号 32. 布局(Layout) 分隔面板(Split Panel) split-pane jQuery UI Layout 33. 演示/幻灯片 reveal.js - The HTML Presentation Framework bespoke.js - DIY Presentation Micro-Framework impress.js shower deck.js 34. 国际化(i18n) jquery-i18n i18next.js jsperanto.js jed.js messageformat.js Polyglot.js 35. 邮件模板(Email Templates) responsive-html-email-template 36. 移动端优化(Optimizing Mobile Performance) FastClick - 处理移动端 click 事件 300 毫秒延迟 tappy jquery-fast-click 37. HTTP请求相关 pako - HTTP 请求正文压缩 参考阅读: 如何压缩 HTTP 请求正文 HTTP 请求正文压缩 DEMO RSA in JavaScript - 用RSA加密实现Web数据加密传输 38. 实用工具/其他插件 jquery-cookie InstantClick - 预加载用户可能会点击的一些链接 Async.js - 异步操作 html2canvas - 实现纯JS网页截图 jquery.qrcode.js - 生成二维码的 jQuery 插件 qrcodejs - JS生成QRCode的库 nakedpassword - 用脱衣女帮助检测密码强度 KityMinder - 脑图编辑工具 MixitUp - 动画过滤和排序 JQuery Tip Cards - 创建卡片交互的cards布局 Fallback.js - JavaScript library for dynamically loading CSS and JS files. swfobject prettyprint.js - An in-browser JavaScript variable dumper Shepherd - 为应用创建用户指南 RulersGuide.js - 类似PhotoShop标尺的js库 Gremlins.js - Monkey 测试库 RoughDraft.js - 简单快速的创建交互式的 HTML 模型的原型工具 favico.js - 动态改变浏览器标签栏中的网站图标 #设计模式( JavaScript Patterns ) javascript-patterns jquery-patterns - A variety of jQuery plugin patterns Learning JavaScript Design Patterns #在线工具( Online Tools ) jsbin - Collaborative JavaScript Debugging App jsbin@Github jsfiddle jsbeautifier - Online JavaScript beautifier resume.github.com 前端开发工具1. 开发工具 Sublime Text 2. 调试工具 Fiddler Weinre Rythem csscss - 用于检查css代码冗余 FECS - 基于 Node.js 的前端代码检查工具 3. 浏览器扩展(Chrome Extensions) Postman - REST Client Fiddler - Fiddler for Chrome Extension WEB前端助手(FeHelper) Web Developer Chrome Logger ColorZilla ColorPick Eyedropper Code Cola 1px AlloyDesigner - 前端重构开发辅助工具 Fontface Ninja PageSpeed Insights (by Google) HTTP Status Redirect Path Responsive Web Design Tester Window Resizer CSSViewer IE Tab Clear Cache JSONView Image Downloader Pretty Beautiful Javascript - 可以自动格式化混淆的js文件 JavaScript Errors Notifier CSS Diff - 在线比对页面上两个元素的CSS样式差异 WhatFont- 识别网页所使用的字体 前端参考集 frontend-guidelines - Some HTML, CSS and JS best practices. frontend-dev-bookmarks Codrops - Useful resources Front-end Code Standards &amp; Best Practices frontend-dev-bookmarks Airbnb 的 JavaScript 编码规范 awesome-javascript","tags":[{"name":"js","slug":"js","permalink":"ly2513.github.com/tags/js/"},{"name":"前端","slug":"前端","permalink":"ly2513.github.com/tags/前端/"}]},{"title":" Git 添加空文件夹的方法","date":"2016-04-27T03:30:57.000Z","path":"2016/04/27/Git-添加空文件夹的方法/","text":"如果你想添加某个文件夹，但是想忽略其下的所有文件在空目录下创建.gitignore文件。 文件内写入如下代码，可以排除空目录下所有文件被跟踪：1234# Ignore everything in this directory*# Except this file!.gitignore","tags":[{"name":"git","slug":"git","permalink":"ly2513.github.com/tags/git/"}]},{"title":"Awesome PHP(PHP相关资料的收集汇总)","date":"2016-04-26T10:15:20.000Z","path":"2016/04/26/Awesome-PHP/","text":"Awesome PHP一个PHP资源列表，内容包括：库、框架、模板、安全、代码分析、日志、第三方库、配置工具、Web 工具、书籍、电子书、经典博文等等 贡献详细内容请查看贡献 和 代码管理. 目录 Awesome PHP 依赖管理 Dependency Management 其他的依赖管理 Dependency Management Extras 框架 Frameworks 其他框架 Framework Extras 框架组件 Components 微型框架 Micro Frameworks 其他微型框架 Micro Framework Extras 路由 Routers 模板 Templating 静态站点生成器 Static Site Generators 超文本传输协议 HTTP 爬虫 Scraping 中间件 Middlewares 网址 URL 电子邮件 Email 文件 Files 流 Streams 依赖注入 Dependency Injection 图像 Imagery 测试 Testing 持续集成 Continuous Integration 文档 Documentation 安全 Security 密码 Passwords 代码分析 Code Analysis Architectural Architectural 调试和分析 Debugging and Profiling 构建工具 Build Tools 任务运行器 Task Runners 导航 Navigation 资源管理 Asset Management 地理位置 Geolocation 日期和时间 Date and Time 事件 Event 日志 Logging 电子商务 E-commerce PDF PDF Office Office 数据库 Database 迁移 Migrations NoSQL NoSQL 队列 Queue 搜索 Search 命令行 Command Line 身份验证和授权 Authentication and Authorization 标记 Markup 字符串 Strings 数字 Numbers 过滤和验证 Filtering and Validation API API 缓存 Caching 数据结构和存储 Data Structure and Storage 通知 Notifications 部署 Deployment 国际化和本地化 Internationalisation and Localisation 第三方API Third Party APIs 扩展 Extensions 杂项 Miscellaneous 软件 Software PHP安装 PHP Installation 开发环境 Development Environment 虚拟机 Virtual Machines 集成开发环境(IDE) Integrated Development Environment Web应用 Web Applications 基础架构 Infrastructure 资源 Resources PHP网站 PHP Websites 其他网站 Other Websites PHP书籍 PHP Books 其他书籍 Other Books PHP视频 PHP Videos PHP阅读 PHP Reading PHP内核阅读 PHP Internals Reading PHP杂志 PHP Magazines 贡献 依赖管理 Dependency Management依赖和包管理库 Climb - 一个Composer版本管理工具 Composer Installers - 一个多框架Composer库安装器 Composer/Packagist - 一个包和依赖管理器 Melody - 一个用于构建Composer脚本文件的工具 Pickle - 一个PHP扩展安装器 其他的依赖管理 Dependency Management Extras其他的相关依赖管理 Composed - 一个在运行时解析你项目Composer环境的库 Composer Checker - 一个校验Composer配置的工具 Composer Merge Plugin - 一个用于合并多个composer.json文件的Composer插件 Composition - 一个在运行时检查Composer环境的库 NameSpacer - 一个转化下划线到命名空间的库 Patch Installer - 一个使用Composer安装补丁的库 Prestissimo - 一个开启并行安装进程的Composer插件 Satis - 一个静态Composer存储库的生成器 Toran Proxy - 一个静态Composer存储库和代理 框架 FrameworksWeb开发框架 Aura PHP - 一个独立的组件框架 CakePHP - 一个快速应用程序开发框架 (CP) Laravel 5 - 简洁优雅的PHP Web开发框架 (L5) Nette - 另一个由个体组件组成的框架 Phalcon - 通过C扩展实现的框架 PPI Framework 2 - 一个互操作性框架 Symfony 2 - 一个独立组件组成的框架 (SF2) Yii2 - 用于开发大型Web应用的高性能PHP框架 Zend Framework 2 - 另一个由独立组件组成的框架 (ZF2) Radar - 一个基于PHP的Action-Domain-Responder实现 Ice - 另一个通过C扩展实现的简单快速的PHP框架 Phalcon - 一个作为C扩展的框架 Yaf - 鸟哥的C扩展的框架 其他框架 Framework Extras其他Web开发框架 CakePHP CRUD - CakePHP的快速应用程序（RAD）插件 Knp RAD Bundle - Symfony 2的快速应用程序（RAD）包 Symfony CMF - 一个创建自定义CMS的内容管理框架 框架组件 Components来自web开发框架的独立组件 CakePHP Plugins - CakePHP插件的目录 Hoa Project - 另一个PHP组件包 League of Extraordinary Packages - 一个PHP软件开发组 Symfony2 Components - Symfony 2组件 Zend Framework 2 Components - Zend Framework 2组件 微型框架 Micro Frameworks微型框架和路由 Bullet PHP - 用于构建REST APIs的微型框架 Lumen - 一个Laravel的微型框架 Proton - 一个StackPHP兼容的微型框架 Silex - 基于Symfony2组件的微型框架 Slim - 另一个简单的微型框架 其他微型框架 Micro Framework Extras其他相关的微型框架和路由 Silex Skeleton - Silex的项目架构 Silex Web Profiler - 一个Silex web的调试工具 Slim Skeleton - Slim架构 Slim View - Slim自定义视图的集合 路由 Routers处理应用路由的库 Fast Route - 一个快速路由的库 Klein - 一个灵活的路由的库 Macaw - 一个简单的 PHP 路由器，超级精简、快速而且很性感。 Pux - 另一个快速路由的库 Route - 一个基于Fast Route的路由的库 模板 Templating模板化和词法分析的库和工具 Foil - 另一个原生PHP模板库 Lex - 一个轻量级模板解析器 MtHaml - 一个HAML模板语言的PHP实现 Mustache - 一个Mustache模板语言的PHP实现 Phly Mustache - 另一个Mustache模板语言的PHP实现 PHPTAL - 一个TAL模板语言的PHP实现 Plates - 一个原生PHP模板库 Smarty - 一个模板引擎 Twig - 一个全面的模板语言 Tale Jade - Jade模版语言的PHP实现 静态站点生成器 Static Site Generators用来生成web页面的预处理内容的工具 Couscous - 一个将Markdown转化为漂亮的网站的工具 Phrozn - 另一个转换Textile，Markdown和Twig为HTML的工具 Sculpin - 转换Markdown和Twig为静态HTML的工具 Spress - 一个能够将Markdown和Twig转化为HTML的可扩展工具 超文本传输协议 HTTP用于HTTP的库 Buzz - 另一个HTTP客户端 Guzzle - 一个全面的HTTP客户端 HTTPFul - 一个链式HTTP库 PHP VCR - 一个录制和重放HTTP请求的库 php-curl-class - PHP的Curl类 Requests - 一个简单的HTTP库 Retrofit - 一个能轻松创建REST API客户端的库 Zend-diactoros - PSR-7 HTTP消息实现 网址 URL解析URL的库 PHP Domain Parser - 一个本地前缀解析库 Purl - 一个URL处理库 Sabre/uri - 一个URI操作库 Uri - 另一个URL处理库 电子邮件 Email发送和解析邮件的库 CssToInlineStyles - 一个在邮件模板中的内联CSS库 Email Reply Parser - 一个邮件回复解析的库 Email Validator - 一个较小的电子邮件验证库 Fetch - 一个IMAP库 Mautic - 邮件营销自动化 Nette Mail - 一个简单优雅的邮件发送模块 PHPMailer - 另一个邮件解决方案 Stampie - 一个邮件服务库，类似于SendGrid,PostMark,MailGun和Mandrill. SwiftMailer - 一个邮件解决方案 文件 Files文件处理和MIME类型检测的库 Apache MIME Types - 一个解析Apache MIME类型的库 Canal - 一个检测互联网媒体类型的库 CSV - 一个CSV数据处理库 Ferret - 一个MIME检测库 Flysystem - 另一个文件系统抽象层 Gaufrette - 一个文件系统抽象层 Hoa Mime - 另一个MIME检测库 Lurker - 一个资源跟踪库 PHP FFmpeg - 一个用于FFmpeg视频包装的库 PHP File Locator - 一个在大型项目中定位文件的库 流 Streams处理流的库 Streamer - 一个简单的面向对象的流包装库 依赖注入 Dependency Injection实现依赖注入设计模式的库 Acclimate - 一个依赖注入容器和服务定位的通用接口 Auryn - 一个递归的依赖注入容器 Container - 另一个可伸缩的依赖注入容器 PHP-DI - 一个支持自动装配和PHP配置的依赖注入容器 Pimple - 一个小的依赖注入容器 Symfony DI - 一个依赖注入容器组件 (SF2) 图像 Imagery处理图像的库 Color Extractor - 一个从图像中提取颜色的库 GIF Creator - 一个通过多张图片创建GIF动画的库 GIF Frame Extractor - 一个提取GIF动画帧信息的库 Glide - 一个按需处理图像的库 Imagine - 一个图像处理库 Image Hash - 一个用于生成图像哈希感知的库 Image Optimizer - 一个优化图像的库 Image With Text - 一个在图像中嵌入文本的库 Imagine - 一个图像处理库 Intervention Image - 另一个图像处理库 PHP Image Workshop - 另一个图像处理库 PHPThumb - 缩略图处理库 Phpqrcode - 二维码生成库 QrCode - 另一个二维码生成库 测试 Testing测试代码和生成测试数据的库 Alice - 富有表现力的一代库 AspectMock - 一个PHPUnit/Codeception的模拟框架。 Atoum - 一个简单的测试库 Behat - 一个行为驱动开发（BDD）测试框架 Codeception - 一个全栈测试框架 DBUnit - 一个PHPUnit的数据库测试库 Faker - 一个伪数据生成库 HTTP Mock - 一个在单元测试模拟HTTP请求的库 Kahlan - 全栈Unit/BDD测试框架，内置stub，mock和代码覆盖率的支持 Locust - 一个Python开发的现代负载测试库 Mink - Web验收测试 Mockery - 一个用于测试的模拟对象的库 ParaTest - 一个PHPUnit的并行测试库 Peridot - 一个事件驱动开发的测试框架 Phake - 另一个用于测试的模拟对象的库 Pho - 另一个行为驱动开发测试框架 PHPSpec - 一个基于功能点设计的单元测试库 PHPUnit - 一个单元测试框架 Prophecy - 一个可选度很高的模拟框架 Samsui - 另一个伪数据生成库 VFS Stream - 一个用于测试的虚拟文件系统流的包装器 VFS - 另一个用于测试虚拟的文件系统 持续集成 Continuous Integration持续集成的库和应用 GitlabCi - 使用GitLab CI测试、构建、部署你的代码，像TravisCI Jenkins - 一个PHP支持的持续集成平台 JoliCi - 一个用PHP编写的由Docker支持的持续集成的客户端 PHPCI - 一个PHP的开源的持续集成平台 SemaphoreCI - 一个开放源码和私人项目的持续集成平台 Shippable - 一个基于开源和私人项目持续集成平台的docker Sismo - 一个持续测试的服务库 Travis CI - 一个持续集成平台 Wercker - 一个持续集成平台 文档 Documentation生成项目文档的库 APIGen - 另一个API文档生成器 Daux.io - 一个使用Markdown文件的文档生成器 PHP Documentor 2 - 一个API文档生成器 PhpDox - 一个PHP项目的文档生成器（不限于API文档） Sami - 一个API文档生成器 安全 Security生成安全的随机数，加密数据，扫描漏洞的库 Halite - 一个简单的使用libsodium的加密库 HTML Purifier - 一个兼容标准的HTML过滤器 IniScan - 一个扫描PHP INI文件安全的库 jose - JSON签名和加密的库 Optimus - 基于Knuth乘法散列方法的身份混淆工具 PHP Encryption - 一个安全的PHP加密库 PHP IDS - 一个结构化的PHP安全层 PHP SSH - 一个试验的面向对象的SSH包装库 PHPSecLib - 一个纯PHP安全通信库 RandomLib - 一个生成随机数和字符串的库 SecurityMultiTool - 一个PHP安全库 SensioLabs Security Check - 一个为检查Composer依赖提供安全建议的web工具 TCrypto - 一个简单的键值加密存储库 True Random - 使用www.random.org生成随机数的库 VAddy - 一个持续安全的web应用测试平台 Zed - 一个集成的web应用渗透测试工具 密码 Passwords处理和存储密码的库和工具 GenPhrase - 一个随机生成安全密码哈希的库 Password Compat - 一个新的PHP5.5密码函数的兼容库 Password Policy - 一个PHP和JavaScript的密码策略库 Password Validator - 一个校验和升级密码哈希的库 Password-Generator - 一个生成随机密码的PHP库 PHP Password Lib - 一个生成和校验密码的库 phpass - 一个便携式的密码哈希框架 Zxcvbn PHP - 一个基于Zxcvbn JS的现实的PHP密码强度估计库 代码分析 Code Analysis分析，解析和处理代码库的库和工具 Athletic - 一个基于注释的基准检测库 Code Climate - 一个自动代码审查工具 Dissect - 一个词法和语法分析的工具集合 Exakat - 一个PHP的静态分析引擎 GrumPHP - 一个用来保护代码质量的Composer插件 Mondrian - 使用图论的代码分析工具 PHP Analyser - 一个分析PHP代码查找缺陷和错误的库 PHP Code Sniffer - 一个检测PHP、CSS和JS代码标准冲突的库 PHP CS Fixer - 一个编码标准库 PHP Manipulator - 一个分析和修改PHP源代码的库 PHP Mess Detector - 一个扫描代码缺陷，次优代码，未使用的参数等等的库。 PHP Metrics - 一个静态测量库 PHP Parser - 一个PHP编写的PHP解析器 PHP Refactoring Browser - 一个重构PHP代码的命令行工具集 PHP Semantic Versioning Checker - 一个比较两个源集和确定适当的应用语义版本的命令行实用程序 PHPCheckstyle - 一个帮助遵守特定的编码惯例的工具 PHPCPD - 一个检测复制和粘贴代码的库 PhpDependencyAnalysis - 一个创建可定制依赖图的工具 PHPLOC - 一个快速测量PHP项目大小的工具 PHPQA - 一个用于运行质量保证工具的工具(phploc, phpcpd, phpcs, pdepend, phpmd, phpmetrics). PHPPHP - 一个PHP实现的PHP虚拟机 PHPSandbox - 一个PHP沙盒环境 Scrutinizer - 一个审查PHP代码的web工具 UBench - 一个简单的微型基准检测库 Architectural Architectural相关的设计模式库，组织代码编程的方法和途径 Compose - 一个功能组合库 Design Patterns PHP - 一个使用PHP实现的设计模式存储库 Finite - 一个简单的PHP有限状态机 Functional PHP - 一个函数式编程库 Galapagos - 语言转换进化 Iter - 一个使用生成器提供迭代原语的库 Monad PHP - 一个简单Monad库 Patchwork - 一个重新定义用户的函数库 PHP Option - 一个可选的类型库 Pipeline - 一个管道模式的实现 Ruler - 一个简单的无状态的生产环境规则引擎 RulerZ - 一个强大的规则引擎和规范模式的实现 调试和分析 Debugging and Profiling调试和分析代码的库和工具 APM - 一个收集SQLite/MySQL/StatsD错误信息和统计信息的监控扩展 Barbushin PHP Console - 另一个使用Google Chrome的web调试控制台 Blackfire.io - 一个低开销的代码分析器 Kint - 一个调试和分析工具 PHP Console - 一个web调试控制台 PHP Debug Bar - 一个调试工具栏 PHPBench - 一个基准测试框架 PHPDBG - 一个交互的PHP调试器 PHP Console - Web调试控制台 Tideways.io - Monitoring and profiling tool Tracy - A一个简单的错误检测，写日志和时间测量库 xDebug - 一个调试和分析PHP的工具 XHProf - 一个最初由Facebook开发的分析工具 Z-Ray - 一个调试和配置Zend服务器的工具 构建工具 Build Tools项目构建和自动化工具 Bob - 一个简单的项目自动化工具 Box - 一个构建PHAR文件的工具 Go - 一个简单的PHP构建工具 Phake - 一个PHP克隆库 Phing - 一个灵感来自于Apache Ant的PHP项目构建系统 任务运行器 Task Runners自动运行任务的库 Bldr - 一个构建在Symfony组件上的PHP任务运行器 Jobby - 一个没有修改crontab的PHP定时任务管理器 Robo - 一个面向对象配置的PHP任务运行器 Task - 一个灵感来源于Grunt和Gulp的纯PHP任务运行器 导航 Navigation构建导航结构的工具 Cartographer - 一个站点地图生成库 KnpMenu - 一个菜单库 资源管理 Asset Management管理，压缩和最小化web站点资源的工具 Assetic - 一个资源管理的管道库 JShrink - 一个JavaScript的最小化库 Munee - 一个资源优化库 Pipe - 另一个资源管理的管道库 Puli - 一个检测资源绝对路径的库 地理位置 Geolocation地理编码地址和使用纬度经度的库 GeoCoder - 一个地理编码库 GeoJSON - 一个GeoJSON的实现 GeoTools - 一个地理工具相关的库 PHPGeo - 一个简单的地理库 日期和时间 Date and Time处理日期和时间的库 CalendR - 一个日历管理库 Carbon - 一个简单的日期时间API扩展 ExpressiveDate - 另一个日期时间API扩展 Moment.php - 灵感来源于Moment.js的PHP DateTime处理库，支持国际化 事件 Event时间驱动或实现非阻塞事件循环的库 Amp - 一个事件驱动的不阻塞的I/O库 Broadway - 一个事件源和CQRS(命令查询责任分离)库 Cake Event - 一个事件调度的库 (CP) Elephant.io - 另一个web socket库 Evenement - 一个事件调度的库 Event - 一个专注于域名事件的库 Hoa EventSource - 一个事件源库 Hoa WebSocket - 另一个web socket库 Icicle - 一个支持协同，非阻塞I/O，多线程的异步库 Prooph Event Store - 一个持久化事件消息的事件源组件 Ratchet - 一个web socket库 React - 一个事件驱动的非阻塞I/O库. Rx.PHP - 一个reactive扩展库 Workerman - 一个事件驱动的不阻塞的I/O库 日志 Logging生成和处理日志文件的库 Analog - 一个基于闭包的微型日志包 KLogger - 一个易用的兼容PSR-3的日志类 Monolog - 一个全面的日志工具 SeasLog - 一个高性能的PHP日志系统 电子商务 E-commerce处理支付和构建在线电子商务商店的库和应用 Money - 一个Fowler金钱模式的PHP实现 OmniPay - 一个框架混合了多网关支付处理的库 Payum - 一个支付抽象库 Sebastian Money - 另一个处理货币值的库 Shopware - 一个可高度定制的电子商务软件 Swap - 一个汇率库 Sylius - 一个开源的电子商务解决方案 Thelia - 另一个开源的电子商务解决方案 PDF PDF处理PDF文件的库和软件 Dompdf - 一个将HTML转换为PDF的工具 PHPPdf - 一个将XML文件转换为PDF和图片的库 Snappy - 一个PDF和图像生成器库 WKHTMLToPDF - 一个将HTML转换为PDF的工具 Office OfficeLibraries for working with office suite documents. ExcelAnt - 一个操作Excel文档的库 PHPExcel - 一个处理Excel文档的库 PHPPowerPoint - 一个处理PPT文档的库 PHPWord - 一个处理Word文档的库 数据库 Database使用对象关系映射（ORM）或数据映射技术的数据库交互的库 Baum - 一个Eloquent的嵌套集实现 Cake ORM - 对象关系映射工具，利用DataMapper模式实现 (CP) Doctrine Extensions - 一个Doctrine行为扩展的集合 Doctrine - 一个全面的DBAL和ORM Eloquent - 一个简单的ORM(L5) LazyRecord - 一个简单、可扩展、高性能的ORM Medoo - 一个轻量级的加速开发的ORM Pomm - 一个PostgreSQL对象模型管理器 Propel - 一个快速的ORM，迁移库和查询构架器 ProxyManager - 一个为数据映射生成代理对象的工具集 RedBean - 一个轻量级，低配置的ORM Spot2 - 一个MySQL的ORM映射器 迁移 Migrations帮助管理数据库模式和迁移的库 Doctrine Migrations - 一个Doctrine的迁移库 Migrations - 一个迁移管理库 Phinx - 另一个数据库迁移的管理库 PHPMig - 另一个迁移管理库 Ruckusing - 基于PHP下ActiveRecord的数据库迁移，支持MySQL, Postgres, SQLite NoSQL NoSQL处理NoSQL后端的库 Monga - 一个MongoDB抽象库 MongoQB - 一个MongoDB查询构建库 Mongo-php-library - MongoDB 官方PHP库 PHPMongo - 一个MongoDB ORM. Predis - 一个功能完整的Redis库 Phpredis - 另一个功能完整的Redis库 队列 Queue处理事件和任务队列的库 Bernard - 一个多后端抽象库 BunnyPHP - 一个高性能的纯PHP AMQP(RabbitMQ)同步和异步(ReactPHP)库 Gearman - 任务分发系统 Pheanstalk - 一个Beanstalkd客户端库 PHP AMQP - 一个纯PHP AMQP库 Php-resque - 基于redis的消息队列 RedisQueue - 一个基于redis的消息队列的、性能高、使用简单、文档齐全、易扩展的队列组件(推荐使用) Tarantool Queue - PHP绑定Tarantool队列 Thumper - 一个RabbitMQ模式库 搜索 Search在数据上索引和执行查询的库和软件 Elastica - ElasticSearch的客户端库 ElasticSearch PHP - ElasticSearch的官方客户端库 Solarium - Solr的客户端库 Sphinx Search - Sphinx搜索库，提供SphinxQL索引和搜索的功能 SphinxQL query builder - Sphinx搜索引擎的的查询库 命令行 Command Line关于命令行工具的库 Boris - 一个微型PHP REPL Cilex - 一个构建命令行工具的微型框架 CLI Menu - 一个构建CLI菜单的库 CLIFramework - 一个支持完全zsh／bash、子命令和选项约束的命令行框架，这也归功于phpbrew CLImate - 一个输出带颜色的和特殊格式的命令行库 Commando - 另一个简单的命令行选择解析器 Cron Expression - 一个计算cron运行日期的库 GetOpt - 一个命令行选择解析器 GetOptionKit - 另一个命令行选择解析器 Hoa Console - 另一个命令行库 OptParse - 另一个命令行选择解析器 Pecan - 一个事件驱动和非阻塞的shell PsySH - 另一个PHP REPL ShellWrap - -一个简单的命令行包装库 Shunt - 一个在多台远程机器上并行运行命令行的库 定时任务 Crontab 定时任务管理 crontab在 PHP 中的相关封装 Cronlingo - Express crontabs as human friendly phrases Dispatcher - 基于Laravel的定时任务管理 Jobby - 一个 PHP 的定时任务管理器 Swoole-crontab - 基于swoole的定时器程序，支持秒级处理 身份验证和授权 Authentication and Authorization实现身份验证和授权的库 EvaOAuth - 统一接口的 OAuth 登录 PHP 类库 Hawk - 一个Hawk HTTP身份认证库 HybridAuth - 一个开源的社交登陆库 Json Web Token - 使用JSON Tokens进行身份验证和信息传输 Lock - 一种实现访问控制列表（ACL）系统的库 OAuth 1.0 Client - 一个OAuth 1.0客户端的库 OAuth 2.0 Client - 一个OAuth 2.0客户端的库 OAuth2 Server - 另一个OAuth2服务器实现 OAuth2 Server - 另一个OAuth2服务器实现 Opauth - 一个多渠道的身份验证框架 PHP oAuthLib - 另一个OAuth库 Sentinel Social - 一个社交网络身份验证库 Sentinel - 一个混合的身份验证和授权的框架库 Sentry - 认证和授权系统 TwitterOAuth - 一个Twitter OAuth库 TwitterSDK - 一个完全测试的Twitter SDK 标记 Markup处理标记的库 Cebe Markdown - 一个快速的可扩展的Markdown解析器 Ciconia - 另一个支持Github Markdown风格的Markdown解析器 CommonMark PHP - 一个对CommonMark spec全支持的Markdown解析器 Decoda - 一个轻量级标记解析库 Emoji - 一个把Unicode字符和名称转换为表情符号图片的库 HTML to Markdown - 将HTML转化为Markdown HTML5 PHP - 一个HTML5解析和序列化库 Parsedown - 另一个Markdown解析器 PHP Markdown - 一个Markdown解析器 Php-emoji - 一个emoji表情转换库 字符串 Strings解析和处理字符串的库 Agent - 一个基于Mobiledetect的桌面／手机端user agent解析库 ANSI to HTML5 - 一个将ANSI转化为HTML5的库 Color Jizz - 处理和转换颜色的库 Device Detector - 另一个解析user agent字符串的库 Hoa String - 另一个UTF-8字符串库 Jieba-PHP - Python的jieba的PHP端口，自然语言处理的中文文本分词 Mobile-Detect - 一个用于检测移动设备的轻量级PHP类(包括平板电脑) Patchwork UTF-8 - 一个处理UTF-8字符串的便携库 Slugify - 转换字符串到slug的库 SQL Formatter - 一个格式化SQL语句的库 Stringy - 一个多字节支持的字符串处理库 Text - 一个文本处理库 UA Parser - 一个解析user agent字符串的库 URLify - 一个Django中URLify.js的PHP版本 UUID - 生成UUIDs的库 数字 Numbers处理数字的库 ByteUnits - 一个在二进制和度量系统中解析,格式化和转换字节单元的库 LibPhoneNumber for PHP - 一个Google电话号码处理的PHP实现库 Hashids.php - 用来把整数生成唯一字符串（比如：通过加密解密id来隐藏真实id) Math - 一个处理巨大数字的库 Numbers PHP - 一个处理数字的库 PHP Conversion - 另一个用于度量单位间转换的库 PHP Units of Measure - 一个计量单位转换的库 过滤和验证 Filtering and Validation过滤和验证数据的库 Cake Validation - 另一个验证库 (CP) DMS Filter - 一个注释过滤库 Filterus - 一个简单的PHP过滤库 ISO-codes - 一个验证各种ISO和ZIP编码的库(IBAN, SWIFT/BIC, BBAN, VAT, SSN, UKNIN) MetaYaml - 一个支持YAML,JSON和XML的模式验证库 Php-readability - 内容分析算法 Respect Validation - 一个简单的验证库 Upload - 一个处理文件上传和验证的库 Valitron - 另一个验证库 Volan - 另一个简单的验证库 REST和API开发REST-ful API的库和Web工具 API Platform - 暴露出REST API的项目，包含JSON-LD, Hydra格式 Apigility - 一个使用Zend Framework 2构建的API构建器 Drest - 一个将Doctrine实体暴露为REST资源节点的库 Fractal - [最佳实践]数据返回的统一化处理 HAL - 一个超文本应用语言(HAL)构建库 Hateoas - 一个HOATEOAS REST web服务库 Negotiation - 一个内容协商库 Restler - 一个将PHP方法暴露为RESTful web API的轻量级框架 Wsdl2phpgenerator - 一个从SOAP WSDL文件生成PHP类的工具 缓存 Caching缓存数据的库 Alternative PHP Cache (APC) - 打开PHP操作码缓存 APIx Cache - 一个轻量级的PSR-6缓存 CacheTool - 一个使用命令行清除apc/opcode缓存的工具 Cake Cache - 一个缓存库 (CP) Doctrine Cache - 一个缓存库 PhpFastCache - PHP 缓存库 Stash - 另一个缓存库 Zend Cache - 另一个缓存库 (ZF2) 数据结构和存储 Data Structure and Storage实现数据结构和存储技术的库 Ardent - 一个数据结构库 Cake Collection - 一个简单的集合库 (CP) Collections - 一个PHP的集合抽象库 Fractal - 一个转换复杂数据结构到JSON输出的库 Ginq - 另一个基于.NET实现的PHP的LINQ库 JsonMapper - 一个将内嵌JSON结构映射为PHP类的库 PHP Collections - 一个简单的集合库 PINQ - 一个基于.NET实现的PHP的LINQ(Language Integrated Query)库 Serializer - 一个序列化和反序列化数据的库 Totem - -一个管理和创建数据交换集的库 YaLinqo - 另一个PHP的LINQ库 Zend Serializer - 另一个序列化和反序列化数据的库 (ZF2) 通知 Notifications处理通知软件的库 JoliNotif - 一个跨平台的桌面通知库(支持Growl, notify-send, toaster等) Nod - 一个通知库(Growl等) Notification Pusher - 一个设备推送通知的独立库 Notificato - 一个处理推送通知的库 Notificator - 一个轻量级的通知库 Php-pushwoosh - 一个使用Pushwoosh REST Web服务轻松推送通知的PHP库 部署 Deployment项目部署库 Deployer - 一个部署工具 Envoy - 一个用PHP运行SSH任务的工具 Plum - 一个部署库 Pomander - 一个PHP应用部署工具 Rocketeer - PHP世界里的一个快速简单的部署器 Walle-web - 一个开源的web代码发布管理系统 国际化和本地化 Internationalisation and Localisation国际化(I18n)和本地化(L10n)的库 Aura Intl Cake I18n - 消息国际化和日期和数字的本地化 (CP) 第三方API Third Party APIs访问第三方API的库 Amazon Web Service SDK - PHP AWS SDK官方库 Campaign Monitor - Campaign Monitor官方PHP库 Digital Ocean - Digital Ocean API接口库 Dropbox SDK - Dropbox SDK官方PHP库 Github - 一个Github API交互库 PHP Github API - 另一个Github API交互库 S3 Stream Wrapper - Amazon S3流包装库 Stripe - Stripe官方PHP库 Twilio - Twilio官方PHP REST API Twitter OAuth - 一个Twitter OAuth工作流交互库 Twitter REST - 一个Twitter REST API交互库 扩展 Extensions帮助构建PHP扩展的库 PHP CPP - 一个开发PHP扩展的C++库 Zephir - 用于开发PHP扩展，且介于PHP和C++之间的编译语言 杂项 Miscellaneous创建一个开发环境的软件 Annotations - 一个注释库(Doctrine的一部分) Cake Utility - 工具类如Inflector，字符串，哈希，安全和XML (CP) Chief - 一个命令总线库 ClassPreloader - 一个优化自动加载的库 Country List - 所有带有名称和ISO 3166-1编码的国家列表 Embera - 一个Oembed消费库 Essence - 一个用于提取网络媒体的库 Flux - 一个正则表达式构建库 Graphviz - 一个图形库 Hprose-PHP - 一个很牛的RPC库，现在支持25+种语言 JSON Lint - 一个JSON lint工具 JSONPCallbackValidator - 验证JSONP回调的库 Jumper - 一个远程服务执行库 LadyBug - 一个dumper库 Lambda PHP - 一个PHP中的Lambda计算解析器 LiteCQRS - 一个CQRS(命令查询责任分离)库 Metrics - 一个简单的度量API库 Nmap - 一个Nmap PHP包装器 Opengraph - 一个开放图库 Pagerfanta - 一个分页库 PHP Expression - 一个PHP表达式语言 PHP PassBook - 一个iOS PassBook PHP库 PHP-GPIO - 一个用于Raspberry PI的GPIO pin的库 PHPCR - 一个Java内容存储库(JCR)的PHP实现 PHPStack - 一个PHP编写的TCP/IP栈概念 print_o - 一个对象图的可视化器 Procrastinator - 一个运行耗时任务的库 Prooph Service Bus - 轻量级的消息总线，支持CQRS和微服务 RMT - 一个编写版本和发布软件的库 sabre/vobject - 一个解析VCard和iCalendar对象的库 Slimdump - 一个简单的MySQL dumper工具 Spork - 一个处理forking的库 Sslurp - 一个使得SSL处理减少的库 SuperClosure - 一个允许闭包序列化的库 Symfony VarDumper - 一个dumper库(SF2) Underscore - 一个Undersccore JS库的PHP实现 Whoops - 一个不错的错误处理库 PHP安装 PHP Installation在你的电脑上帮助安装和管理PHP的工具 HomeBrew PHP - 一个HomeBrew的PHP通道 HomeBrew - 一个OSX包管理器 PHP Brew - 一个PHP版本管理和安装器 PHP Build - 另一个PHP版本安装器 PHP Env - 另一个PHP版本管理器 PHP OSX - 一个OSX下的PHP安装器 PHP Switch - 另一个PHP版本管理器 VirtPHP - 一个创建和管理独立PHP环境的工具 开发环境 Development Environment创建沙盒开发环境的软件和工具 Ansible - 一个非常简单的编制框架 Phansible - 一个用Ansible构建PHP开发虚拟机的web工具 Protobox - 另一个构建PHP开发虚拟机的web工具 PuPHPet - 一个构建PHP开发虚拟机的web工具 Puppet - 一个服务器自动化框架和应用 Vagrant - 一个便携的开发环境工具 虚拟机 Virtual Machines相关的PHP虚拟机 Hack - 一个PHP进行无缝操作的HHVM编程语言 HHVM - Facebook出品的PHP虚拟机，Runtime和JIT HippyVM - 另一个PHP虚拟机 集成开发环境(IDE) Integrated Development Environment支持PHP的集成开发环境 Eclipse for PHP Developers - 一个基于Eclipse平台的PHP IDE Netbeans - 一个支持PHP和HTML5的IDE PhpStorm - 一个商业PHP IDE Web应用 Web Applications基于Web的应用和工具 3V4L - 一个在线的PHP和HHVM shell Adminer - 一个数据库管理工具 Cachet - 开源状态页面系统 DBV - 一个数据库版本控制应用 Grav - 一个现代的flat－file的CMS MailCatcher - 一个抓取和查看邮件的web工具 PHP Queue - A一个管理后端队列的应用 PhpRedisAdmin - 一个用于管理Redis数据库的简单web界面 PhpPgAdmin - 一个PostgreSQL的web管理工具 PhpMyAdmin - 一个MySQL/MariaDB的web界面 rockmongo - MongoDB管理工具 基础架构 Infrastructure提供PHP应用和服务的基础架构 appserver.io - 一个用PHP写的多线程的PHP应用服务器 php-pm - 一个PHP应用的进程管理器、修改器和负载平衡器 PHP网站 PHP WebsitesPHP相关的有用的网站 Nomad PHP - 一个在线PHP学习资源 PHP Best Practices - 一个PHP最佳实践指南 PHP FIG - PHP框架交互组 PHP Mentoring - 点对点PHP导师组织 PHP School - 学习PHP的开源资源 PHP Security - 一个PHP安全指南 PHP The Right Way - 一个PHP最佳实践的快速指引手册 PHP UG - 一个帮助用户定位最近的PHP用户组(UG)的网站 PHP Versions - 哪些版本的PHP可以用在哪几种流行的Web主机上的列表 PHP Weekly - 一个PHP新闻周刊 PHPTrends - 一个快速增长的PHP类库的概述 Securing PHP - 一个关于PHP安全和库的建议的简报 Seven PHP - 一个PHP社区成员采访的网站 其他网站 Other Websitesweb开发相关的有用网站 Atlassian Git Tutorials - 一个Git教程系列 Hg Init - 一个Mercurial教程系列 Semantic Versioning - 一个解析语义版本的网站 Servers for Hackers - 一个关于服务器管理的新闻通讯 The Open Web Application Security Project (OWASP) - 一个开放软件安全社区 WebSec IO - 一个web安全社区资源 PHP书籍 PHP BooksPHP相关的非常好的书籍 Functional Programming in PHP - 这本书将告诉你如何利用PHP5.3+的新功能的认识函数式编程的原则 Grumpy PHPUnit - 一本Chris Hartjes关于使用PHPUnit进行单元测试的书 Mastering Object-Orientated PHP - 一本Brandon Savage关于PHP面向对象的书 Modern PHP New Features and Good Practices - 一本Josh Lockhart关于新的PHP功能和最佳做法的书 Modernising Legacy Applications in PHP - 一本Paul M.Jones关于遗留PHP应用进行现代化的书 PHP 7 Upgrade Guide - 一本Colin O’Dell的包含所有PHP 7功能和改变的书 PHP Pandas - 一本Dayle Rees关于如何学习写PHP的书 Scaling PHP Applications - 一本Steve Corona关于扩展PHP应用程序的电子书 Securing PHP: Core Concepts - 一本Chris Cornutt关于PHP常见安全条款和实践的书 Signaling PHP - 一本Cal Evans关于在CLI脚本捕获PCNTL信号的书 The Grumpy Programmer’s Guide to Building Testable PHP Applications - 一本Chris Hartjes关于构建PHP应用程序测试的书 XML Parsing with PHP - 这本书涵盖的解析和验证XML文档，利用XPath表达式，使用命名空间，以及如何创建和修改XML文件的编程 其他书籍 Other Books与一般计算和web开发相关的书 Elasticsearch: The Definitive Guide - Clinton Cormley和Zachary Tong编写的与Elasticsearch工作的一本指南 Eloquent JavaScript - Marijin Haverbeke关于JavaScript编程的一本书 Head First Design Patterns - 解说软件设计模式的一本书 Pro Git - Scott Chacon和Ben Straub关于Git的一本书 The Linux Command Line - William Shotts关于Linux命令行的一本书 The Tangled Web — Securing Web Applications - Michal Zalewski关于web应用安全的一本书 Understanding Computation - Tom Stuart关于计算理论的一本书 Vagrant Cookbook - Erika Heidi关于创建 Vagrant环境的一本书 PHP视频 PHP VideosPHP相关的非常不错的视频 PHP Town Hall - 一个随意的Ben Edmunds和Phil Sturgeon的PHP播客 PHP UK Conference - 一个PHP英国会议的视频集合 Programming with Anthony - Anthony Ferrara的视频系列 Taking PHP Seriously - 来自Facebook Keith Adams 讲述PHP优势 PHP阅读 PHP ReadingPHP相关的阅读资料 Composer Primer - Composer初级使用 Composer Stability Flags - 一篇关于Composer稳定性标志的文章 Composer Versioning - 一篇关于Composer版本的文章 Create Your Own PHP Framework - 一部Fabien Potencier的关于如何创建你自己的PHP框架的系列文章 Don’t Worry About BREACH - 一篇关于BREACH攻击和CSRF令牌的文章 On PHP 5.3, Lambda Functions and Closures - 一篇关于lambda函数和闭包的文章 PHP Is Much Better Than You Think - 一篇关于PHP语言和生态圈的文章 PHP Package Checklist - A checklist for successful PHP package development. PHP Sucks! But I Like It! - 一篇关于PHP利弊的文章 Preventing CSRF Attacks - 一篇阻止CSRF攻击的文章 Seven Ways to Screw Up BCrypt - 一篇关于纠正BCrypt实现的文章 Use Env - 一篇关于使用unix环境帮助的文章 PHP内核阅读 PHP Internals Reading阅读PHP内核或性能相关的资料 Disproving the Single Quotes Myth - 一篇关于单，双引号字符串性能的文章 How Big Are PHP Arrays (And Values) Really? - 一篇关于数组原理的文章 How Foreach Works - StackOverflow关于foreach回答的详情 How Long is a Piece of String - 一篇关于字符串原理的文章 PHP Evaluation Order - 一篇关于PHP评估顺序的文章 PHP Internals Book - 一本由三名核心开发编写的关于PHP内核的在线书 PHP RFCs - PHP RFCs主页(请求注解) Print vs Echo, Which One is Faster? - 一篇关于打印和echo性能的文章 The PHP Ternary Operator. Fast or Not? - 一篇关于三元操作性能的文章 Understanding OpCodes - 一篇关于opcodes的文章 When Does Foreach Copy? - 一篇关于foreach原理的文章 Why Objects (Usually) Use Less Memory Than Arrays - 一篇关于对象和数组原理的文章 You’re Being Lied To - 一篇关于内核ZVALs的文章 PHP杂志 PHP Magazines有趣的PHP相关的杂志 php[architect] - 一个致力于PHP的月更的杂志 本博客参考以下资料整理– awesome-php","tags":[{"name":"php","slug":"php","permalink":"ly2513.github.com/tags/php/"}]},{"title":"如何配置nodejs nginx的反向代理","date":"2016-04-25T09:44:17.000Z","path":"2016/04/25/如何配置nodejs-nginx的反向代理/","text":"本篇博客主要介绍在linux下配置nodejs在nginx下的反向代理 1、首先应安装nodejs,在此就不做介绍，自己可以用程序猿的方法去完成 2、安装nginx,我采用的是源码安装，这里也不做介绍 3、进入到/usr/local/nginx/conf目录（这里以你们自己的nginx的实际配置目录为主）在该目录下创建include 文件下，我的配置文件就写在这个文件夹里面 4、进入/usr/local/nginx/conf/include 目录，创建 nginx.node.conf 文件，在里面输入如下代码： 12345678910111213141516171819upstream nodejs &#123; server 127.0.0.1:3000; #server 127.0.0.1:3001; keepalive 64;&#125;server &#123; listen 80; server_name www.demoweb.com demoweb.com; access_log /var/log/nginx/test.log; location / &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-Nginx-Proxy true; proxy_set_header Connection \"\"; proxy_pass http://nodejs; &#125;&#125; 5、进入/usr/local/nginx/conf ，打开nginx.conf, 在http 里面添加 include /usr/local/nginx/conf/include/* 6、重启nginx , 输入 /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf 7、在浏览器输入demoweb.com 就好了.","tags":[{"name":"nginx","slug":"nginx","permalink":"ly2513.github.com/tags/nginx/"},{"name":"nodejs","slug":"nodejs","permalink":"ly2513.github.com/tags/nodejs/"}]},{"title":"她(一)","date":"2016-04-21T13:27:08.000Z","path":"2016/04/21/她/","text":"2015年10月03日–一个很重要的日子，对于我来说是个很重要的日子，因为我和她这天相遇了。当时我并不知道这女孩后来会占据了我的心，而且对我影响很大。我和她相遇是在我们的那个县城–安福，顺便介绍下安福吧，安福寓为平安幸福之意，一个坐落在江西中西部的县城，众所周知的景点武功山的南麓就在该县的西北部；安福是中国火腿之乡、中国竹子之乡、中国樟树之乡、陈山红心杉原产地、全国第一批商品粮基地县、全国首批生态文明建设典范城市,森林覆盖率为61%。回到我与她相遇的情节上来吧，我是经过我姐认识的她。有时候我在想人与人是不是有缘分谁也说不准，我在此之前从未和她有过交集，但是我们还是认识了。。。。。。 这是我第一次加她QQ见到的第一张照片。这是她15年国庆回家前在火车站火车室照的(其实是我猜的。。嘿嘿)。我们相遇的那天其实是有人早已“预谋”好的，那人想必也知道是我姐啦。本来我那个国庆是不打算回家，我的几个姐姐都说回来，叫我也回来，说什么“我爷爷病重了，你不回来看看爷爷吗”，所以我临时买了1号的车票，2号到家的。之后我姐说3号去逛街，我说好啊，我和我姐好久没去县城逛街了，于是3号那天我姐、我姐夫和我一起在县城的街上走着，突然旁边走来一位很靓丽的女孩，这女孩就是她。中午我们就在一家饭馆吃饭，吃饭的时候我就坐她旁边（我眼睛还偷偷往她身上瞟），是个男孩子都会这样的那天她的穿着是这样的，一套粉色的衣服，脚穿的是一双粉色的中跟鞋。之后我们互相留下了各自的联系方式，之后有了接下的一个又一个的故事。。。。回去后我们聊天，得知互相的感觉都很好，我们之间的感情也谈的很顺畅，犹如掉进了蜜罐，相互关心对方，在乎对方，这也是我第一次感受到爱情原来如此的甜美。 记得认识的那年11月底，她去厦门玩，我没时间陪她去，她跟我开玩笑说：“唉，以后我不想和你走的太近”。但是我当真了（当时我并不知道她在跟我开玩笑，因为我把我们之间的感情看得很重）。我还记得当时的聊天记录是这样的1234567她：“唉，以后我不想和你走的太近” “还想我了，都是骗人的” ：我她：“本来开始是想你，开始突然觉得坑爹了” “如果你对我没意思那就算了” ：我她：“那算了，就算了，888” “那你好好玩吧” : 我她：“谢谢你的好意” 这天我心情不好，感觉被她戏弄我的感情了，郁闷。。。。。。但是事情往往会往你的另一个方向发展。第二天上午她主动回我信息，她这样写道1她：“我没心情玩了” 我顿时我明白了她昨天说的那些都是假的，跟我开玩笑的，心情好了很多，因为她至少还会主动跟我聊。我们的聊天记录是这样的1234567891011121314151617181920她：“我没心情玩了” “怎么没心情玩了” ：我她：“因为你” “你不是说不想和我走的太近吗” ：我她：“你想气死我是吗？” “是你气我好吗，你以为我心情好吗” ：我她：“气的想吐血了” “那你以后就不要说那样的话，好吗” ：我她：“那你也不要气我啊！” “我什么时候气过你啊” ：我她：“那你的意思是怪我吗？” “如果你是真心对我有意思就不要在乎别人的看法，好吗？” ：我 “这跟怪谁没意思” ：我 “我只想听你一句你的真心话，你对我有没有想交往的想法” ：我她：“你都没有表态难道要我干嘛吗？”她：“我天天在和你聊天，你的意思就是我吃了没事干，是吗？” “我是认真的，我想和你交往下去” ：我她：“难道我不是吗？” “我也是这么想的，但是刚刚你说的那句话太让我失望了，我觉得有时候我自作多情” ：我她：“我快没疯了” 她：:sob: :sob: :sob: 她的这次旅行厦门算是确定我们都是认真的想谈场恋爱。我心里当时不知道有多开心，就像是蜜蜂采花粉时的喜悦。此时我们相识已经将近两个月了，我都不敢想我们之间的感情发展的那么顺利。她是那么认真（我能感觉到她是认真的）的跟我说：“我天天在和你聊天，你的意思就是我吃了没事干，是吗？”,我还记得当时她说这句话的语气（生气带着严肃）","tags":[{"name":"情感","slug":"情感","permalink":"ly2513.github.com/tags/情感/"}]},{"title":"Git bash 操作记住密码","date":"2016-04-14T13:45:33.000Z","path":"2016/04/14/Git-bash-操作记住密码/","text":"这篇博客主要介绍怎么解决在Windows系统下使用git Bush软件，每次提交代码时出现输入账号、密码问题，怎样记住提交者的账号和密码。git for windows （又名 msysgit）如何记住用户名和密码 第一步、创建存储用户名密码的文件 在home文件夹，一般是在 C:\\Documents and Settings\\Administrator下建立文件 .git-credentials （windows下不允许直接创建以.开头的文件，所以有一个小技巧：先创建一个文件名叫 ）git-credentials ,然后进入 git bash 使用命令：1mv git-credentials .git-credentials 用记事本、sublime等编辑器打开这文件输入，如果用户名中有 @，那么使用 % 代替：1https://&#123;username&#125;:&#123;password&#125;@github.com 比如：1https://zhangsan:123456@github.com 编辑完后保存 .git-credentials这个文件 第二步、添加config项 在任意文件夹下右键进入git bash然后输入：1git config --global credential.helper store 执行完后去查看 C:\\Documents and Settings\\Administrator\\.gitconfig 这个文件，发现最后多了一项：12[credential]helper = store 这样就成功了。然后要重开 git bash 窗口，再提交就不用输入用户名密码 – 此博客用于技术交流与传播，不用于商用，如需转载请注明出处，谢谢","tags":[{"name":"git","slug":"git","permalink":"ly2513.github.com/tags/git/"}]},{"title":"markdown使用教程","date":"2016-04-13T17:00:02.000Z","path":"2016/04/14/markdown使用教程/","text":"段落、标题、区域代码在你要表示的一级标题下面使用三个连续=以上的====字符而二级标题就是在其下面使用三个连续-以上的---字符1234一级标题=========二级标题---------- 效果如下： 一级标题二级标题 使用#号个数来表示html中对应标题大小如果想让标题在html中显示h1效果，你可以这样写：1# h1标题 效果如下： h1标题以此类推12345## h2标题 ### h3标题 #### h4标题##### h5标题###### h6标题 显示效果： h2标题h3标题h4标题h5标题h6标题无序列表markdown也可以编写html中的列表以下是几种无序列表的写法 以 -开头的无序列表123- 菜单1- 菜单2- 菜单3 显示效果： 菜单1 菜单2 菜单3 以*开头的无序列表1234* 部门1* 部门2* 部门3* 部门4 显示效果： 部门1 部门2 部门3 部门4 以`+开头的无序列表1234+ 员工甲+ 员工乙+ 员工丙+ 员工丁 显示效果： 员工甲 员工乙 员工丙 员工丁 问题来了，那对应html中有序列表了，markdown又是怎么写的了接下来就来感受下有序列表 有序列表1231. 攻城狮2. 产品狗3. 程序猿 显示效果： 攻城狮 产品狗 程序猿 对于html中加粗、斜体，markdown的写法如下 修辞和强调表示斜体：将要显示的文字用一对单个*包含1有志者事近成，三千越甲可吞吴，*苦心人天不负*，百二秦关终属楚. 显示效果： 有志者事近成，三千越甲可吞吴，苦心人天不负，百二秦关终属楚. 表示加粗：将要显示的文字用一对两个 ** 或者 一对两个__包含 123有志者事近成，三千越甲可吞吴，**苦心人天不负**，百二秦关终属楚.有志者事近成，三千越甲可吞吴，__苦心人天不负__，百二秦关终属楚. 显示效果： 有志者事近成，三千越甲可吞吴，苦心人天不负，百二秦关终属楚. 有志者事近成，三千越甲可吞吴，苦心人天不负，百二秦关终属楚. 链接markdown中链接的写法也比较简单，下面我们来看看到底怎么写的，见证奇迹的时候到了。。。123[链接的文字](链接的url &quot;这是链接title&quot;)[百度](www.baidu.com &quot;这是百度的链接，你信吗？&quot;) 显示效果： 百度 ##图片网页的中的图片那又是怎么表示了？会跟&lt;a&gt;链接一样吗？啥也别想了，我们看看不就知道啦123![图片的title](图片的url)![百度的logo,你说是吗？](https://ss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/logo/bd_logo1_31bdc765.png ) 显示效果： 代码高亮显示123456function add()&#123; $a = 22; $b = 10; $c = $a * $b; return $c;&#125; &lt;code&gt;标签的代码块 function add(){ $a = 22; $b = 10; $c = $a * $b; return $c; } Use the add() function. &lt;pre&gt;标签的代码块function add(){ $a = 22; $b = 10; $c = $a * $b; return $c; } 问题标题1&gt; 问题1 显示效果： 问题1 绘制表格使用|、-、:符号绘制表格，通过:来控制对其方式12345| 项目 | 价格 | 数量 || --------- | --------: | :-------: || 计算机 | \\$1600 | 5 | | 手机 | \\$600 | 12 || 管线 | \\$16 | 234 | 效果如下 项目 价格 数量 计算机 \\$1600 5 手机 \\$600 12 管线 \\$16 234 以下是Markdown 的相关资料（待完善） Markdown 免费编辑器Windows 平台 MarkdownPad MarkPadLinux平台 ReTextMac平台 Mou在线编辑器 Markable.in Dillinger.io浏览器插件MaDe (Chrome) 高级应用 Sublime Text 2 + MarkdownEditing / 教程 – 此博客用于技术交流与传播，不用于商用，如需转载请注明出处，谢谢","tags":[{"name":"markdown","slug":"markdown","permalink":"ly2513.github.com/tags/markdown/"}]},{"title":"HEXO+Github,搭建属于自己的博客","date":"2016-03-25T05:23:00.000Z","path":"2016/03/25/HEXO-Github-搭建属于自己的博客/","text":"本文是基于mac OS x 系统上搭建hexo环境 hexo是一款基于Node.js的静态博客框架,hexo github链接,这篇教程是针对与Mac的，参考链接，由于原文讲到的hexo是以前的老版本，所以现在的版本配置的时候会有些改动。 之前是想着写博客，一方面是给自己做笔记，可以提升自己的写作、总结能力，一个技术点我们会使用并不难，但是要做到简单通俗的让别人理解，还是需要一定的技巧和经验的。很多类似于CSDN、博客园也都可以写文章，不喜欢它们的页码样式。最近看到一些大神们的博客,貌似都是用hexo写得,我也依葫芦画瓢的搭建了一个。不罗嗦了，直接上搭建步骤。 ##配置环境 安装Node（必须） 作用：用来生成静态页面的 到Node.js官网下载相应平台的最新版本，一路安装即可。 安装Git（必须） 作用：把本地的hexo内容提交到github上去. 安装Xcode就自带有Git，我就不多说了。 申请GitHub（必须） 作用：是用来做博客的远程创库、域名、服务器之类的，怎么与本地hexo建立连接等下讲。 github账号我也不再啰嗦了,没有的话直接申请就行了，跟一般的注册账号差不多，SSH Keys，看你自己了，可以不配制，不配置的话以后每次对自己的博客有改动提交的时候就要手动输入账号密码，配置了就不需要了，怎么配置我就不多说了，网上有很多教程。 正式安装HexoNode和Git都安装好后,首先创建一个文件夹,如blog,用户存放hexo的配置文件,然后进入blog里安装Hexo。 执行如下命令安装Hexo： 1sudo npm install -g hexo 初始化然后，初始化hexo命令： 1hexo init 好啦，至此，全部安装工作已经完成！blog就是你的博客根目录，所有的操作都在里面进行。 生成静态页面 1hexo generate（hexo g也可以） 执行下面命令，可以启动本地服务，进行文章预览调试： 1hexo server 浏览器输入http://localhost:4000 我不知道你们能不能，反正我不能，因为我还有环境没配置好配置Github 建立Repository 建立与你用户名对应的仓库，仓库名必须为【your_user_name.github.io】，固定写法 12# 假如你的github账号是zhangsan,那么仓库的名称就是 zhangsan.github.io 然后建立关联，我的blog在本地/usr/local/var/www/blog，blog是我之前建的东西也全在这里面，有： _config.yml node_modules public source db.json package.json scaffolds themes 现在我们需要_config.yml文件，来建立关联，命令： 1vim _config.yml 翻到最下面，改成我这样子的 deploy: type: git repo: https://github.com/leopardpan/leopardpan.github.io.git branch: master 然后执行命令： 1npm install hexo-deployer-git --save 网上会有很多说法，有的type是github, 还有repository最后面的后缀也不一样，是github.com.git，我也踩了很多坑，我现在的版本是hexo: 3.1.1，执行命令hexo -vsersion就出来了,貌似3.0后全部改成我上面这种格式了。 忘了说了，我没用SSH Keys如果你用了SSH Keys的话直接在github里复制SSH的就行了，总共就两种协议，相信你懂的。 然后，执行配置命令： 1hexo deploy 然后再浏览器中输入http://ly2513.github.io/就行了，我的github的账户叫leopardpan,把这个改成你github的账户名就行了 部署步骤 每次部署的步骤，可按以下三步来进行。 12345hexo cleanhexo generatehexo deploy 一些常用命令： 12345678910111213hexo new&quot;postName&quot; #新建文章hexo new page&quot;pageName&quot; #新建pageName文件夹，在其文件夹下生产index.md文件hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 报错总结 ERROR Deployer not found: git 或者 ERROR Deployer not found: github 解决方法： 1npm install hexo-deployer-git --save 如发生报错： ERROR Process failed: layout/.DS_Store , 那么进入主题里面layout和_partial目录下，使用删除命令： 1rm-rf.DS_Store ERROR Plugin load failed: hexo-server 原因：Besides,utilities are separated into a standalone module.hexo.util is not reachable anymore. 解决方法，执行命令： 1sudo npm install hexo-server 执行命令hexo server，提示：Usage: hexo …. 原因：我认为是没有生成本地服务解决方法，执行命令： 1npm install hexo-server --save 提示：hexo-server@0.1.2 node_modules/hexo-server 表示成功了 这个时候再执行： 1hexo-server 得到：INFOHexois running at http://localhost:4000/,按 Ctrl+C进行撤销 这个时候再点击http://localhost:4000,正常情况下应该是最原始的画面，但是我看到的是： 白板和Cannot GET / 几个字 原因： 由于2.6以后就更新了，我们需要手动配置些东西，我们需要输入下面三行命令： 12345npm install hexo-renderer-ejs --savenpm install hexo-renderer-stylus --savenpm install hexo-renderer-marked --save 这个时候再重新生成静态文件，命令： 1hexo generate（或hexo g） 启动本地服务器： 1hexo server（或hexo s） 再点击网址http://localhost:4000终于可以看到属于你自己的blog啦，😄，虽然很简陋，但好歹有了一个属于自己的小窝了。参考链接，本地已经简单的设置好了，但是现在域名和服务器都是基于自己的电脑，接下来需要跟github进行关联。主题推荐 这里有大量的主题列表使用方法里面 都有详细的介绍，我就不多说了。 我这里有几款个人认为不错的主题，免去你们，一个一个的选了，欢迎吐槽我的审美，😄 Cover- A chic theme with facebook-like cover photo Oishi- A white theme based on Landscape plus and Writing. Sidebar- Another theme based on Light with a simple sidebar TKL- A responsive design theme for Hexo. 一个设计优雅的响应式主题 Tinnypp- A clean, simple theme based on Tinny Writing- A small and simple hexo theme based on Light Yilia- Responsive and simple style 优雅简洁响应式主题，我用得就是这个。 Pacman voidy- A theme with dynamic tagcloud and dynamic snow 一些基本目录文章在source/_posts, 文章支持Markdown语法，可以使用一些MarkDown渲染工具。如果想修改头像可以直接在主题的_config.yml文件里面修改，友情链接，之类的都在这里。开始打理你的博客吧，有什么问题或者建议，都可以提出来，我会继续完善的。修改头像 我当前的路径/usr/local/var/www/blog/themes/yilia，ls 你可以看到 Gruntfile.js _config.yml package.json README.md layout source vim _config.yml 进去，找到 #你的头像url avatar: 后接一个URL就行了，头像就修改成功了 修改主题和作者名字 我当前的路径/usr/local/var/www/blog，执行ls命令 你可以看到 123_config.yml node_modules public source themesdb.json package.json scaffolds ssh-keygen vim _config.yml 进去，找到 author: 李勇，修改成你自己的名字就行了 修改主题，然后继续往下找到 扩展Plugins: http://hexo.io/plugins/Themes: http://hexo.io/themes/进入到/usr/local/var/www/blog/_config.yml中找到 theme:theme:后面接你自己的主题名字就行了,然后分别执行 部署 hexo g 提交 hexo d 你的主题，和名字就修改成功了","tags":[{"name":"gitbub","slug":"gitbub","permalink":"ly2513.github.com/tags/gitbub/"},{"name":"hexo","slug":"hexo","permalink":"ly2513.github.com/tags/hexo/"}]}]